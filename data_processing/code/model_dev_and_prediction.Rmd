---
title: "model_dev_and_prediction"
author: "Valentin"
date: "`r Sys.Date()`"
output: 
  html_document:
      self_contained: false
---

# Set up and inputs
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

library(aws.s3)
aws.signature::use_credentials()
Sys.setenv("AWS_DEFAULT_REGION" = "eu-west-1")


library(tidyverse)
library(sf)
library(readxl)
library(xlsx)
library(stringr)
library(DescTools)
library(rnaturalearth)
library(ggpubr)
library(units)
library(scales)
library(kableExtra)
library(here)
library(tictoc)
library(readstata13)
library(sjmisc)
library(terra) # put it after {raster} such that it superceeds homonym functions. 
library(exactextractr)
library(stars)
library(pals)
library(modelsummary) # necessary to load it after DescTools
library(doParallel)

# ML libraries
library(ranger)
library(parsnip)
library(xgboost)
library(pROC)
library(yardstick)
library(CAST)
library(caret)

library(familiar)

# and packages required but some uses of familiar: 
library(microbenchmark)
library(glmnet)
library(fastcluster)
library(praznik)
library(power.transform)
library(isotree)
library(laGP)
library(harmonicmeanp)
library(mboost)

set.seed(8888)

# install.packages("https://cran.r-project.org/src/contrib/Archive/isotree/isotree_0.5.5.tar.gz",
#                  repos = NULL, 
#                  type = "source")

dir.create(here("temp_data", "model", "caret", "stage_1"), recursive = TRUE)
dir.create(here("temp_data", "model", "caret", "stage_2"), recursive = TRUE)

# These are were summon_familiar will write its outputs
dir.create(here("temp_data", "model", "familiar", "stage_1"), recursive = TRUE)
dir.create(here("temp_data", "model", "familiar", "stage_2"), recursive = TRUE)

# And to save the predictions 
dir.create(here("temp_data", "model", "predictions", "stage_1"), recursive = TRUE)
dir.create(here("temp_data", "model", "predictions", "stage_2"), recursive = TRUE)

# This is to save some of the outputs from being overwritten subsequently (not really used currently). 
dir.create(here("outputs", "familiar", "stage_1", "performance"), recursive = TRUE)
dir.create(here("outputs", "familiar", "stage_1", "explanation_vimp"), recursive = TRUE)
dir.create(here("outputs", "familiar", "stage_1", "models"), recursive = TRUE)
dir.create(here("outputs", "familiar", "stage_2", "performance"), recursive = TRUE)
dir.create(here("outputs", "familiar", "stage_2", "explanation_vimp"), recursive = TRUE)
dir.create(here("outputs", "familiar", "stage_2", "models"), recursive = TRUE)

# This is to save bootstrapped predictions of the model 
dir.create(here("temp_data", "predictions", "stage_1"), recursive = TRUE)


## Functions
# Trase palettes etc. for plots
source(here("code", "theme_trase.R"))

# load in particular the function fn_trader_to_group_names, str_trans, ... 
source(here("code", "USEFUL_STUFF_supplyshedproj.R"))

# use the projected CRS used by BNETD for their 2020 land use map. 
civ_crs <- 32630

MODEL_RESOLUTION_KM = 4
cell_area_ha =(100*MODEL_RESOLUTION_KM^2)

xlabs = c(-8, -6, -4)
ylabs = c(5, 7, 9)

# Model development results - to report model performance
# stage1_results = here(stage1_expdes_repo, "results", "pooled_data")

## Assets

coopbsy = read.csv(file = here("temp_data/private_IC2B/IC2B_v2_coop_bs_year.csv"))
coopbs22 = coopbsy %>% filter(YEAR == 2022)

consol = readRDS(here("temp_data", "actual_links_consolidated.Rdata"))

cells = readRDS(here("temp_data", "prepared_main_dataset", paste0("cell_", MODEL_RESOLUTION_KM, "km.Rdata"))) 

links = readRDS(here("temp_data", "prepared_main_dataset", paste0("cell_links_", MODEL_RESOLUTION_KM, "km.Rdata")))

departements <- read_sf(here("input_data/s3/CIV_DEPARTEMENTS.geojson"))
departements = 
  st_transform(departements, crs = civ_crs)

```

## Pre-process columns
```{r}
cells = 
  cells %>% 
  rename(CELL_PROP_COOP_STATUS_COOPCA = `CELL_PROP_COOP_STATUS_COOP-CA`, 
         CELL_PROP_1_NEAREST_COOP_STATUS_COOPCA = `CELL_PROP_1_NEAREST_COOP_STATUS_COOP-CA`, 
         CELL_PROP_5_NEAREST_COOP_STATUS_COOPCA = `CELL_PROP_5_NEAREST_COOP_STATUS_COOP-CA`) %>% 
  # this is just for descriptives a priori
  mutate(CELL_1ST_STAGE_ONLY = !CELL_ANY_ACTUAL_COOP_LINK & CELL_IS_OBSERVED_COOPOUTLET, 
         CELL_2ND_STAGE_ONLY = CELL_ANY_ACTUAL_COOP_LINK & !CELL_IS_OBSERVED_COOPOUTLET, 
         CELL_BOTH_STAGES    = CELL_ANY_ACTUAL_COOP_LINK & CELL_IS_OBSERVED_COOPOUTLET,
         CELL_NO_DATA = !CELL_NO_POTENTIAL_LINK & !CELL_2ND_STAGE_ONLY & !CELL_1ST_STAGE_ONLY & !CELL_BOTH_STAGES
         # CELL_NO_ACTUAL_LINK_DATA = CELL_ONLY_VIRTUAL_LINK | (!CELL_ONLY_VIRTUAL_LINK & 
         #                                                      CELL_ACTUAL_ONLYOTHER_LINK & 
         #                                                      !CELL_IS_OBSERVED_COOPOUTLET)
         )
# check that these categories are mutually exclusive
stopifnot(
  cells %>% filter(1 == CELL_NO_DATA + CELL_NO_POTENTIAL_LINK + CELL_2ND_STAGE_ONLY + CELL_1ST_STAGE_ONLY + CELL_BOTH_STAGES) %>% 
    nrow() == nrow(cells)
)
# cells %>% filter(CELL_1ST_STAGE_ONLY) %>% View

links = 
  links %>% 
  rename(COOP_STATUS_COOPCA = `COOP_STATUS_COOP-CA`,
         COOP_DISTRICT_SAN_PEDRO = `COOP_DISTRICT_SAN-PEDRO`,
         COOP_DISTRICT_GRAND_LAHOU = `COOP_DISTRICT_GRAND-LAHOU`,
         COOP_DISTRICT_YAKASSE_ATTOBROU = `COOP_DISTRICT_YAKASSE-ATTOBROU`,
         COOP_DISTRICT_ZOUAN_HOUNIEN = `COOP_DISTRICT_ZOUAN-HOUNIEN`,
         COOP_DISTRICT_KOUN_FAO = `COOP_DISTRICT_KOUN-FAO`,
         COOP_DISTRICT_GRAND_BASSAM = `COOP_DISTRICT_GRAND-BASSAM`, 
         COOP_DISTRICT_MBATTO = `COOP_DISTRICT_M'BATTO`)

# Make target a factor
links = 
  links %>% 
  mutate(LINK_IS_ACTUAL_COOP_class = factor(if_else(LINK_IS_ACTUAL_COOP, "Actual_links", "Virtual_links"), 
                                               levels = c("Virtual_links", "Actual_links"))) 

# Make COOP (and not Buying station) ID: 
links = 
  links %>% 
  mutate(LINK_ACTUAL_COOP_ID = gsub("_BS.*", "", LINK_ACTUAL_COOP_BS_ID), 
         LINK_POTENTIAL_COOP_ID = gsub("_BS.*", "", LINK_POTENTIAL_COOP_BS_ID))


# Productive & prospective cells
cells = 
  cells %>% 
  mutate(
    # This one could be moved to Outputs.Rmd
    CELL_PRODUCTION_STATUS = if_else(CELL_COCOA_HA < 80, 
                                          "PROSPECTIVE", 
                                          "PRODUCTIVE"),
   # also compute the share of area under cocoa use, to use below
   CELL_COCOA_SHARE = CELL_COCOA_HA / cell_area_ha, 
   # and of impossible land
   CELL_IMPOSSIBLE_SHARE = CELL_IMPOSSIBLE_HA / cell_area_ha 
 )
table(cells$CELL_PRODUCTION_STATUS)

links = 
  links %>% 
  left_join(
    cells %>% select(CELL_PRODUCTION_STATUS, CELL_COCOA_SHARE, CELL_IMPOSSIBLE_SHARE, CELL_ID), 
    by = "CELL_ID")

# Distribution of cocoa extent in cells where we know there are cocoa farms 
cells_actual = 
  cells %>% 
  filter(CELL_ACTUAL_LINK) 

cells_actual$CELL_COCOA_HA %>% summary() # It goes as low as 2 ha, but has on average 443 ha. 
cells_actual$CELL_COCOA_HA %>%quantile(probs = seq(0.01,0.1,0.01))

# 5% of the cells with actual links is a good threshold, because the cocoa area quantile corresponds to 5% of a cell area
# (i.e., 80ha, since one cell is 1600ha)

cells = 
  cells %>% 
  mutate(tmp_value = TRUE, 
         CELL_DISTRICT_NAME_ORG = CELL_DISTRICT_NAME) %>% # to be able to keep it, bc these non-ohe vars can be useful for summaries
  pivot_wider(names_from = CELL_DISTRICT_NAME, 
              names_prefix = "CELL_DISTRICT_",
              values_from = tmp_value,  
              values_fill = FALSE) 


# Convert DISTANCES from meter to kilometer
cells = 
  cells %>% 
  mutate(CELL_AVG_COOP_BS_10KM_TRI_METER = CELL_AVG_COOP_BS_10KM_TRI*1e-3, 
         CELL_AVG_1_NEAREST_COOP_BS_10KM_TRI_METER = CELL_AVG_1_NEAREST_COOP_BS_10KM_TRI*1e-3, 
         CELL_AVG_5_NEAREST_COOP_BS_10KM_TRI_METER = CELL_AVG_5_NEAREST_COOP_BS_10KM_TRI*1e-3)


# Convert TRI from mm to meter
cells = 
  cells %>% 
  mutate(CELL_TRI_METER = CELL_TRI_MM*1e-3,
         CELL_AVG_COOP_BS_10KM_TRI_METER = CELL_AVG_COOP_BS_10KM_TRI*1e-3, 
         CELL_AVG_1_NEAREST_COOP_BS_10KM_TRI_METER = CELL_AVG_1_NEAREST_COOP_BS_10KM_TRI*1e-3, 
         CELL_AVG_5_NEAREST_COOP_BS_10KM_TRI_METER = CELL_AVG_5_NEAREST_COOP_BS_10KM_TRI*1e-3) %>% 
  select(-CELL_TRI_MM, -CELL_AVG_COOP_BS_10KM_TRI, -CELL_AVG_1_NEAREST_COOP_BS_10KM_TRI, -CELL_AVG_5_NEAREST_COOP_BS_10KM_TRI)

links = 
  links %>% 
  mutate(CELL_TRI_METER = CELL_TRI_MM*1e-3, 
         COOP_BS_10KM_TRI_METER = COOP_BS_10KM_TRI*1e-3) %>% 
  select(-CELL_TRI_MM, -COOP_BS_10KM_TRI)

```


## Remove some cells and links
```{r}
# BASED ON LU 

# Remove cells with virtually no cocoa and no space for cocoa to grow 
summary(cells$CELL_COCOA_HA)
summary(cells$CELL_IMPOSSIBLE_HA)

# Very few cells have more than 80% of their area impossible for cocoa expansion. 
# Consequently, most cells are possibly of interest to our study, either because they have cocoa, or because 
# they have some space for cocoa to expand. 
bef = nrow(cells)
cells = 
  cells %>%  
  filter(!(CELL_COCOA_SHARE < 0.01 & CELL_IMPOSSIBLE_SHARE > 0.8))
aft = nrow(cells)
(bef-aft)

bef = nrow(links)
links = 
  links %>% 
  filter(!(CELL_COCOA_SHARE < 0.01 & CELL_IMPOSSIBLE_SHARE > 0.8))
aft = nrow(links)
(bef-aft)
rm(bef, aft)

# FEW EXTRA-TERRITORIAL LEFT
# Remove cells with no district (very few since we already removed cells outside inland territory in data preparation)
# cells %>% filter(is.na(CELL_DISTRICT_NAME)) %>% pull(SPLIT) %>% unique()
# cells %>% filter(is.na(CELL_DISTRICT_NAME)) %>% nrow()
cells = 
  cells %>% 
  # Remove the few cells that still fall in no district 
  filter(!is.na(CELL_DISTRICT_NAME_ORG)) 

links = 
  links %>% 
  # Remove the few cells that still fall in no district 
  filter(!is.na(CELL_DISTRICT_NAME))
  
# LINKS NOT TO TRAIN NOR PREDICT 
links =
  links %>%
  # Remove links with non-IC2B coops or with other buyers, 
  # because they would count as FALSE on LINK_IS_ACTUAL_COOP, the target var, while not being exactly what we are after
  filter(!LINK_IS_ACTUAL_OTHER) %>% 
  # Remove empty links that only represent cells from where no coop is reachable. 
  # We are not interested in describing, learning from, or predicting in these cells. We will recollect them post-estimation
  filter(!CELL_NO_POTENTIAL_LINK) # %>% 
  # remove  
  # summarise(.by = LINK_POTENTIAL_COOP_BS_ID, CELL_ID)

stopifnot(links %>% 
            filter(!LINK_IS_VIRTUAL) %>% nrow() == sum(links$LINK_IS_ACTUAL_COOP))

```


## Subset development data
Any processing on explanatory features' values should be made before this step so it is done once for all the data
```{r}
cells = 
  cells %>% 
  mutate(SPLIT = if_else(CELL_IS_OBSERVED_COOPOUTLET, "Development set", "No data")) 

stg1_development =
  cells %>% 
  filter(SPLIT == "Development set") 

links = 
  links %>% 
  mutate(SPLIT = if_else(CELL_ACTUAL_LINK & 
                         !CELL_ACTUAL_ONLYOTHER_LINK & 
                         !LINK_POSSIBLE_FALSENEG, # removes links with uncertain outcome status
                         "Development set", "No data")) 

stg2_development =
  links %>% 
  filter(SPLIT == "Development set") 

cells_stg2_development =
  stg2_development %>%
  distinct(CELL_ID, .keep_all = TRUE) %>% 
  select(starts_with("CELL_"))

```

## Formula 1st stage
```{r}
# PREDICTORS

# Identify predictors with no variance to exclude them
# E.g.: 
stg1_development$CELL_PROP_COOP_SSI_ECOM %>% summary()

stg1_development_nocharcol = 
  stg1_development %>% 
  select_if(~!is.character(.)) %>% 
  select(-CELL_DATA_SOURCE)
 
stg1_features_toexclude = 
names(stg1_development_nocharcol)[map_lgl(stg1_development_nocharcol, 
                                function(x){
                                  var(x, na.rm = TRUE) == 0 |
                                  all(is.na(x))
                                })]
# stg1_development %>% select(all_of(stg1_features_toexclude)) %>% View()

# Formula, for all models: 
anyNA(stg1_development)
# names(cells)
# anyNA(cells)
# stg1_development %>% names() %>% grep(~., pattern = "CELL_COUNT_", value = T)
# stg1_development %>% select(
#     contains("_HA") & contains("CELL_AVG"),
# ) %>% names() %>% grep(~., pattern = "CELL_AVG_", value = T)

stg1_allfeatures = 
  cells %>% 
  select(
    # Exclude all LU vars, on both cell and coop level, except for settlements. 
    (starts_with("CELL_AVG_") & !ends_with("_HA")) | 
    contains("SETTLEMENT")                         |
    starts_with("CELL_PROP_")                      | 
    # starts_with("CELL_DISTRICT_")                  |
    starts_with("CELL_COUNT_"),
    # some corrections/additions
    -CELL_PROP_VOLUME_COOPS, -CELL_PROP_VOLUME_OTHERS,
    # - CELL_DISTRICT_NAME_ORG, -CELL_DISTRICT_GEOCODE,
    
    CELL_TRI_METER, CELL_N_BS_WITHIN_DIST, CELL_N_COOP_IN_DPT, CELL_MIN_DISTANCE_METERS, CELL_MIN_TRAVEL_METERS
  ) %>%
  names()

# Include coop-level features summarised at the 1 nearest, the 5 nearest and the whole-coop levels. This increases the simple fit performance from an OOB R2 of .17 (with only summaries at 5 nearest coop-level) to .24  names()  
  
#Exclude features with no variance
stg1_allfeatures %>% length()
stg1_allfeatures = 
  stg1_allfeatures[!stg1_allfeatures %in% stg1_features_toexclude]
stg1_allfeatures %>% length()

# Restricted list of more exogenous features regarding the further applications of the model. 
# This excludes SSI and certification related variables, as well as the number of known buyers. 
stg1_exofeatures = 
  stg1_allfeatures %>% 
  grep(~., pattern = "DISTANCE|TRAVEL|N_BS_WITHIN|N_KNOWN_BS|N_COOP_IN|LICBUY|STATUS|_TRI|SETTLEMENT|COOP_FARMERS$", 
       value = TRUE) %>% 
  unique()

setdiff(stg1_allfeatures, stg1_exofeatures)

stg1_formula_allfeatures = as.formula(paste0("CELL_PROP_VOLUME_COOPS ~ ", paste0(stg1_allfeatures, collapse = " + ")))
stg1_mformula_exofeatures = as.formula(paste0("CELL_PROP_VOLUME_COOPS ~ ", paste0(stg1_exofeatures, collapse = " + ")))

```

## Formula 2nd stage
```{r}
stg2_development %>% names() %>% grep(~., pattern = "CELL_PROP_", value = T)
stg2_development %>% select(
     (ends_with("_HA"))
) %>% names() %>% grep(~., pattern = "CELL_AVG_", value = T)

stg2_allfeatures = 
  stg2_development %>% 
  select(
    # Do not include any LU var, on either cell or coop level, except for settlements. 
    # Do not include SSI-related vars either, because the development data is biased towards CCP, and thus ANY_SSI and not any other SSI. 
    # stg2_development %>% filter(grepl("CARGILL", PRO_ID)) %>% pull(COOP_SSI_CARGILL) %>% summary()
    
    # Cell level
    CELL_TRI_METER, CELL_SETTLEMENT_HA,
    
    # Topologic - i.e. other-link level
    CELL_N_BS_WITHIN_DIST, CELL_N_COOP_IN_DPT, CELL_AVG_N_LICBUY_IN_DPT, 
    # We could add cell level average/prop/count of nearest and potential coops, but they are not in the link data set currently. 
    # CELL_MIN_DISTANCE_METERS, this does not exist either currently. 
    
    # Link level
    LINK_TRAVEL_MINUTES, LINK_TRAVEL_METERS, LINK_DISTANCE_METERS, 
    
    # Coop level 
    starts_with("COOP_FARMERS"), 
    COOP_N_KNOWN_BS, 
    # starts_with("COOP_DISTRICT_"), -COOP_DISTRICT_NAME,
    starts_with("COOP_STATUS_"), 
    COOP_BS_10KM_TRI_METER, COOP_BS_10KM_SETTLEMENT_HA, 
    
    # More endogenous: 
    COOP_N_KNOWN_BUYERS, 
    COOP_RFA, COOP_UTZ, COOP_FT,
    COOP_CERTIFIED 
    # Never include those, because our observed links data is biased towards link existence when there is a SSI (the CCP, Cargill's SSI). 
    # Don't include dummies for other SSIs than Cargill's, because having many training data from Cargill means our observed links data is biased towards link existence when there is no other SSI than Cargill's. 
    # COOP_HAS_SSI, 
    # starts_with("COOP_SSI_"), 
   
  ) %>%
  names()  
  
# Restricted list of more exogenous features regarding the further applications of the model. 
# This additionally exclude certification related variables, as well as the number of known buyers. 
stg2_exofeatures = 
  stg2_allfeatures %>% 
  grep(~., pattern = "DISTANCE|TRAVEL|N_BS_WITHIN|N_KNOWN_BS|N_COOP_IN|LICBUY|STATUS|_TRI|SETTLEMENT|COOP_FARMERS$", 
       value = TRUE) %>% 
  unique()

setdiff(stg2_allfeatures, stg2_exofeatures)

stg2_formula_allfeatures = as.formula(paste0("LINK_IS_ACTUAL_COOP_class ~ ", paste0(stg2_allfeatures, collapse = " + ")))
stg2_mformula_exofeatures = as.formula(paste0("LINK_IS_ACTUAL_COOP_class ~ ", paste0(stg2_exofeatures, collapse = " + ")))



```

# INPUT DESCRIPTIVES 

## Observed links data

### Table 
```{r}
# Filter data
jrcdata <- consol %>% filter(grepl("JRC", PRO_ID))
scdata <- consol %>% filter(grepl("SUSTAINCOCOA", PRO_ID))
kitdata <- consol %>% filter(grepl("KIT", PRO_ID))
wbdata <- consol %>% filter(grepl("WB_", PRO_ID))
cargilldata <- consol %>% filter(grepl("CARGILL", PRO_ID))

paste0("There are ", 
       nrow(jrcdata), " farmer-buyer links, between ",
       length(unique(jrcdata$PRO_ID)), " farmers located in ",
       length(unique(jrcdata$PRO_VILLAGE_NAME)), " villages and ",
       nrow(filter(jrcdata, BUYER_IS_COOP)) + nrow(filter(jrcdata, !BUYER_IS_COOP)), " buyers. ",
       nrow(filter(jrcdata, BUYER_IS_COOP)), " links are with cooperatives, ", 
       nrow(filter(jrcdata, !is.na(COOP_BS_ID))), " of which are links with IC2B, and ", 
       nrow(filter(jrcdata, !BUYER_IS_COOP)), " are links with another kind of buyers. ",
       nrow(filter(jrcdata, !is.na(LINK_VOLUME_KG))), " links have clean volume information."
)

paste0("There are ", 
       nrow(scdata), " farmer-buyer links, between ",
       length(unique(scdata$PRO_ID)), " farmers located in ",
       length(unique(scdata$PRO_VILLAGE_NAME)), " villages and ",
       nrow(filter(scdata, BUYER_IS_COOP)) + nrow(filter(scdata, !BUYER_IS_COOP)), " buyers. ",
       nrow(filter(scdata, BUYER_IS_COOP)), " links are with cooperatives, ", 
       nrow(filter(scdata, !is.na(COOP_BS_ID))), " of which are links with IC2B, and ", 
       nrow(filter(scdata, !BUYER_IS_COOP)), " are links with another kind of buyers. ",
       nrow(filter(scdata, !is.na(LINK_VOLUME_KG))), " links have clean volume information."
)

paste0("There are ", 
       nrow(kitdata), " farmer-buyer links, between ",
       length(unique(kitdata$PRO_ID)), " farmers located in ",
       length(unique(kitdata$PRO_VILLAGE_NAME)), " villages and ",
       "an unknown number of buyers. ",
       nrow(filter(kitdata, BUYER_IS_COOP)), " farmers sell to cooperatives, ", 
       nrow(filter(kitdata, !BUYER_IS_COOP)), " farmers sell to another kind of buyers. ",
       nrow(filter(kitdata, !is.na(LINK_VOLUME_KG))), " links have clean volume information."
)

paste0("There are ", 
       nrow(wbdata), " farmer-buyer links, between ",
       length(unique(wbdata$PRO_ID)), " farmers located in ",
       length(unique(wbdata$PRO_VILLAGE_NAME)), " villages and ",
       "an unknown number of buyers. ",
       nrow(filter(wbdata, BUYER_IS_COOP)), " farmers sell to cooperatives, ", 
       nrow(filter(wbdata, !BUYER_IS_COOP)), " farmers sell to another kind of buyers. ",
       nrow(filter(wbdata, !is.na(LINK_VOLUME_KG))), " links have clean volume information."
)

paste0("There are ", 
       nrow(cargilldata), " farmer-buyer links, between ",
       length(unique(cargilldata$PRO_ID)), " farmers located in ",
       length(unique(cargilldata$PRO_VILLAGE_NAME)), " villages and ",
       nrow(filter(cargilldata, BUYER_IS_COOP)) + nrow(filter(cargilldata, !BUYER_IS_COOP)), " buyers. ",
       nrow(filter(cargilldata, BUYER_IS_COOP)), " links are with cooperatives, ", 
       nrow(filter(cargilldata, !is.na(COOP_BS_ID))), " of which are links with IC2B, and ", 
       nrow(filter(cargilldata, !BUYER_IS_COOP)), " are links with another kind of buyers. ",
       nrow(filter(cargilldata, !is.na(LINK_VOLUME_KG))), " links have clean volume information."
)


(jrc_coop_vol_kg =  sum(filter(jrcdata, BUYER_IS_COOP)$LINK_VOLUME_KG, na.rm = T) )
sum(filter(jrcdata, !BUYER_IS_COOP)$LINK_VOLUME_KG, na.rm = T)
(jrc_total_vol_kg = sum(filter(jrcdata, BUYER_IS_COOP)$LINK_VOLUME_KG, na.rm = T)+sum(filter(jrcdata, !BUYER_IS_COOP)$LINK_VOLUME_KG, na.rm = T))

(sc_coop_vol_kg =  sum(filter(scdata, BUYER_IS_COOP)$LINK_VOLUME_KG, na.rm = T) )
sum(filter(scdata, !BUYER_IS_COOP)$LINK_VOLUME_KG, na.rm = T)
(sc_total_vol_kg = sum(filter(scdata, BUYER_IS_COOP)$LINK_VOLUME_KG, na.rm = T)+sum(filter(scdata, !BUYER_IS_COOP)$LINK_VOLUME_KG, na.rm = T))

(kit_coop_vol_kg =  sum(filter(kitdata, BUYER_IS_COOP)$LINK_VOLUME_KG, na.rm = T) )
sum(filter(kitdata, !BUYER_IS_COOP)$LINK_VOLUME_KG, na.rm = T)
(kit_total_vol_kg = sum(filter(kitdata, BUYER_IS_COOP)$LINK_VOLUME_KG, na.rm = T)+sum(filter(kitdata, !BUYER_IS_COOP)$LINK_VOLUME_KG, na.rm = T))

(wb_coop_vol_kg =  sum(filter(wbdata, BUYER_IS_COOP)$LINK_VOLUME_KG, na.rm = T) )
sum(filter(wbdata, !BUYER_IS_COOP)$LINK_VOLUME_KG, na.rm = T)
(wb_total_vol_kg = sum(filter(wbdata, BUYER_IS_COOP)$LINK_VOLUME_KG, na.rm = T)+sum(filter(wbdata, !BUYER_IS_COOP)$LINK_VOLUME_KG, na.rm = T))
  
# Create summary data frames
summary_data <- data.frame(
Metric = c("Source", 
            "Year", 
            "Spatial information",
            "Avg. Eucl. distance to cooperative (km)",
            "Villages", 
            "Cocoa farmers", 
            "Farmer-Buyer Links", 
                 "With cooperatives", 
                 "With other buyers", 
            "% of cooperative outlet in volumes sold", 
            "Buyers", 
                 "Cooperatives", 
                 "Other buyers"
           ),
  JRC = c(
          "JRC", "2019", "Farm point", 
          round(mean(filter(jrcdata, !is.na(COOP_BS_ID))$LINK_ACTUALONLY_DISTANCE_METERS, na.rm = T)/1000,1),
          length(unique(jrcdata$PRO_VILLAGE_NAME)), 
          length(unique(jrcdata$PRO_ID)), 
          nrow(jrcdata), 
          nrow(filter(jrcdata, !is.na(COOP_BS_ID))), 
          nrow(filter(jrcdata, !BUYER_IS_COOP)),
          paste0(round(100*jrc_coop_vol_kg / jrc_total_vol_kg, 1)),
          length(unique(jrcdata$BUYER_ACTUAL_LINK_ID)),
          length(unique(pull(filter(jrcdata, !is.na(COOP_BS_ID)), BUYER_ACTUAL_LINK_ID))),
          length(unique(pull(filter(jrcdata, !BUYER_IS_COOP), BUYER_ACTUAL_LINK_ID)))
          ),
  SC = c(
          "Sustain-Cocoa project", "2022", "Village point", 
          round(mean(filter(scdata, !is.na(COOP_BS_ID))$LINK_ACTUALONLY_DISTANCE_METERS, na.rm = T)/1000,1),
          length(unique(scdata$PRO_VILLAGE_NAME)), 
          length(unique(scdata$PRO_ID)),
          nrow(scdata), 
          nrow(filter(scdata, !is.na(COOP_BS_ID))), 
          nrow(filter(scdata, !BUYER_IS_COOP)),
          paste0(round(100*sc_coop_vol_kg / sc_total_vol_kg, 1)),
          length(unique(scdata$BUYER_ACTUAL_LINK_ID)),
          length(unique(scdata$BUYER_ACTUAL_LINK_ID)), # repeat the count in links. This other way to count does not count coops that are not in IC2B: length(unique(pull(filter(scdata, !is.na(COOP_BS_ID)), BUYER_ACTUAL_LINK_ID))),
          "NA"
          ),
  KIT = c(
          "Bymolt et al. 2018", "2017", "Village point",
          "NA",
          length(unique(kitdata$PRO_VILLAGE_NAME)), 
          length(unique(kitdata$PRO_ID)), 
          nrow(kitdata), 
          nrow(filter(kitdata, BUYER_IS_COOP)), 
          nrow(filter(kitdata, !BUYER_IS_COOP)), 
          paste0(round(100*kit_coop_vol_kg / kit_total_vol_kg, 1)),
          "NA", 
          "NA",
          "NA"
            ),
  WB = c(
          "World Bank LSMS", "2021", "Village point",
          "NA",
          length(unique(wbdata$PRO_VILLAGE_NAME)), 
          length(unique(wbdata$PRO_ID)), 
          nrow(wbdata), 
          nrow(filter(wbdata, BUYER_IS_COOP)), 
          nrow(filter(wbdata, !BUYER_IS_COOP)), 
          paste0(round(100*wb_coop_vol_kg / wb_total_vol_kg, 1)),
          "NA", 
          "NA",
          "NA"
            ),
  CARGILL = c(
          "Online repository", "2019", "Farm polygon", 
          round(mean(filter(cargilldata, !is.na(COOP_BS_ID))$LINK_ACTUALONLY_DISTANCE_METERS, na.rm = T)/1000,1),
          "NA", # don't know the number of villages
          length(unique(cargilldata$PRO_ID)), 
          nrow(filter(cargilldata, !is.na(COOP_BS_ID))), 
          length(unique(cargilldata$BUYER_ACTUAL_LINK_ID)),
          nrow(filter(cargilldata, !BUYER_IS_COOP)),
          "NA",
          length(unique(pull(filter(cargilldata, !is.na(COOP_BS_ID)), BUYER_ACTUAL_LINK_ID))),
          length(unique(pull(filter(cargilldata, !is.na(COOP_BS_ID)), BUYER_ACTUAL_LINK_ID))),
          length(unique(pull(filter(cargilldata, !BUYER_IS_COOP), BUYER_ACTUAL_LINK_ID)))
          )
) 
colnames(summary_data) = c(" ", "JRC Data", "SC Data", "KIT Data", "WB Data", "Cargill Data")

summary_data

# Create table with kableExtra and add indentation
table <- summary_data %>%
  kbl() %>%
  kable_classic(full_width = F, html_font = "Arial") %>% 
  # column_spec(1, width = "4.7cm") %>% # that's to cut nicely $
  column_spec(1, width = "7.5cm") %>%
  row_spec(8, extra_css = "padding-left: 10px;") %>%
  row_spec(9, extra_css = "padding-left: 10px;") %>%
  row_spec(12, extra_css = "padding-left: 10px;") %>%
  row_spec(13, extra_css = "padding-left: 10px;")

table
# has to be saved manually now, because save_kable saving work
# save_kable(table, here("outputs", "input_data_descriptives", "Observed_links_table.png"))

```


### Map 
Appendix Figure A1
```{r}
consol = 
  consol %>% 
  mutate(Source = case_when(grepl("JRC", PRO_ID) ~ "JRC", 
                            grepl("SUSTAINCOCOA", PRO_ID) ~ "SC", 
                            grepl("CARGILL", PRO_ID) ~ "CARGILL", 
                            TRUE ~ "None"))

consol_farm_sf = 
  consol %>% 
  st_as_sf(coords = c("PRO_LONGITUDE", "PRO_LATITUDE"), crs = 4326, remove = FALSE)

consol_coop_sf = 
  consol %>% 
  filter(!is.na(BUYER_LONGITUDE) & BUYER_IS_COOP) %>% # This excludes other buyers
  distinct(COOP_BS_ID, .keep_all = TRUE) %>% 
  st_as_sf(coords = c("BUYER_LONGITUDE", "BUYER_LATITUDE"), crs = 4326, remove = FALSE) %>% 
  mutate(`Cooperative with observed link` = "Yes") %>% 
  select(Source, `Cooperative with observed link`, geometry)

unlinked_coopbs_sf = 
  coopbs22 %>% 
  filter(!is.na(LONGITUDE) & !COOP_BS_ID %in% consol_coop_sf$COOP_BS_ID) %>% # This excludes coops already in observed links data
  distinct(COOP_BS_ID, .keep_all = TRUE) %>% 
  st_as_sf(coords = c("LONGITUDE", "LATITUDE"), crs = 4326) %>% 
  mutate(
    Source = NA,
    `Cooperative with observed link` = "No"
    ) %>% 
  select(Source, `Cooperative with observed link`, geometry)
  
coops_toplot = 
  rbind(consol_coop_sf,
        unlinked_coopbs_sf) %>% 
  mutate(`Cooperative with observed link` = factor(`Cooperative with observed link`, 
                                                   levels = c("Yes", "No")))

# Function to create a line geometry
create_line <- function(b_lon, b_lat, p_lon, p_lat) {
  st_linestring(matrix(c(b_lon, b_lat, 
                         p_lon, p_lat), 
                       ncol = 2, 
                       byrow = TRUE))
}

# Apply the function to each row
df = consol %>% 
  # Exclude other buyers than IC2B (thus geolocalised) coops. 
  filter(!is.na(BUYER_LONGITUDE) & BUYER_IS_COOP) 

df = df %>% 
  mutate(geometry = mapply(create_line, 
                      BUYER_LONGITUDE, BUYER_LATITUDE, 
                      PRO_LONGITUDE, PRO_LATITUDE, 
                      SIMPLIFY = FALSE)
  )

# Convert to sf object
df <- st_sf(df, crs = 4326) %>% 
  st_transform(civ_crs) %>% 
  arrange(Source)

df = df %>% arrange(desc(PRO_ID))
coops_toplot = coops_toplot %>% arrange(desc(Source))

map_links = 
  ggplot() +
    geom_sf(data = df, aes(col = Source), size = 5) + #, shape = 15
    geom_sf(data = consol_coop_sf, aes(col = Source, 
                                     size  = `Cooperative with observed link`,
                                     shape = `Cooperative with observed link`)) +
    geom_sf(data = unlinked_coopbs_sf, col = "black", alpha = 0.5, aes(
                                     size  = `Cooperative with observed link`,
                                     shape = `Cooperative with observed link`)) +
    
    scale_colour_manual(
        values = c("skyblue", "purple", "darkgreen"), 
        na.value = "black") +

    scale_size_manual(
      values = c("Yes" = 2, 
                 "No" = 0.8)
    ) +
    scale_shape_manual(
      values = c("Yes" = 16, 
                 "No" = 3)
    ) +
  geom_sf(data = departements, fill = "transparent", col = "black") +
  
  theme_bw() + 
  theme(legend.key.size = unit(1, "cm")) +
    scale_x_continuous(breaks = xlabs, labels = paste0(xlabs,'°W')) +
    scale_y_continuous(breaks = ylabs, labels = paste0(ylabs,'°N'))
 
map_links 

ggsave(
  plot = map_links, 
  filename = "map_links.png",
  path = here("outputs", "input_data_descriptives"), 
  width = 25, 
  height = 20,
  units = "cm")

# ggplot() +
#     geom_sf(data = coopbs_omit_sf, col = "red", size = 2) +
#   geom_sf(data = departements, fill = "transparent", col = "black") +
# 
#   theme_bw() +
#   theme(legend.key.size = unit(1, "cm")) +
#     scale_x_continuous(breaks = xlabs, labels = paste0(xlabs,'°W')) +
#     scale_y_continuous(breaks = ylabs, labels = paste0(ylabs,'°N'))

```


## Cells data 

### Structure
```{r}
# How many cells with actual links
# cells %>% 
#   select(!starts_with("CELL_PROP") & !starts_with("CELL_COUNT")) %>% 
#   datasummary_skim()

# nocat = 
#   cells %>%
#   filter(!CELL_NO_POTENTIAL_LINK & !CELL_NO_ACTUAL_LINK_DATA & !CELL_2ND_STAGE_ONLY & !CELL_IS_OBSERVED_COOPOUTLET) %>% 
#   View()
# cell_ids_check = nocat$CELL_ID
# links %>% filter(CELL_ID %in% cell_ids_check) %>% 
#   pull(CELL_ACTUAL_ONLYOTHER_LINK) %>% summary()
# check that these categories are mutually exclusive
stopifnot(
  cells %>% filter(1 == CELL_NO_POTENTIAL_LINK + CELL_NO_DATA + CELL_2ND_STAGE_ONLY + CELL_1ST_STAGE_ONLY + CELL_BOTH_STAGES) %>% 
    nrow() == nrow(cells)
)

# export
(datasummary(N + Percent()  ~ 
              (`No buying station within 72 km` = (CELL_NO_POTENTIAL_LINK==T)) + 
              (`No data` = (CELL_NO_DATA==T)) + 
              (`Data for 1st stage only` = (CELL_1ST_STAGE_ONLY==T)) + 
              (`Data for 2nd stage only` = (CELL_2ND_STAGE_ONLY==T)) + 
              (`Data for both stages` = (CELL_BOTH_STAGES==T)) + 
              1, 
              data = cells,
              fmt = 1,
              align = "ccccccc",
              output = here("outputs", "input_data_descriptives", paste0("cells_",MODEL_RESOLUTION_KM,"km_destat.png"))))
# Cells with actual link data for both stages have JRC or SC data on actual link existence, size and type of buyer (cooperative or other), that is representative for the whole cell. 

# ?tables::Heading

stg1_development %>% nrow()

```
The map on the information content of the Cells data is produced after model development, in order to add the AoA status.

### Features

```{r}
# Report des. stats. for a selection of predictors 

selected_cells_preds =
  stg1_development %>%
  # Some ad hoc conversions
  mutate(across(.cols = contains("_PROP_"), .fns = ~round(.x*100, 1))) %>% 
  mutate(CELL_MIN_DISTANCE_KM = CELL_MIN_DISTANCE_METERS*1e-3,
         CELL_MIN_TRAVEL_KM = CELL_MIN_TRAVEL_METERS*1e-3, 
         CELL_AVG_TRAVEL_KM_5_NEAREST_COOPS = CELL_AVG_TRAVEL_METERS_5_NEAREST_COOPS*1e-3) %>% 
  select(#Sample = SPLIT, 
         `% of cooperative outlet` = CELL_PROP_VOLUME_COOPS, 
         #`Cocoa output from cell (kg)` = CELL_VOLUME_KG,
         `Shortest Eucl. distance to a coop (km)` = CELL_MIN_DISTANCE_KM, 
         `Shortest road distance to a coop (km)` = CELL_MIN_TRAVEL_KM, 
         `Avg. road distance to the 5 nearest coops (km)` = CELL_AVG_TRAVEL_KM_5_NEAREST_COOPS, 
         # `Avg. Eucl. distance to the 5 nearest coops (m)` = CELL_AVG_DISTANCE_METERS_5_NEAREST_COOPS, 
         
         `# buying stations within 72km` = CELL_N_BS_WITHIN_DIST, 
         `# coops in department` = CELL_N_COOP_IN_DPT,
         `# registered licensed buyers in department` = CELL_AVG_N_LICBUY_IN_DPT, 

         `Terrain ruggedness index (m)` = CELL_TRI_METER,
         `Settlements area (ha)` = CELL_SETTLEMENT_HA,
         
         `Avg. settlements area 10km around 5 nearest coops (ha)` = CELL_AVG_5_NEAREST_COOP_BS_10KM_SETTLEMENT_HA,

         `# farmers in 5 nearest coops` = CELL_COUNT_5_NEAREST_COOP_FARMERS,
         `# BS of 5 nearest coops` = CELL_COUNT_5_NEAREST_COOP_N_KNOWN_BS,
         # `Avg. TRI around 5 nearest coops (mm)` = CELL_AVG_5_NEAREST_COOP_BS_10KM_TRI_METER,
         
         # `Nearest coop being SCOOPS` = CELL_PROP_1_NEAREST_COOP_STATUS_SCOOPS,
         `% of 5 nearest coops being SCOOPS` = CELL_PROP_5_NEAREST_COOP_STATUS_SCOOPS,
         # `% of all coops within 72 km being SCOOPS` = CELL_PROP_COOP_STATUS_SCOOPS,
         
         # `Nearest coop being COOP-CA` = CELL_PROP_1_NEAREST_COOP_STATUS_COOPCA,
         `% of 5 nearest coops being COOP-CA` = `CELL_PROP_5_NEAREST_COOP_STATUS_COOPCA`,
         # `% of all coops within 72 km being COOP-CA` = CELL_PROP_COOP_STATUS_COOPCA,

        `% of 5 nearest coops certified` = CELL_PROP_5_NEAREST_COOP_CERTIFIED,
        # `% of 5 nearest coops certified Rainforest Alliance` = CELL_PROP_5_NEAREST_COOP_RFA,
        # `% of 5 nearest coops certified UTZ` = CELL_PROP_5_NEAREST_COOP_UTZ,
        `% of 5 nearest coops with corporate programmes` = CELL_PROP_5_NEAREST_COOP_HAS_SSI
         ) 

selected_cells_preds_tableplot = datasummary_skim(selected_cells_preds)

# These do not work for some reasons
# datasummary_skim(selected_cells_preds, 
#                  output = here("outputs", "input_data_descriptives", paste0("stg1_",MODEL_RESOLUTION_KM,"km_features.html")))
# ggsave(selected_cells_preds_tableplot, 
#        filename = paste0("stg1_",MODEL_RESOLUTION_KM,"km_features.jpeg"), 
#        path = here("outputs", "input_data_descriptives"),
#        width = 25, 
#        height = 20,
#        units = "cm")

# So, to save, run selected_cells_preds_tableplot in Console with Rstudio dezoomed enough, and save it manually. 

```


### LU 
```{r}
# (cells_stg2_development$CELL_COCOA_HA/cell_area_ha) %>% summary()
#   
# stg1_development %>% 
#   select(Sample = SPLIT, 
#           `Dense forest extent (ha)` = CELL_DENSEFOREST_HA,
#           `other forests extent (ha)` = CELL_OTHERFORESTS_HA,
#           `Cocoa extent (ha)` = CELL_COCOA_HA,
#           `Coffee extent (ha)` = CELL_COFFEE_HA,
#           `Rubber extent (ha)` = CELL_RUBBER_HA,
#           `Palm extent (ha)` = CELL_PALM_HA,
#           `Coconut extent (ha)` = CELL_COCONUT_HA,
#           `Cashew extent (ha)` = CELL_CASHEW_HA,
#           `Other agricultural extent (ha)` = CELL_OTHERAG_HA,
#           `Settlements extent (ha)` = CELL_SETTLEMENT_HA,
#           `Water, rock or infrastructure extent (ha)` = CELL_IMPOSSIBLE_HA
#   ) %>% 
#   datasummary_skim(output = here("outputs", "input_data_descriptives", paste0("stg1_",MODEL_RESOLUTION_KM,"km_features.html")))

```


## Links data

### Imbalance
```{r}
# Compute the imbalance ratio in the data (N(N-1) - E)/E as in Mungo et al. 2023.
# "the number of pairs that do not have a link to the number of pairs that do have a link."
# but here, be N the number of cells, and replace 'N-1' by M, the number of cooperatives.
(N_cells = stg2_development$CELL_ID %>% unique() %>% length())
(M_coops = coopbs22$COOP_BS_ID %>% unique() %>% length())
(E_links = stg2_development$LINK_IS_ACTUAL_COOP %>% sum())

(initial_imbalance = (N_cells*M_coops - E_links)/E_links)

# And the actual imbalance, i.e. now that we limited the number of virtual links to within 72km (the max observed distance in actual links).
# i.e. the ratio of virtual to actual links
(N_actual = stg2_development %>% filter(LINK_IS_ACTUAL_COOP) %>% nrow())
(N_virtual = stg2_development %>% filter(!LINK_IS_ACTUAL_COOP) %>% nrow())
(current_imbalance = N_virtual / N_actual)
```

Observed links - redundant with Table 1 currently so not doing it
```{r}
# # This is links data, where no under-sampling has been applied (so the biased sampling towards coops is still in there).

# obs_links_des = 
#   links %>% 
#   filter(LINK_IS_ACTUAL) %>% 
#   mutate(LINK_SOURCE = case_when(
#     grepl("CARGILL", PRO_ID) ~ "Cargill data",
#     grepl("SUSTAINCOCOA", PRO_ID) ~ "SC data",
#     grepl("JRC", PRO_ID) ~ "JRC data",
#   ))
# 
# length(unique(na.omit(obs_links_des$LINK_ID_OTHERS)))
# 
# totbl = 
#   obs_links_des %>% 
#   summarise(.by = LINK_SOURCE,  
#     # Number of 
#     # Links
#     `with coops` = length(unique(na.omit(LINK_ID_COOPS))), # if_else(BUYER_IS_COOP, LINK_ID, NA)
#     `with others` = length(unique(na.omit(LINK_ID_OTHERS))), 
#     # Farms
#     `Farms` = length(unique(na.omit(PRO_ID))), 
#     `Villages` = length(unique(na.omit(PRO_VILLAGE_NAME))), 
#     # Buyers
#     Buyers = length(unique(na.omit(BUYER_ID))),
#     `IC2B coops` = length(unique(na.omit(LINK_ACTUAL_COOP_ID))),
#     `IC2B buying stations` = length(unique(na.omit(LINK_ACTUAL_COOP_BS_ID))),
#     `Not coops` = length(unique(na.omit(if_else(!BUYER_IS_COOP, BUYER_ID, NA))))
#     
#     )
# totbl

```

### Features
```{r}
stg2_allfeatures
selected_links_preds =
  stg2_development %>%
  # Some ad hoc conversions
  # mutate(across(.cols = contains("_PROP_"), .fns = ~round(.x*100, 1))) %>% 
  mutate(LINK_DISTANCE_KM = LINK_DISTANCE_METERS*1e-3,
         LINK_TRAVEL_KM = LINK_TRAVEL_METERS*1e-3, 
         LINK_TRAVEL_MINUTES = LINK_TRAVEL_MINUTES,
         LINK_IS_EXISTING = if_else(LINK_IS_ACTUAL_COOP_class == "Actual_links","Yes","No")
         ) %>% 
  select(#Sample = SPLIT, 
         `Cell-coop link exists` = LINK_IS_EXISTING ,
         `Eucl. distance to coop (km)` = LINK_DISTANCE_KM, 
         `Road distance to coop (km)` = LINK_TRAVEL_KM, 
         `Road duration to coop (min)` = LINK_TRAVEL_MINUTES, 

         `# buying stations within 72km` = CELL_N_BS_WITHIN_DIST, 
         `# coops in department` = CELL_N_COOP_IN_DPT,
         `# registered licensed buyers in department` = CELL_AVG_N_LICBUY_IN_DPT, 

         `Terrain ruggedness index (m)` = CELL_TRI_METER,
         `Settlements area (ha)` = CELL_SETTLEMENT_HA,
         
         `Terrain ruggedness index 10km around coop (m)` = COOP_BS_10KM_TRI_METER,
         `Settlements area 10km around coop (ha)` = COOP_BS_10KM_SETTLEMENT_HA,

         `# farmers in linked coop` = COOP_FARMERS,
         `# BS of linked coop` = COOP_N_KNOWN_BS,

         `SCOOPS status of linked coop` = COOP_STATUS_SCOOPS,
         `COOP-CA status of linked coop` = COOP_STATUS_COOPCA,
         
         `# FT farmers in linked coop` = COOP_FARMERS_FT,
         `# RFA farmers in linked coop` = COOP_FARMERS_RFA,
         `FT certification in linked coop` = COOP_FT,
         `RFA certification in linked coop` = COOP_RFA,
         `UTZ certification in linked coop` = COOP_UTZ,
         
        `Number of known buyers of linked coop` = COOP_N_KNOWN_BUYERS
        ) 


selected_links_preds_tableplot = 
  datasummary_skim(selected_links_preds, 
                 fun_numeric = list(Mean = Mean, SD = SD, Min = Min, Median = Median, Max = Max), 
                 fmt = 2)

selected_links_preds_tableplot
# ggsave(selected_links_preds_tableplot,
#        filename = paste0("stg2_",MODEL_RESOLUTION_KM,"km_features.jpeg"),
#        path = here("outputs", "input_data_descriptives"),
#        width = 25,
#        height = 20,
#        units = "cm")

datasummary_skim(selected_links_preds, 
                 fun_numeric = list(Mean = Mean, SD = SD, Min = Min, Median = Median, Max = Max),
                 output = here("outputs", "input_data_descriptives", paste0("stg2_",MODEL_RESOLUTION_KM,"km_features.html")))


```



# MODEL DEVELOPMENT

## 1st stage

The first stage has `r length(stg1_allfeatures)` potential features, `r length(stg1_exofeatures)` of which are exogenous to further applications. 

### Simple fits 
This is simple because it does not do feature selection nor optimize hyper-parameters.
```{r}
stg1_mfit = ranger(data = stg1_development, 
                   # splitrule="beta",
                   # num.trees = 500,
                   # mtry = 30,
                   # min.node.size= 10,
                   # case.weights = stg1_development$CELL_REPRESENTATIVITY_WEIGHT,
                   formula = stg1_formula_allfeatures, 
                   seed = 8888
                   )
print(stg1_mfit)

stg1_mfit = ranger(data = stg1_development, 
                   # splitrule="beta",
                   # mtry = 20,
                   # case.weights = stg1_development$CELL_REPRESENTATIVITY_NORM_WEIGHT,
                   formula = stg1_mformula_exofeatures, 
                   seed = 8888
                   )
print(stg1_mfit)

```
### CARET 
```{r}
set.seed(8888)
cl <- makePSOCKcluster(detectCores() - 2)
registerDoParallel(cl)
getDoParWorkers()
tic()

model1_loocv5k <- 
  train(form = stg1_formula_allfeatures, 
        data = stg1_development,
        trControl = trainControl(method="LOOCV", returnResamp = "all"), # necessary to assess variability in error est. 
        method = "ranger", 
        importance="permutation", 
        num.trees = 5000,
        tuneLength=10
        # tuneGrid = data.frame(splitrule = "poisson",
        #                         min.node.size = 5,
        #                        mtry = c(8, 20, 40, 60, 77)
        #                        )
        )

toc()
stopCluster(cl)
registerDoSEQ()

saveRDS(model1_loocv5k, 
        file = here("temp_data", "model", "caret", "stage_1", "allfeatures_ranger_LOOCV_10tuneL_5ktrees"))

## EVALUATE 
# model1_loocv5k = readRDS(here("temp_data", "model", "caret", "stage_1", "allfeatures_ranger_LOOCV_10tuneL_5ktrees"))
to_eval = model1_loocv5k
# In this case, all three caret methods give exactly the same values. 
# the finalModel performs better. 
mean(to_eval$resample$RMSE)
to_eval$results[row.names(to_eval$bestTune),"RMSE"]
getTrainPerf(to_eval)$TrainRMSE 
to_eval$finalModel$prediction.error %>% sqrt()

mean(to_eval$resample$Rsquared)
to_eval$results[row.names(to_eval$bestTune),"Rsquared"]
getTrainPerf(to_eval)$TrainRsquared 
to_eval$finalModel$r.squared

# Make RSE manually 
# Relative Squared Error is the ratio of the MSE to the mean of squared differences between observed and averaged outcome values.  
avg_stg1_outcome = stg1_development$CELL_PROP_VOLUME_COOPS %>% mean()
stg1_development = 
  stg1_development %>% 
  mutate(NAIVE_MODEL_SQUARED_ERROR = (CELL_PROP_VOLUME_COOPS - avg_stg1_outcome)^2)
stg1_mean_squared_naive_error = mean(stg1_development$NAIVE_MODEL_SQUARED_ERROR)
(to_eval$resample$RMSE^2 / stg1_mean_squared_naive_error)

# Plot 
metric_resamples = rbind(
  data.frame(distribution = to_eval$resample$RMSE, 
             metric = "RMSE"), 
  data.frame(distribution = to_eval$resample$MAE, 
             metric = "MAE"), 
  data.frame(distribution = to_eval$resample$Rsquared, 
             metric = "R2"), 
  data.frame(distribution = (to_eval$resample$RMSE^2) / stg1_mean_squared_naive_error, 
             metric = "RSE")
)

ggplot(metric_resamples, aes(x = factor(metric, levels = c("RMSE", "MAE", "R2", "RSE")), y = distribution)) + 
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75), trim = FALSE) + 
  labs(title="Performance estimates across resamples of final model", x="Metric", y = "") +
  theme_minimal() +
  theme(title = element_text(face="bold"),
        plot.title = element_text(face="bold", size = 15))

```


### CARET RFE
```{r}
# MAIN MODEL NOW 
# 32h quand même, pour allfeatures_ranger_LOOCV_15tuneL_sizeseveryten_repeatedcv5x5
# on peut faire moins de tuning la prochaine fois, et aussi sauver les resamples pour pouvoir étudier leur variabilité. 

cl <- makePSOCKcluster(detectCores() - 2)
registerDoParallel(cl)
getDoParWorkers()
tic()

model1_rfe <- 
  rfe(form = stg1_mformula_exofeatures, #stg1_formula_allfeatures, 
        data = stg1_development,
        trControl = trainControl(method="boot", number = 20, returnResamp = "all"),
        method = "ranger", 
        importance="permutation", 
        num.trees = 500,
        tuneLength=5,
        # tuneGrid = data.frame(splitrule = "poisson",
        #                   min.node.size = 5,
        #                  mtry = c(8, 20, 40, 60, 77)
        #                  ),
        sizes = c(5, 15, 30, 45, 60), # tell caret how many different feature subset sizes to try. DOes not need to include the total, which is tried out automatically. 
        rfeControl = rfeControl(method = "cv",
                          			# repeats = 5, 
                          			number = 5,
                                returnResamp = "all",
                          			functions = caretFuncs)) #"The latter is useful if the model has tuning parameters that must be determined at each iteration."

model1_rfe   # Have a look at the output
model1_rfe$optVariables   # What features got picked?
# names(stg1_development)[!names(stg1_development) %in% model1_rfe$optVariables] # What variables didn't get picked
toc()
stopCluster(cl)
registerDoSEQ()

model1_rfe$fit
model1_rfe$fit %>% getTrainPerf()

saveRDS(model1_rfe, 
        file = here("temp_data", "model", "caret", "stage_1", "allfeatures_ranger_20boot_10tuneL_sizesevery15_cv5_nootlwb"))

## EVALUATE 
to_eval = model1_rfe$fit 
# Make RSE manually 
# Relative Squared Error is the ratio of the MSE to the mean of squared differences between observed and averaged outcome values.  
avg_stg1_outcome = stg1_development$CELL_PROP_VOLUME_COOPS %>% mean()
stg1_development = 
  stg1_development %>% 
  mutate(NAIVE_MODEL_SQUARED_ERROR = (CELL_PROP_VOLUME_COOPS - avg_stg1_outcome)^2)
stg1_mean_squared_naive_error = mean(stg1_development$NAIVE_MODEL_SQUARED_ERROR)
(to_eval$resample$RMSE^2 / stg1_mean_squared_naive_error)

# Plot 
metric_resamples = rbind(
  data.frame(distribution = to_eval$resample$RMSE, 
             metric = "RMSE"), 
  data.frame(distribution = to_eval$resample$MAE, 
             metric = "MAE"), 
  data.frame(distribution = to_eval$resample$Rsquared, 
             metric = "R2"), 
  data.frame(distribution = (to_eval$resample$RMSE^2) / stg1_mean_squared_naive_error, 
             metric = "RSE")
)

ggplot(metric_resamples, aes(x = factor(metric, levels = c("RMSE", "MAE", "R2", "RSE")), y = distribution)) + 
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75), trim = FALSE) + 
  labs(title="Performance estimates across resamples of final model", x="Metric", y = "") +
  theme_minimal() +
  theme(title = element_text(face="bold"),
        plot.title = element_text(face="bold", size = 15))

```

#### Tests RFE
```{r}
# RFE above: 
# These are not the same, for some reason
impvar1 = varImp(model1_rfe) %>% row.names() 
t_impvar1 = varImp(model1_rfe) %>% filter(Overall>0) %>% row.names() 

impvar2 = varImp(model1_rfe$fit)$importance %>% row.names()
t_impvar2 = varImp(model1_rfe$fit)$importance %>% filter(!duplicated(Overall) & !duplicated(Overall, fromLast = T) ) %>% row.names()

all.equal(impvar1, impvar2)
all.equal(t_impvar1, t_impvar2)

# But use model1_rfe$fit as the reference. 

# 400 boostraps prend quand même ~1h+ ! 
# library(doParallel)
cl <- makePSOCKcluster(detectCores() - 1)
registerDoParallel(cl)

# library(e1071)
model1_caret_rcv = 
  train(
    x = stg1_development %>% select(all_of(stg1_allfeatures)),
    y = stg1_development$CELL_PROP_VOLUME_COOPS, 
    method="ranger", 
    importance="permutation", 
    # splitrule = "variance",
    tuneLength=5,
    trControl=trainControl(method = "repeatedcv",
                              			repeats = 5, 
                              			number = 5)#,savePredictions=T
  )
stopCluster(cl)
registerDoSEQ()

model1_caret_oob$finalModel
model1_caret_boot632$finalModel
model1_caret_boot632$pred

model1_caret_boot632$resample$Rsquared %>% mean()
model1_caret_boot632$finalModel$rsq %>% mean()
model1_caret_boot632$results

updated = update(model1_caret_boot632, param = list(mtry = 24))
updated$finalModel

model1_caret_boot632
model1_caret_oob$pred # this is null, has 1000 rows for bootstrap, and 238 for cv... 

saveRDS(model1_caret, 
        file = here("temp_data", "model", "caret", "stage_1", "allfeatures_rf_400boot632_5tuneL"))

### Different methods 
model1_caret_xgb = 
  train(
    x = stg1_development %>% select(all_of(stg1_allfeatures)),
    y = stg1_development$CELL_PROP_VOLUME_COOPS, 
    method="xgbTree", 
    importance=TRUE, 
    tuneLength=5,
    trControl=trainControl(method="boot632",number=4,savePredictions=T)
  )

saveRDS(model1_caret_xgb, 
        file = here("temp_data", "model", "caret", "stage_1", "allfeatures_xgbtree_4boot632_5tuneL"))
```

### Test Performance across resampling schemes
From readings, (documentation, Stackoverflow etc.) and tests below, it appears that: 
Metrics obtained from $finalModel objects are always a different because they are provided by the native package (ranger here), and notably for R2 because the formula to calculate it is special in caret. 

Metrics obtained from averages across resamples are exactly the same for boot and cv, different (lower in this case at least) for boot632, and not available for oob. 

In any case, getTrainPerf always gives the same metrics as $results (for the best tuned model).  
```{r}
set.seed(8888)
boot10 <- 
  train(form = stg1_formula_allfeatures, 
        data = stg1_development,
        trControl = trainControl(method="boot", number = 10),
        method = "ranger", 
        importance="permutation", 
        # tuneLength=5, 
        tuneGrid = data.frame(splitrule = "poisson",
                                min.node.size = 5,
                               mtry = c(8, 20, 40, 60, 77)
                               )
        )

to_eval = boot10
# In this case, all three caret methods give exactly the same values. 
# the finalModel performs better. 
mean(to_eval$resample$RMSE)
to_eval$results[row.names(to_eval$bestTune),"RMSE"]
getTrainPerf(to_eval)$TrainRMSE 
to_eval$finalModel$prediction.error %>% sqrt()

mean(to_eval$resample$Rsquared)
to_eval$results[row.names(to_eval$bestTune),"Rsquared"]
getTrainPerf(to_eval)$TrainRsquared 
to_eval$finalModel$r.squared

boot63210 <- 
  train(form = stg1_formula_allfeatures, 
        data = stg1_development,
        trControl = trainControl(method="boot632", number = 10),
        method = "ranger", 
        importance="permutation", 
        tuneLength=15)

to_eval = boot63210
# In this case, all three caret methods DO NOT give the same values. 
# The mean is much different than results/getTrainPerf
# the finalModel performs better than the mean, but not than results/getTrainPerf
mean(to_eval$resample$RMSE)
to_eval$results[row.names(to_eval$bestTune),"RMSE"]
getTrainPerf(to_eval)$TrainRMSE 
to_eval$finalModel$prediction.error %>% sqrt()

mean(to_eval$resample$Rsquared)
to_eval$results[row.names(to_eval$bestTune),"Rsquared"]
getTrainPerf(to_eval)$TrainRsquared 
to_eval$finalModel$r.squared

to_eval = readRDS(here("temp_data", "model", "caret", "stage_1", "allfeatures_ranger_100boot632_15tuneL_sizeseveryten_repeatedcv5x5"))$fit
mean(to_eval$resample$RMSE)
to_eval$results[row.names(to_eval$bestTune),"RMSE"]
getTrainPerf(to_eval)$TrainRMSE 
to_eval$finalModel$prediction.error %>% sqrt()

mean(to_eval$resample$Rsquared)
to_eval$results[row.names(to_eval$bestTune),"Rsquared"]
getTrainPerf(to_eval)$TrainRsquared 
to_eval$finalModel$r.squared


boot6321 <- 
  train(form = stg1_formula_allfeatures, 
        data = stg1_development,
        trControl = trainControl(method="boot632", number = 1),
        method = "ranger", 
        importance="permutation", 
        tuneLength=5)

to_eval = boot6321
mean(to_eval$resample$RMSE)
to_eval$results[row.names(to_eval$bestTune),"RMSE"]
getTrainPerf(to_eval)$TrainRMSE 
to_eval$finalModel$prediction.error %>% sqrt()

mean(to_eval$resample$Rsquared)
to_eval$results[row.names(to_eval$bestTune),"Rsquared"]
getTrainPerf(to_eval)$TrainRsquared 
to_eval$finalModel$r.squared


bootoptimism10 <- 
  train(form = stg1_formula_allfeatures, 
        data = stg1_development,
        trControl = trainControl(method="optimism_boot", number = 10),
        method = "ranger", 
        importance="permutation", 
        tuneLength=5)

to_eval = bootoptimism10
# In this case, all three caret methods DO NOT give the same values. 
# The mean is much different than results/getTrainPerf
# the finalModel performs better than the mean, but not than results/getTrainPerf
mean(to_eval$resample$RMSE)
to_eval$results[row.names(to_eval$bestTune),"RMSE"]
getTrainPerf(to_eval)$TrainRMSE 
to_eval$finalModel$prediction.error %>% sqrt()

mean(to_eval$resample$Rsquared)
to_eval$results[row.names(to_eval$bestTune),"Rsquared"]
getTrainPerf(to_eval)$TrainRsquared 
to_eval$finalModel$r.squared

bootall10 <- 
  train(form = stg1_formula_allfeatures, 
        data = stg1_development,
        trControl = trainControl(method="boot_all", number = 10),
        method = "ranger", 
        importance="permutation", 
        tuneLength=5)

to_eval = bootall10
# In this case, all three caret methods DO NOT give the same values. 
# The mean is much different than results/getTrainPerf
# the finalModel performs better than the mean, but not than results/getTrainPerf
mean(to_eval$resample$RMSE)
to_eval$results[row.names(to_eval$bestTune),"RMSE"]
getTrainPerf(to_eval)$TrainRMSE 
to_eval$finalModel$prediction.error %>% sqrt()

mean(to_eval$resample$Rsquared)
to_eval$results[row.names(to_eval$bestTune),"Rsquared"]
getTrainPerf(to_eval)$TrainRsquared 
to_eval$finalModel$r.squared


oob <- 
  train(form = stg1_formula_allfeatures, 
        data = stg1_development,
        trControl = trainControl(method="oob"),
        method = "ranger", 
        importance="permutation", 
          # tuneLength=5
        tuneGrid = data.frame(splitrule = "beta",
                                min.node.size = 5,
                               mtry = c(2, 10, 20, 40, 60, 77)
                               )
  )

to_eval = oob
mean(to_eval$resample$RMSE)
to_eval$results[row.names(to_eval$bestTune),"RMSE"]
getTrainPerf(to_eval)$TrainRMSE 
to_eval$finalModel$prediction.error %>% sqrt()

mean(to_eval$resample$Rsquared)
to_eval$results[row.names(to_eval$bestTune),"Rsquared"]
getTrainPerf(to_eval)$TrainRsquared 
to_eval$finalModel$r.squared

# CROSS VALIDATION
cv5 <- 
  train(form = stg1_formula_allfeatures, 
        data = stg1_development,
        trControl = trainControl(method="cv", number = 5),
        method = "ranger", 
        importance="permutation", 
        tuneLength=5)

to_eval = cv5
# In this case, all three caret methods give exactly the same values. 
# the finalModel performs better. 
mean(to_eval$resample$RMSE)
to_eval$results[row.names(to_eval$bestTune),"RMSE"]
getTrainPerf(to_eval)$TrainRMSE 
to_eval$finalModel$prediction.error %>% sqrt()

mean(to_eval$resample$Rsquared)
to_eval$results[row.names(to_eval$bestTune),"Rsquared"]
getTrainPerf(to_eval)$TrainRsquared 
to_eval$finalModel$r.squared

cv5_10 <- 
  train(form = stg1_formula_allfeatures, 
        data = stg1_development,
        trControl = trainControl(method="repeatedcv", number = 5, repeats = 10),
        method = "ranger", 
        importance="permutation", 
        tuneLength=5)

to_eval = cv5_10
# In this case, all three caret methods give exactly the same values. 
# the finalModel performs better. 
mean(to_eval$resample$RMSE)
to_eval$results[row.names(to_eval$bestTune),"RMSE"]
getTrainPerf(to_eval)$TrainRMSE 
to_eval$finalModel$prediction.error %>% sqrt()

mean(to_eval$resample$Rsquared)
to_eval$results[row.names(to_eval$bestTune),"Rsquared"]
getTrainPerf(to_eval)$TrainRsquared 
to_eval$finalModel$r.squared

rf_boot632 <- 
  train(form = stg1_formula_allfeatures, 
        data = stg1_development,
        trControl = trainControl(method="boot632", number = 10),
        method = "rf", 
        # importance="permutation", 
        tuneLength=5)
rf_boot632
to_eval = rf_boot632
mean(to_eval$resample$RMSE)
to_eval$results[row.names(to_eval$bestTune),"RMSE"]
getTrainPerf(to_eval)$TrainRMSE 
to_eval$finalModel$mse %>% mean() %>% sqrt()

mean(to_eval$resample$Rsquared)
to_eval$results[row.names(to_eval$bestTune),"Rsquared"]
getTrainPerf(to_eval)$TrainRsquared 
to_eval$finalModel$rsq %>% mean()

rf_boot <- 
  train(form = stg1_formula_allfeatures, 
        data = stg1_development,
        trControl = trainControl(method="boot", number = 10),
        method = "rf", 
        # importance="permutation", 
        tuneLength=5)

to_eval = rf_boot
# In this case, all three caret methods give exactly the same values. 
# the finalModel performs better. 
mean(to_eval$resample$RMSE)
to_eval$results[row.names(to_eval$bestTune),"RMSE"]
getTrainPerf(to_eval)$TrainRMSE 
to_eval$finalModel$mse %>% mean() %>% sqrt()

mean(to_eval$resample$Rsquared)
to_eval$results[row.names(to_eval$bestTune),"Rsquared"]
getTrainPerf(to_eval)$TrainRsquared 
to_eval$finalModel$rsq %>% mean()

# LOOCV 
# This is quite longer, so parallelize.  
cl <- makePSOCKcluster(detectCores() - 1)
registerDoParallel(cl)
getDoParWorkers()
tic()
lvoo <-
  train(form = stg1_formula_allfeatures,
        data = stg1_development,
        trControl = trainControl(method="LOOCV"),
        method = "ranger",
        importance="permutation",
        tuneLength=5
        # tuneGrid = data.frame(splitrule = "beta",
        #                         min.node.size = 5,
        #                        mtry = c(8, 20, 40, 60, 77)
        #                        )
        )
toc()
stopCluster(cl)
registerDoSEQ()

to_eval = lvoo
mean(to_eval$resample$RMSE)
to_eval$results[row.names(to_eval$bestTune),"RMSE"]
getTrainPerf(to_eval)$TrainRMSE 
to_eval$finalModel$mse %>% mean() %>% sqrt()

mean(to_eval$resample$Rsquared)
to_eval$results[row.names(to_eval$bestTune),"Rsquared"]
getTrainPerf(to_eval)$TrainRsquared 
to_eval$finalModel$rsq %>% mean()


# Plot pred object

# That's the average across all the 500 trees
model1_caret$finalModel$mse %>% mean()
# But the ensemble's mse is averaged with weights equal to each tree's performance (or something)
model1_caret$finalModel

# For XGB
resample1_xgb = model1_caret_xgb$resample 

mean(resample1_xgb$RMSE^2)
sd(resample1_xgb$RMSE^2)
mean(resample1_xgb$Rsquared)
sd(resample1_xgb$Rsquared)

saved_caret_xgb = readRDS(here("temp_data", "model", "caret", "stage_1", "allfeatures_xgbtree_4boot632_5tuneL"))
resample1_xgb = saved_caret_xgb$resample 

mean(resample1_xgb$RMSE^2)
sd(resample1_xgb$RMSE^2)
mean(resample1_xgb$Rsquared)
sd(resample1_xgb$Rsquared)


# This is to access performance metrics aggregated over resamples for different models 
resamps <- resamples(list(first_caret = model1_caret_xgb, first_caret2 = model1_caret))
summary(resamps)

```

## 2nd stage

### Simple fit
```{r}
set.seed(8888)

stg2_simple =
  boost_tree(
    mode = "classification",
    mtry = 6,
    trees = 5,
    min_n = 5
  ) %>%
  fit(data = stg2_development,
      formula = stg2_formula_allfeatures)

stg2_simple

```


### CARET
```{r}
# Custom summary function (otherwise not possible to obtain all metrics in a single run of caret::train)
customSummary <- function(data, lev = NULL, model = NULL) {
  # Calculate twoClassSummary metrics
  twoClass <- twoClassSummary(data, lev, model)
  
  # Calculate prSummary metrics
  pr <- prSummary(data, lev, model)
  
  # Combine the results
  out <- c(twoClass, pr)
  
  # Add accuracy
  accuracy <- sum(data$obs == data$pred) / length(data$obs)
  out <- c(out, Accuracy = accuracy)
  
  return(out)
}

# grid of Hyper-Parameters to search in, fixing some to the most conservative values
stg2_HPgrid =  expand.grid(
  eta = 0.01,
  gamma = 10,
  colsample_bytree = c(1, 0.5, 1),
  min_child_weight = c(0.1, 0.5, 1),
  subsample = 0.5,
  max_depth = c(1, 5, 10),
  nrounds = (1:3)*50
)

# *WITHOUT* INSIDE UNDER-SAMPLING OF NONEXISTENT LINKS TO SAME FREQUENCY AS EXISTENT ONES
# tic()
# 
# set.seed(8888)
# seeds <- vector(mode = "list", length = 6)
# for(i in 1:6) seeds[[i]] <- sample.int(1000, 36)
# seeds[[6]] = sample.int(1000, 1)
# 
# model2_caret_xgb = 
#   train(
#     x = stg2_development %>% select(all_of(stg2_allfeatures)),
#     y = stg2_development$LINK_IS_ACTUAL_COOP_class, 
#     method="xgbTree", 
#     metric = "ROC",
#     # tuneLength=5, 
#     tuneGrid = stg2_HPgrid,
#     # More about this function here: https://topepo.github.io/caret/model-training-and-tuning.html#control
#     trControl=trainControl(method="cv",number=5,
#                            savePredictions=T, # TRUE is equivalent to "all" and is necessary to use thresholder() later on. 
#                            #returnResamp = TRUE,
#                            verboseIter = TRUE,
#                            summaryFunction = customSummary, 
#                            classProbs = TRUE,  # necessary to obtain ROC, 
#                            # sampling = "down",
#                            seeds = seeds
#                            ), 
#     verbose = FALSE
#   )
# toc()
# 
# model2_caret_xgb

# *WITH* INSIDE UNDER-SAMPLING OF NONEXISTENT LINKS TO SAME FREQUENCY AS EXISTENT ONES
# (sampling argument in trainControl)
tic()

set.seed(8888)
seeds <- vector(mode = "list", length = 6)
for(i in 1:6) seeds[[i]] <- sample.int(1000, 36)
seeds[[6]] = sample.int(1000, 1)

model2_caret_xgb_down = 
  train(
    x = stg2_development %>% select(all_of(stg2_allfeatures)),
    y = stg2_development$LINK_IS_ACTUAL_COOP_class, 
    method="xgbTree", 
    metric = "ROC",
    # tuneLength=5, 
    tuneGrid = stg2_HPgrid,
    # More about this function here: https://topepo.github.io/caret/model-training-and-tuning.html#control
    trControl=trainControl(method="cv",number=5,
                           savePredictions=T, # TRUE is equivalent to "all" and is necessary to use thresholder() later on. 
                           #returnResamp = TRUE,
                           verboseIter = TRUE,
                           summaryFunction = customSummary, 
                           classProbs = TRUE,  # necessary to obtain ROC, 
                           sampling = "down",
                           seeds = seeds
                           ), 
    verbose = FALSE
  )
toc()

saveRDS(model2_caret_xgb_down, 
        here("temp_data", "model", "caret", "stage_2", "model2_caret_xgb"))
```



# READ MODELS
## 1st stage
This block produces convenient finder objects to access familiar outputs. 
```{r}
# Read caret model 
model1_rfe = readRDS(here("temp_data", "model", "caret", "stage_1", "allfeatures_ranger_20boot_10tuneL_sizesevery15_cv5_nootlwb"))

```

## 2nd stage 
```{r}
model2_caret_xgb = readRDS(here("temp_data", "model", "caret", "stage_2", "model2_caret_xgb"))

```


# PERFORMANCE

## 1st stage 
CARET
```{r}
  
to_eval = model1_rfe$fit

mean(to_eval$resample$RMSE)
to_eval$results[row.names(to_eval$bestTune),"RMSE"]
getTrainPerf(to_eval)$TrainRMSE 
to_eval$finalModel$prediction.error %>% sqrt()

mean(to_eval$resample$Rsquared)
to_eval$results[row.names(to_eval$bestTune),"Rsquared"]
getTrainPerf(to_eval)$TrainRsquared 
to_eval$finalModel$r.squared

# Make RSE manually 
# Relative Squared Error is the ratio of the MSE to the mean of squared differences between observed and averaged outcome values.  
avg_stg1_outcome = stg1_development$CELL_PROP_VOLUME_COOPS %>% mean()
stg1_development = 
  stg1_development %>% 
  mutate(NAIVE_MODEL_SQUARED_ERROR = (CELL_PROP_VOLUME_COOPS - avg_stg1_outcome)^2)
stg1_mean_squared_naive_error = mean(stg1_development$NAIVE_MODEL_SQUARED_ERROR)
(mean(to_eval$resample$RMSE^2) / stg1_mean_squared_naive_error)

# Plot 
metric_resamples = rbind(
  data.frame(distribution = to_eval$resample$RMSE, 
             metric = "RMSE"), 
  data.frame(distribution = to_eval$resample$MAE, 
             metric = "MAE"), 
  data.frame(distribution = to_eval$resample$Rsquared, 
             metric = "R2"), 
  data.frame(distribution = (to_eval$resample$RMSE^2) / stg1_mean_squared_naive_error, 
             metric = "RSE")
)

perf_plot_stg1 = 
  ggplot(metric_resamples, aes(x = factor(metric, levels = c("RMSE", "MAE", "R2", "RSE")), y = distribution)) + 
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75), trim = FALSE) + 
  labs(title="", x="Metric", y = "") +
  theme_minimal() +
  theme(title = element_text(face="bold"),
        plot.title = element_text(face="bold", size = 15))

perf_plot_stg1

ggsave(perf_plot_stg1, 
       filename = "perf_exofeatures_ranger_20boot_10tuneL_sizesevery15_cv5_nootlwb.png", 
       path = here("outputs"),
       width = 25, 
       height = 20,
       units = "cm")

```

## 2nd stage
```{r}
model2_caret_xgb

model2_perf = getTrainPerf(model2_caret_xgb)

# Access resampling results
resamples <- model2_caret_xgb$resample

# Calculate mean of performance metrics
#  standard deviations commented out because meaningless in current 5 cv 
(mean_roc <- mean(resamples$ROC))
# sd_roc <- sd(resamples$ROC)
(mean_sens <- mean(resamples$Sens))
# sd_sens <- sd(resamples$Sens)
(mean_spec <- mean(resamples$Spec))
# sd_spec <- sd(resamples$Spec)

(mean_prec <- mean(resamples$Precision))
(mean_recall <- mean(resamples$Recall))
(mean_accu <- mean(resamples$Accuracy))

# This is to access performance metrics aggregated over resamples for different models 
# resamps <- resamples(list(first_caret = model2_caret_xgb, first_caret2 = model2_caret_rf))
# summary(resamps)

# And to run statistical tests of the diff in performance between models: 
# https://topepo.github.io/caret/model-training-and-tuning.html#extracting-predictions-and-class-probabilities



```

# EXPLANATION
Importance ranks features wrt. each other but cannot tell direction or magnitude. 
Explanation is produced by feature, and can be interpreted in target scale. 

### 1st stage 
#### Feature importance
CARET
```{r}
featimp = varImp(model1_rfe$fit)$importance %>% as.data.frame() %>% arrange(desc(Overall))

featimp = varImp(model1_rfe) %>% as.data.frame() %>% arrange(desc(Overall))

```

```{r}
# # Report separately for most exogenous features and more endogenous ones 
# featimp_w =
#   featimp %>% 
#   mutate(PREDICTOR = row.names(featimp)) %>% 
#   pivot_wider(names_from = PREDICTOR, 
#               values_from = Overall) 
# 
# # %>% 
#   select(#Sample = SPLIT, 
#          #`Coop outlet share` = CELL_PROP_VOLUME_COOPS, 
#          #`Cocoa output from cell (kg)` = CELL_VOLUME_KG,
#          
#          `Shortest Eucl. distance to a coop (m)` = CELL_MIN_DISTANCE_METERS, 
#          `Shortest road distance to a coop (m)` = CELL_MIN_TRAVEL_METERS, 
#          `Avg. Eucl. distance of the 5 nearest coops (m)` = CELL_AVG_DISTANCE_METERS_5_NEAREST_COOPS, 
#          `Avg. road distance of the 5 nearest coops (m)` = CELL_AVG_TRAVEL_METERS_5_NEAREST_COOPS, 
#          
#          `# buying stations within 72km` = CELL_N_BS_WITHIN_DIST, 
#          `# coops in department` = CELL_N_COOP_IN_DPT,
#          `# registered licensed buyers in department` = CELL_AVG_N_LICBUY_IN_DPT, 
# 
#          `Settlements area (ha)` = CELL_SETTLEMENT_HA,
#          `Terrain ruggedness (TRI, mm)` = CELL_TRI_MM,
#          
#          `Nearest coop being SCOOPS` = CELL_PROP_1_NEAREST_COOP_STATUS_SCOOPS,
#          `Proportion of 5 nearest coops being SCOOPS` = CELL_PROP_5_NEAREST_COOP_STATUS_SCOOPS,
#          `Proportion of all coops within 72 km being SCOOPS` = CELL_PROP_COOP_STATUS_SCOOPS,
#          
#          `Nearest coop being COOP-CA` = CELL_PROP_1_NEAREST_COOP_STATUS_COOPCA,
#          `Proportion of 5 nearest coops being COOP-CA` = `CELL_PROP_5_NEAREST_COOP_STATUS_COOPCA`,
#          `Proportion of all coops within 72 km being COOP-CA` = CELL_PROP_COOP_STATUS_COOPCA,
#          
#          `Total # farmers of 5 nearest coops` = CELL_COUNT_5_NEAREST_COOP_FARMERS,
#          `Total # BS of 5 nearest coops` = CELL_COUNT_5_NEAREST_COOP_N_KNOWN_BS,
#          `Avg. TRI around 5 nearest coops (mm)` = CELL_AVG_5_NEAREST_COOP_BS_10KM_TRI,
# 
#          `Avg. settlements extent around 5 nearest coops` = CELL_AVG_5_NEAREST_COOP_BS_10KM_SETTLEMENT_HA
#          ) 

# datasummary_skim(data = stg1_development_exo_features,
#                 output = here("outputs", "input_data_descriptives", paste0("cells_stg1dev_",MODEL_RESOLUTION_KM,"km_exo_features.png")))


# For more endogeneous features


```

#### Explanation
CARET
```{r}
featurePlot(x = BostonHousing[, regVar], 
            y = BostonHousing$medv, 
            plot = "scatter",
            type = c("p", "smooth"),
            span = .5,
            layout = c(3, 1))
```

## 2nd stage 
Feature importance
```{r}
varImp(model2_caret_xgb)

model2_caret_xgb$finalModel$importance %>% as.data.frame() %>% arrange(desc(`%IncMSE`))

```

Explanation
```{r}

```


# PREDICTION
Here, we build the prediction setS for each stage. 
Based on AoA of 1st stage model. 
Moreover, we report here on the Novelty of cells and links in (and out of) the prediction sets. 

## AoA
```{r}
# Requires the model to be trained with caret, with cross-validation and not bootstraps
set.seed(8888)
all_or_used_features = varImp(model1_rfe$fit)$importance %>% row.names()
  # model1_rfe$optVariables # model1@required_features #stg1_allfeatures #

# This is if we model 1st stage with a package/function that does not work with AOA 
# model1_caret_cv = 
#   train(
#     x = stg1_development %>% select(all_of(all_or_used_features)), 
#     y = stg1_development$CELL_PROP_VOLUME_COOPS, 
#     method="rf", 
#     importance=TRUE, 
#     tuneLength=1,
#     trControl=trainControl(method="cv",number=5,savePredictions=T)
#   )
# model1_caret_cv$results

# this can be removed once we have retrained 1st stage model. 
# all_or_used_features = gsub("_TRI$", "_TRI_METER", all_or_used_features)
# all_or_used_features = gsub("CELL_TRI_MM", "CELL_TRI_METER", all_or_used_features)

aoa1 = 
  aoa(newdata = cells %>% select(all_of(all_or_used_features)),
      model = model1_rfe$fit)

cells$CELL_IS_IN_AOA = if_else(aoa1$AOA == 1, TRUE, FALSE)

cells$CELL_IS_IN_AOA %>% summary()

# note that it discards some Cargill links (~330)
```

### Map 
Map here the AoA together with all cell information statuses.  
```{r}
# Check overlaps between AOA and other categories
datasummary_crosstab(data = cells, CELL_IS_IN_AOA ~ CELL_NO_DATA)
datasummary_crosstab(data = cells, CELL_IS_IN_AOA ~ CELL_2ND_STAGE_ONLY)
datasummary_crosstab(data = cells, CELL_IS_IN_AOA ~ CELL_1ST_STAGE_ONLY)
datasummary_crosstab(data = cells, CELL_IS_IN_AOA ~ CELL_BOTH_STAGES)

cells %>% filter(CELL_BOTH_STAGES)

mean_diff = list()
for(var in all_or_used_features){
  outaoa = cells %>% filter(!CELL_IS_IN_AOA) %>% pull(var) %>% mean(na.rm = TRUE)
  inaoa  = cells %>% filter(CELL_IS_IN_AOA) %>% pull(var) %>% mean(na.rm = TRUE)
  mean_diff[[var]] = abs(outaoa - inaoa) / inaoa
  print( mean_diff[[var]] )
}

mean_diff %>% unlist() %>% sort()

toplot = 
  cells %>% 
  st_as_sf(coords = c("CELL_LONGITUDE", "CELL_LATITUDE"), crs = 4326) %>% 
  st_transform(civ_crs) %>% 
    mutate(
      `Cell information content` = case_when(
        # order matters, AoA condition should be first to overwrite "No data" or "Data for..." statuses for instance (case_when gives value corresponding to the first match).  
        !CELL_IS_IN_AOA ~ "Outside AoA of 1st stage", 
        CELL_NO_POTENTIAL_LINK ~ "No cooperative within 72 km", 
        CELL_NO_DATA ~ "No data",
        CELL_BOTH_STAGES ~ "Data for both stages",
        CELL_2ND_STAGE_ONLY ~ "Data for 2nd stage only",
        CELL_1ST_STAGE_ONLY ~ "Data for 1st stage only",
        ), 
      `Cell information content` = factor(`Cell information content`,
                                          levels = c("Data for both stages",
                                                     "Data for 2nd stage only", 
                                                     "Data for 1st stage only",
                                                     "No data", 
                                                     "Outside AoA of 1st stage", 
                                                     "No cooperative within 72 km"))
) 

toplot$`Cell information content` %>% table()    

cell_info_map = 
  ggplot(toplot) +
    geom_sf(aes(col = `Cell information content`, fill = `Cell information content`), 
            size = 0.3, shape = 22) + #, shape = 15
    scale_colour_manual(
      values = c("Data for both stages" = "purple",
                 "Data for 2nd stage only" = "blue", 
                 "Data for 1st stage only" = "darkgreen",
                 "No data" = "yellow", 
                 "Outside AoA of 1st stage" = "darkgrey",
                 "No cooperative within 72 km" = "lightgrey")
    ) +
    scale_fill_manual(
      values = c("Data for both stages" = "purple",
                 "Data for 2nd stage only" = "blue", 
                 "Data for 1st stage only" = "darkgreen",
                 "No data" = "yellow", 
                 "Outside AoA of 1st stage" = "darkgrey",
                 "No cooperative within 72 km" = "lightgrey")
    ) +
    geom_sf(data = departements, fill = "transparent", col = "black") +
    theme_bw() + 
    # theme(legend.key.size = unit(3, "cm")) + 
    guides(colour = guide_legend(override.aes = list(size=10))) +
    scale_x_continuous(breaks = xlabs, labels = paste0(xlabs,'°W')) +
    scale_y_continuous(breaks = ylabs, labels = paste0(ylabs,'°N'))
  
cell_info_map

ggsave(  
  plot = cell_info_map, 
  filename = "map_cell_info.png",
  path = here("outputs", "input_data_descriptives"), 
  width = 25, 
  height = 20,
  units = "cm")

# ggplot(toplot) +
#     geom_sf(aes(col = CELL_IS_IN_AOA), size = 1) + #, shape = 15
#     geom_sf(data = departements, fill = "transparent", col = "black")

```


## Novelty
### caret
The caret package does not have built-in functionality specifically for computing the "novelty" of samples in the same way the familiar package does. 
One alternative approach proposed by Copilot is to use distance-based methods or leverage the Mahalanobis distance to measure how far new samples are from the training data distribution.
```{r}
# Calculate Mahalanobis distance for test data
cov_matrix <- cov(stg2_development %>% select(all_of(stg2_allfeatures)))
center <- colMeans(stg2_development %>% select(all_of(stg2_allfeatures)))
links$LINK_mahalanobis_dist <- mahalanobis(links %>% select(all_of(stg2_allfeatures)), center, cov_matrix)

```


## Prediction sets

### Cells
/!/ FROM THERE WE WORK ONLY ON CELLS IN THE PREDICTION SET 
(we had kept the cells out of AOA and with no potential links until now for the purpose of doing the above map)
```{r}
# Keep cells that are sufficiently close in terms of AoA, and, 
#to reduce computation, that have no close (72km) coop, as for those we attribute null probas.  
cells_topredict = 
  cells %>% 
  filter(CELL_IS_IN_AOA & !CELL_NO_POTENTIAL_LINK)

```

### Links
```{r}

# # Predict link existence only for cells in the AOA - NO, do predict link existence there. 
# # the discarding by AoA will be applied when unconditioning second stage's probas. 
# links = 
#   links %>%
#   left_join(cells %>% select(CELL_ID, CELL_IS_IN_AOA), 
#             by = "CELL_ID")

# Should it be the under-sampled data rather here? NO, I don't think so. We DO want to make predictions for all virtual links! 
links_topredict =
  links %>% 
  filter(!CELL_NO_POTENTIAL_LINK) # CELL_IS_IN_AOA & 


# Aggregate links connecting a cell to a coop, to have one row per cell-coop pair. 
# Multiple links exist because there may be links from different producers of a cell 
# and/or to different buying stations of a coop.  

# The variables to aggregate are distance vars, which differ across both producers and BS, 
# and other BS-specific attributes 
links %>% select(starts_with("COOP_BS_")) %>% names()
links %>% select(starts_with("LINK_")) %>% names()

links_topredict = 
  links_topredict %>% 
  group_by(CELL_ID, LINK_POTENTIAL_COOP_ID) %>% 
  mutate(
    # Even if the names of the summarised variables are not rigorously correct, do not change them, 
    # for the formula and model to still pick them. 
    across(.cols = starts_with("COOP_BS_"), .fns = ~mean(.x, na.rm = TRUE)),
    LINK_TRAVEL_MINUTES  = mean(LINK_TRAVEL_MINUTES, na.rm = TRUE),  
    LINK_TRAVEL_METERS   = mean(LINK_TRAVEL_METERS, na.rm = TRUE),
    LINK_DISTANCE_METERS = mean(LINK_DISTANCE_METERS, na.rm = TRUE)
  ) %>% 
  ungroup() %>% 
  # The other variables should not vary within a cell-coop combination, and we can thus take the first occurring value. 
  distinct(CELL_ID, LINK_POTENTIAL_COOP_ID, .keep_all = TRUE)

links_topredict %>% select(starts_with("COOP_BS_")) %>% names()
links_topredict %>% select(starts_with("LINK_")) %>% names()

links_topredict_save = links_topredict
# links_summarised = 
#   links_topredict %>% 
#   summarise(.by = c(CELL_ID, LINK_POTENTIAL_COOP_ID), 
#           # Don't select all COOP_BS_ variables to save some computation for now. 
#           across(.cols = c(COOP_BS_10KM_TRI_METER, COOP_BS_10KM_SETTLEMENT_HA), .fns = ~mean(.x, na.rm = TRUE)),
#           LINK_TRAVEL_MINUTES  = mean(LINK_TRAVEL_MINUTES, na.rm = TRUE),  
#           LINK_TRAVEL_METERS   = mean(LINK_TRAVEL_METERS, na.rm = TRUE),
#           LINK_DISTANCE_METERS = mean(LINK_DISTANCE_METERS, na.rm = TRUE)
#           ) 
# nrow(links_summarised)
# links_unique = 
#   links_topredict %>% 
#   # select(starts_with("LINK_"), all_of(model2_caret_xgb$finalModel$feature_names)) %>% 
#   select(!any_of(names(links_summarised))) %>% names()
#   distinct(CELL_ID, LINK_POTENTIAL_COOP_ID, .keep_all = TRUE)
# 
# rm(links_topredict)
# 
# stopifnot(nrow(links_summarised) == nrow(links_unique))
# 
# links_topredict = 
#   inner_join(
#     links_summarised, 
#     links_unique,
#     .by = c("CELL_ID", "LINK_POTENTIAL_COOP_ID"))
#   
# stopifnot(nrow(links_topredict) == nrow(links_unique))
# links_topredict %>% 
#   distinct(CELL_ID, 
#            LINK_POTENTIAL_COOP_BS_ID, 
#            PRO_ID,
#            BUYER_LONGITUDE, BUYER_LATITUDE) %>% 
#   nrow() == nrow(links)

```



## 1st stage
### Main prediction
```{r}
# This is the point estimate from the ensemble of models 
tic()
cells_topredict$CELL_SHARE_COOP = predict(model1_rfe$fit$finalModel, data = cells_topredict)$predictions
# add $predicted_outcome if model1 is a familiar model
toc()
# From here, to shorten code, let's start a fresh naming convention for the target feature LINK_PROBA. 

cells_topredict$CELL_SHARE_COOP %>% summary()

```


### Bootstrapped predictions 
Run a prediction for each model built by a bootstrap

- Time&memory benchmark for 1st stage: 13k cells x 100 bootstraps = 1.3M dataset. Prediction with one model = 156s., i.e. ~5h for 100 models.  Même pas en fait, car apparemment c'est vachement plus rapide avec un seul modèle (il doit utiliser tous les modèles quand il prédit avec l'ensemble, logique.)
- Time&memory benchmark for 2nd stage: repeat the above on slices of the predict set, otherwise the stack will be very large (4M * n boostraps = 1.6bn obs for 400 boostraps). We could set up a foreach loop to run on slices in parallel.    

```{r}
# # this can be long, so run it only if output not available, or manually (if new model)
# if(!file.exists(here("temp_data", "predictions", "stage_1", "cells_topredict_nest.RDS"))){
#   m = 10
#   temp_stack_list = list()
#   cells_topredict_light = cells_topredict %>% select(CELL_ID)
#   
#   for(m in 1:length(indiv_model1_names)){#
#     mdl = readRDS(here(stage1_model_path, indiv_model1_names[m]))
#     
#     # cells_topredict_light[,paste0("CELL_SHARE_COOP_DRAWS",m)] = predict(mdl, newdata = cells_topredict)$predicted_outcome
#     cells_topredict_light$CELL_SHARE_COOP_DRAWS = predict(mdl, newdata = cells_topredict)$predicted_outcome
#     
#     cells_topredict_light$BOOTSTRAP_MODEL_IDX = as.integer(m)
#     
#     temp_stack_list[[m]] = cells_topredict_light
#   }
#   # From a list of 100 dataframes, make a dataframe with a column where each value is a list with 100 elements. 
#   temp_stack_df = bind_rows(temp_stack_list)
#   
#   cells_topredict_nest = 
#     temp_stack_df %>% 
#     nest(
#       CELL_SHARE_COOP_DRAWLIST = c(BOOTSTRAP_MODEL_IDX, CELL_SHARE_COOP_DRAWS),
#       .by = CELL_ID)
#   
#   saveRDS(cells_topredict_nest, here("temp_data", "predictions", "stage_1", "cells_topredict_nest.RDS"))
# } else {
#   cells_topredict_nest = readRDS(here("temp_data", "predictions", "stage_1", "cells_topredict_nest.RDS"))
# }
# 
# # Join to the rest of predict data 
# cells_topredict = 
#   cells_topredict %>% 
#   left_join(cells_topredict_nest %>% select(CELL_ID, CELL_SHARE_COOP_DRAWLIST), 
#             by = "CELL_ID")
# 
# # (Notes about the handling of nested data)
# 
# # no-match rows get an empty list (NULL, of length 0)
# cells_topredict %>% filter(lengths(CELL_SHARE_COOP_DRAWLIST) == 0) %>% select(CELL_SHARE_COOP_DRAWLIST, everything()) %>% View()
# # These empty rows are removed when unnesting, with keep_empty = FALSE (the default)
# cells_un = 
#   cells_topredict %>% 
#   unnest(cols = CELL_SHARE_COOP_DRAWLIST, 
#          keep_empty = F)
# 
# nrow(cells_un) / nrow(cells_topredict_nest)
```



### Threshold
1st stage threshold: whether the share of coop outlet is significantly different from zero
```{r}
# # Significantly, given the model error: 
# 
# # Set the prediction to 0 when it does not differ from 0 by more than the model average error. 
# # Performance metrics are stored here
# model1_rmse = model1_perf_metrics$value %>% as.numeric() %>% sqrt()
# 
# cells_topredict =
#   cells_topredict %>% 
#   mutate(
#     CELL_SHARE_COOP_GTRMSE = if_else(CELL_SHARE_COOP < model1_rmse,
#                                      0,
#                                      CELL_SHARE_COOP)
#     )
#   
# # This is when we will have a disitrution spread by the model error. 
# # In each cell, conduct a one-sided t-test that the predicted share of coop outlet is positive. 
#   # rowwise() %>% 
#   # mutate(
#   #   # CELL_SHARE_COOP_SE = sd(CELL_SHARE_COOP_DRAWLIST[["CELL_SHARE_COOP_DRAWS"]]) / sqrt(nrow(CELL_SHARE_COOP_DRAWLIST)), 
#     # 
#     # CELL_SHARE_COOP_LB = CELL_SHARE_COOP - 1.96 * CELL_SHARE_COOP_SE,
#     
#     # CELL_SHARE_COOP = if_else(CELL_SHARE_COOP_TTEST[["p.value"]] > 0.05,  
#     #                                 0, 
#     #                                 CELL_SHARE_COOP),
# 
# 
# # cells_topredict$CELL_SHARE_COOP_DRAWLIST[[1456]]$CELL_SHARE_COOP_DRAWS %>% mean()
# # cells_topredict$CELL_SHARE_COOP[1456]
# # 
# # cells_topredict$CELL_SHARE_COOP_DRAWLIST[[1456]]$CELL_SHARE_COOP_DRAWS %>% sd()/10
# # cells_topredict$CELL_SHARE_COOP_SE[1456]
# # cells_topredict$CELL_SHARE_COOP_TTEST[[1456]]$estimate  
# 
# cells_topredict$CELL_SHARE_COOP %>% summary()
# # cells_topredict$CELL_SHARE_COOP_SE %>% summary()
# # cells_topredict$CELL_SHARE_COOP_LB %>% summary()
# # cells_topredict$CELL_SHARE_COOP %>% summary()
# cells_topredict$CELL_SHARE_COOP_GTRMSE %>% summary()

```



Determinist prediction - deprecated since now the prediction set does not have these cells. 
This is just where we are sure that cocoa can be sold to a cooperative because there is no coop. 
```{r}
# cells_topredict =
#   cells_topredict %>% 
#   mutate(CELL_SHARE_COOP = if_else(CELL_NO_POTENTIAL_LINK, 
#                                           0, 
#                                           CELL_SHARE_COOP))
# 
# cells_topredict$CELL_SHARE_COOP %>% summary()

```


## 2nd stage
### Main prediction
```{r}
# Predict the existence that a link exists
tic()
links_topredict$LINK_PROBA = predict(model2_caret_xgb, 
                                     newdata = links_topredict %>% select(all_of(model2_caret_xgb$finalModel$feature_names)), 
                                     type = "prob")$Actual_links
toc()

# From here, to shorten code, let's start a fresh naming convention for the target feature LINK_PROBA. 

links_topredict$LINK_PROBA %>% summary()

```


### Threshold
2nd stage threshold: whether the (still conditional and not normalised) proba that there is a link is higher than the threshold that minimises false positives. 
NOT DOING IT ANYMORE BECAUSE NO REAL NEED FOR IT AND IT REQUIRES AN ADDITIONAL ARBITRARY CHOICE FOR THE METRIC TO USE (especially as FPR would be the most preferred one but is not available in caret::thresholder). 
```{r}
# Identify the optimal class probability threshold, and apply it to every link

# To be conservative regarding the extent of supply sheds, we have a preference for false negatives with regards to false positives.
# I.e. we consider false positives as "expensive".
# Thus, we choose the threshold based on the false positive rate (https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall)
# But this metric is not available in thresholder!! 
# Thus, we choose a threshold that maximises the performance metric about false positives: we minimise the   
# optimal_thresholds = thresholder(model2_caret_xgb, threshold = seq(0, 1, by = 0.05))
# seq(0.0001, 0.001, by = 0.0001)

# (thresh = 
#   optimal_thresholds %>% 
#   arrange(desc(F1)) %>% 
#   head(1) %>% 
#   pull(prob_threshold))
# 
# links_topredict = 
#   links_topredict %>% 
#   mutate(LINK_PROBA_THRESH = if_else(LINK_PROBA < thresh, 
#                                      0, 
#                                      LINK_PROBA)) # returns NA for NA probas (!AoA)
```


Determinist prediction - deprecated since now the prediction set does not have these cells. 
This is just where we are sure that no link can exist because there is no coop. 
```{r}
# links_topredict =
#   links_topredict %>% 
#   mutate(LINK_PROBA_THRESH = if_else(CELL_NO_POTENTIAL_LINK, 
#                               0, 
#                               LINK_PROBA_THRESH)) # it could also be 1 here, since we normalise then. But keep some variation between links_topredict with a positive proba. 
# 
# # After this, the number of links_topredict that are NA is 297074
# links_topredict$LINK_PROBA_THRESH %>% summary()
# # which is the same as the number of links_topredict in cells_topredict outside the AoA. 
# n_links_outside_aoa = nrow(filter(links_topredict, !CELL_IS_IN_AOA)) 
# 
# stopifnot(
#   links_topredict %>% filter(is.na(LINK_PROBA_THRESH)) %>% nrow() ==  n_links_outside_aoa
# )
# 
# 
# links_topredict$LINK_PROBA %>% quantile(seq(0.99, 1, 0.001), na.rm = T)
# links_topredict$LINK_PROBA_THRESH %>% quantile(seq(0.999, 1, 0.0001), na.rm = T)

```



# EXPORT 
```{r}
saveRDS(cells_topredict, 
        here("temp_data", "model", "cells_topredict.Rdata"))

saveRDS(links_topredict, 
        here("temp_data", "model", "links_topredict.Rdata"))
```


# DEPRECATED CODE   
## Understand caret
Alternative predictions
```{r}
set.seed(8888)
# Alternative predictions

model1_caret_cv = 
  train(
    x = stg1_development %>% select(all_of(all_or_used_features)), 
    y = stg1_development$CELL_PROP_VOLUME_COOPS, 
    method="rf", 
    importance=TRUE, 
    tuneLength=1,
    trControl=trainControl(method="cv",number=5,savePredictions=T)
  )
model1_caret_cv

cells_topredict$CELL_SHARE_COOP_SIMPLECV = predict(model1_caret_cv, newdata = cells_topredict)

# model1_caret_boot632 = readRDS(here("temp_data", "model", "caret", "stage_1", "allfeatures_rf_100boot632_5tuneL"))

cells_topredict$CELL_SHARE_COOP_RFE_finalModel = 
  predict(model1_rfe$fit$finalModel, data = cells_topredict %>% select(all_or_used_features))$predictions

cells_topredict$CELL_SHARE_COOP_RFE = 
  predict(model1_rfe$fit, newdata = cells_topredict %>% select(all_or_used_features))

# cells_topredict$CELL_SHARE_COOP_CARET_boot632 = predict(model1_caret_boot632, newdata = cells_topredict)
# cells_topredict$CELL_SHARE_COOP_CARET_boot632_2 = predict(model1_caret_boot632, newdata = cells_topredict %>% select(stg1_allfeatures))


# EXACTEMENT LES MM PREDICTIONS AVEC FINALMODEL OU L'OBJET BOOTSTRAP
cells_topredict$CELL_SHARE_COOP_RFE_finalModel %>% summary()
cells_topredict$CELL_SHARE_COOP_RFE %>% summary()
cells_topredict$CELL_SHARE_COOP_CARET_rfe %>% summary()
# cells_topredict$CELL_SHARE_COOP_CARET_boot632 %>% summary()
# cells_topredict$CELL_SHARE_COOP_CARET_boot632_2 %>% summary()

cor.test(cells_topredict$CELL_SHARE_COOP_RFE_finalModel,
         cells_topredict$CELL_SHARE_COOP_RFE)
# cor.test(cells_topredict$CELL_SHARE_COOP_CARET_finalModel,
#          cells_topredict$CELL_SHARE_COOP_CARET_boot632)
# cor.test(cells_topredict$CELL_SHARE_COOP_CARET_boot632,
#          cells_topredict$CELL_SHARE_COOP_CARET_rfe)

stg1_mfit = ranger(data = stg1_development, 
                   importance = 'permutation',
                   # case.weights = stg1_development$CELL_REPRESENTATIVITY_NORM_WEIGHT,
                   formula = stg1_formula_allfeatures, 
                   seed = 8888
                   )
print(stg1_mfit)
cells_topredict$CELL_SHARE_COOP_RANGER = predict(stg1_mfit, data = cells_topredict)$predictions
cells_topredict$CELL_SHARE_COOP_RANGER %>% summary()

cor.test(cells_topredict$CELL_SHARE_COOP_RANGER,
         cells_topredict$CELL_SHARE_COOP_RFE)

# Compare predictions and observation in development data 
# cells_topredict %>% 
#   filter(!is.na(CELL_PROP_VOLUME_COOPS)) %>% 
#   mutate(CELL_PROP_VOLUME_COOPS)

```



## Balance tests
Balance tests between 1st stage development and prediction data 
```{r}
# cells_features_tobalance = 
#   rbind(
#     stg1_development %>% select(CELL_ID, stg1_allfeatures), 
#     cells_topredict  %>% select(CELL_ID, stg1_allfeatures) %>% filter(!CELL_ID %in% stg1_development$CELL_ID), 
#   )
# stopifnot(cells_features_tobalance %>% filter(SPLIT == "Development set") %>% norw() == nrow(stg1_development))
# 
# (datasummary_balance(~SPLIT,
#                      data = cells_features_tobalance,
#                      stars = TRUE,
#                      # title = "Summaries and balance tests on cell data sets to development and to predict the cell share of cooperative outlet",
#                      notes = "'BS' stands for cooperative buying stations.",
#                      output = here("outputs", "input_data_descriptives", "cells_balance.png")))

```


Balance tests between 2nd stage development and prediction data 
```{r}

```


Balance tests between actual and/virtual in development. 
Alternatively, we could look at: 
- Only in actual 
- Only in all (potential)
```{r}
names(stg2_development)
stg2_features_tobalance_1 =
  stg2_development %>%
  mutate(across(where(is.logical), as.integer)) %>% 
  select(LINK_IS_ACTUAL_COOP_class, 
         LINK_DISTANCE_METERS,
         LINK_IS_WITH_1_NEAREST_COOPS,
         LINK_IS_WITH_5_NEAREST_COOPS,
         CELL_N_BS_WITHIN_DIST,
         CELL_N_COOP_IN_DPT, CELL_AVG_N_LICBUY_IN_DPT,
         CELL_COCOA_HA, CELL_SETTLEMENT_HA, CELL_IMPOSSIBLE_HA, CELL_TRI_METER)

datasummary_balance(~LINK_IS_ACTUAL_COOP_class, 
                    data = stg2_features_tobalance_1, 
                    output = here("outputs", "input_data_descriptives", paste0("stg2_",MODEL_RESOLUTION_KM,"km_features.png")))

stg2_features_tobalance_2 =
  stg2_development %>%
  mutate(across(where(is.logical), as.integer)) %>% 
  select(LINK_IS_ACTUAL_COOP_class,
         COOP_FARMERS, 
         COOP_N_KNOWN_BUYERS, COOP_N_KNOWN_BS,
         starts_with("COOP_STATUS_"), 
         COOP_CERTIFIED, COOP_RFA, COOP_UTZ, COOP_FT, 
         COOP_HAS_SSI, starts_with("COOP_SSI_"), 
         starts_with("COOP_BS_10KM_"))

datasummary_balance(~LINK_IS_ACTUAL_COOP_class, 
                    data = stg2_features_tobalance_2)

```


## FAMILIAR

### Train 1st stage
Doing feature ranking and hyper-parameter optimization
This is the model development. 

This is all handled by package familiar. 

For warm starts: 
precompute_data_assignment --> An experimentData object.
precompute_feature_info    --> An experimentData object.
precompute_vimp            --> An experimentData object.
(each does additional steps)

predict function like anywhere else. 
```{r}
# ?train_familiar
# ?theme_familiar
# ?summon_familiar
# ?as_familiar_data
# ?predict  

## For familiar: 
# Experimental design 
exp_design = "fs+mb" #bt(fs+mb,100)

# Repository for familiar 1st stage 
# summon_familiar writes all it does in a repo. It returns nothing here.  
stage1_expdes_repo = here("temp_data", "model", "familiar", "stage_1", gsub(",","_",exp_design))
dir.create(stage1_expdes_repo)

# let's go
summon_familiar(
  
  formula = stg1_formula_allfeatures,
  data = stg1_development %>% select(CELL_ID, CELL_PROP_VOLUME_COOPS, all_of(stg1_allfeatures)),
  experiment_dir = stage1_expdes_repo,
  sample_id_column = "CELL_ID",
  outcome_name = "Cooperative outlet share",
  # outcome_column = "CELL_PROP_VOLUME_COOPS",
  outcome_type = "count",
  experimental_design = exp_design, #"fs + mb", #  # "cv(bt(fs,100) + mb, 3)",
  
  # Use bt because we have many features compared to the number of observations.  "The most practical application of bt is for repeating feature selection multiple times (e.g. bt(fs,50)+mb+ev), as this allows for aggregating variable importance and reducing the effect of random selection."

  # imbalance_correction_method = "random_undersampling", # do not specify, to avoid any kind of under-sampling, since we did it in external pre-processing.  
  parallel = TRUE,
  parallel_nr_cores = detectCores() - 1, 
  
  # PRE-PROCESSING ARGS
  # (not all "none" are the defaults)
  filter_method = "none", # no feature to be filtered in pre-processing
  transformation_method = "none", 
  normalisation_method = "none",
  cluster_method = "hclust", # the default. This is categorised in data processing, but it will affect feature selection. 
  # imputation_method does not matter bc we have no NA. 
  
  # FEATURE SELECTION & OPTIMIZATION 
  fs_method = "random_forest_ranger_permutation", # see https://cran.r-project.org/web/packages/familiar/vignettes/feature_selection_precompiled.html
  # fs_method_parameter = list("random_forest_ranger_permutation" = list()), # this would need to have this format, but let it unspecified, so that feature selection optimizes on ALL hyper-parameters. 
  # vimp_aggregation_method = "borda", # borda is the default. Check guidance to depart. 
  
  learner = "random_forest_ranger",
  # hyperparameter = list("ranger" = list()), #  "If no parameters are provided, sequential model-based optimisation is used to determine optimal hyperparameters."
  
  # EVALUATION INFERENCE
  # The default is "bootstrap_confidence_interval" or "bci", so we have to tell the model to do just quick point estimates where we don't care much about inference. Especially as bci can easily be queried in post-processing.  
  skip_evaluation_elements = c("feature_expressions", "feature_similarity", 
                               "fs_vimp",  "hyperparameters", "ice_data", 
                                "permutation_vimp", "univariate_analysis"), #"model_vimp", 
  
  # part of this list elements are useless now, since they are skipped by the above. 
  estimation_type = list("prediction_data" = "bci", "model_performance"="bci", 
                         "permutation_vimp" = "point", 
                         "ice_data"= "point", 
                         "auc_data" = "point", 
                         "decision_curve_analyis" = "point"), 
  # estimation_type = list("prediction_data" = "hybrid", "model_performance"="hybrid", 
  #                        "permutation_vimp" = "ensemble", 
  #                        "ice_data"= "ensemble", 
  #                        "auc_data" = "ensemble", 
  #                        "decision_curve_analyis" = "ensemble"), 
  confidence_level = 0.95, # the default. 
  detail_level = "hybrid" # the default. See evaluation vignette. 
)

```
The results folder outputed by summon_familiar is saved manually to different place, to protect it from being overwritten in a subsequent run of the above block. 

Several things to say: 

The relative squared error being lower than 1, the model still performs better than the mean predictor. 

The most important variables, across the ensemble of models, are: 
1/ the number of (CELL_COUNT_5_NEAREST_COOP_N_KNOWN_BS); 
2/ the road distance to the closest buying station (CELL_MIN_DISTANCE_METERS);
3/ the number of RFA farmers in the 5 nearest cooperatives (CELL_COUNT_5_NEAREST_COOP_FARMERS_RFA); 
4/ the cocoa extent in a 10km buffer around all the cooperatives located within ~70km (CELL_AVG_COOP_BS_10KM_COCOA_HA); 
5/ the number of buying stations within 70km (CELL_N_BS_WITHIN_DIST);
6/ the proportion of simplified coops in the 5 nearest cooperatives (CELL_PROP_5_NEAREST_COOP_STATUS_SCOOPS); 
7/ the proportion of sustainable sourcing initiatives in the 5 nearest cooperatives (CELL_PROP_5_NEAREST_COOP_HAS_SSI);

Now, let's see whether these features contribute to increase or decrease the cell's share of cooperative outlet.  


All important features positively explain the share of coop outlet, except the distance of the closest coop (which is expected), and the number of RFA farmers (which is surprising). 

#### Finders
```{r}
# All 1st stage models have been written by familiar in 
stage1_expdes_repo

# We use one specifically, identified by: 
# ---------------------------------------------
model1_date = "20250117092735"
  # the name for 5 "20250116210833"
  # the name for 100 "20250114231822" 
  # the name for 40 "20241204190335"
# ---------------------------------------------

# Finders of familiar outputs

# Feature info - this gives generic info about the features in the model 
# feature_info = readRDS(here(stage1_expdes_repo, "20241127134237_feature_info.RDS"))

# Iterations - this gives info about the structure of the sampling applied by the experimental design. 
# iterations = readRDS(here(stage1_expdes_repo, "20241125194101_iterations.RDS"))

# familiar_data - a maxi list containing all info of a run. 
# famdat = readRDS(here(stage1_expdes_repo, "familiar_data", "20241128122859_random_forest_ranger_random_forest_ranger_permutation_1_1_ensemble_1_1_development_data.RDS"))

collection = readRDS(here(stage1_expdes_repo, "familiar_collections", "pooled_data.RDS"))

# Models - to run predictions
stage1_model_path = here(stage1_expdes_repo, 
                         "trained_models", 
                         "random_forest_ranger", 
                         "random_forest_ranger_permutation")

# Select the model/ensemble we want to work with 

stage1_ensemble_names = list.files(stage1_model_path, pattern = "ensemble")
model1_name = 
  stage1_ensemble_names %>% 
  grep(~.,  
       pattern = model1_date, 
       value = TRUE) # take the first one in case there are several
                                          
model1 = readRDS(here(stage1_model_path, model1_name))
model1

# These are the names of all individual trained ("built") models 
stage1_model_names = list.files(stage1_model_path, pattern = "model")
indiv_model1_names = 
  stage1_model_names %>% 
  grep(~.,  
       pattern = model1_date, 
       value = TRUE) # take the first one in case there are several
```


### Train 2nd stage
```{r}
stop()
# Columns with NAs
names(which(colSums(is.na(stg2_development)) > 0)) %>% intersect(stg2_allfeatures)


# Experimental design
exp_design = "cv(fs+mb, 5)"  # "bt(fs+mb,5)"

# Repository for familiar 1st stage
# summon_familiar writes all it does in a repo. It returns nothing here.
stage2_expdes_repo = here("temp_data", "model", "familiar", "stage_2", gsub(",","_",exp_design))
dir.create(stage2_expdes_repo)

# let's go
summon_familiar(

  formula = stg2_formula_allfeatures,
  data = stg2_development %>% select(LINK_ID, LINK_IS_ACTUAL_COOP_class, all_of(stg2_allfeatures)),
  experiment_dir = stage2_expdes_repo,
  sample_id_column = "LINK_ID",
  outcome_name = "Link existence",
  outcome_column = "LINK_IS_ACTUAL_COOP_class",
  outcome_type = "binomial",
  experimental_design = exp_design, #"fs + mb", #  # "cv(bt(fs,100) + mb, 3)",

  # Use bt because we have many features compared to the number of observations.  "The most practical application of bt is for repeating feature selection multiple times (e.g. bt(fs,50)+mb+ev), as this allows for aggregating variable importance and reducing the effect of random selection."

  learner = "xgboost_tree",

  # Set hyper-parameters so that feature selection and model building stages do not tune this.
  # Otherwise, training the model is too long!
  # However, do NOT specify the signature size (i.e. the number of features to include "sign_size"), to let this be optimized based on feature importance.

  # input default values in ranger
  fs_method_parameter = list("random_forest_ranger_permutation"=
                               list("n_tree" = 500,
                                     "sample_size" = 0.632,
                                     "m_try" = 0.5, # ranger's default is round(sqrt(length(stg2_allfeatures)),0),
                                      # but apparently familiar uses a share
                                     "node_size" = 10,
                                     "tree_depth" = 10 # ranger's default is 0 which means infinite. Here Inf is not accepted, so put the higher bound of familiar's default range.
                                     )),
  # Input default values in xgboost::xgb.train
  # hyperparameter = list("xgboost_tree"=
  #                         list("n_boost" = 2.7, # this means 500 trees
  #                              "learning_rate" = -1.5, # for log10 to make ~0.3, the default (eta)
  #                              "alpha" = -6, # for log10 to make 0, the default
  #                              "lambda" = 0, # for log10 to make 1, the default
  #                              "tree_depth" = 6,
  #                              "sample_size" = 0.632,
  #                              "min_child_weight" = 0.3, # for log10 -1 to make 1, the default
  #                              "gamma" = -1 # no default in xgboost, just the middle of familiar's default range
  #
  #                               )),

  # imbalance_correction_method = "random_undersampling", # do not specify, to avoid any kind of under-sampling, since we did it in external pre-processing.
  parallel = TRUE,
  parallel_nr_cores = detectCores() - 1,

  # PRE-PROCESSING ARGS
  # (not all "none" are the defaults)
  filter_method = "none", # no feature to be filtered in pre-processing
  transformation_method = "none",
  normalisation_method = "none",
  cluster_method = "hclust", # the default. This is categorised in data processing, but it will affect feature selection.
  # imputation_method does not matter bc we have no NA.

  # FEATURE SELECTION & OPTIMIZATION
  fs_method = "random_forest_ranger_permutation", # see https://cran.r-project.org/web/packages/familiar/vignettes/feature_selection_precompiled.html

  # vimp_aggregation_method = "borda", # borda is the default. Check guidance to depart.

  # hyperparameter = list("ranger" = list()), #  "If no parameters are provided, sequential model-based optimisation is used to determine optimal hyperparameters."

  # EVALUATION INFERENCE
  # The default is "bootstrap_confidence_interval" or "bci", so we have to tell the model to do just quick point estimates where we don't care much about inference. Especially as bci can easily be queried in post-processing.
  skip_evaluation_elements = c("feature_expressions", "feature_similarity",
                               "fs_vimp",  "hyperparameters", "ice_data",
                                "univariate_analysis"), #"model_vimp", "permutation_vimp",

  # part of this list elements are useless now, since they are skipped by the above.
  estimation_type = list("prediction_data" = "bci",
                         "model_performance"="bci",
                         "permutation_vimp" = "point",
                         "ice_data"= "point",
                         "auc_data" = "point",
                         "decision_curve_analyis" = "point"),
  # estimation_type = list("prediction_data" = "hybrid", "model_performance"="hybrid",
  #                        "permutation_vimp" = "ensemble",
  #                        "ice_data"= "ensemble",
  #                        "auc_data" = "ensemble",
  #                        "decision_curve_analyis" = "ensemble"),
  confidence_level = 0.95, # the default.
  detail_level = "hybrid" # the default. See evaluation vignette.
)

```

Finders
This block produces convenient finder objects to access familiar outputs. 
```{r}
## ID TO SELECT MODEL TO WORK WITH IN ALL OF BELOW SCRIPT
# ---------------------------------------------
model2_date = "20241206091347"
# ---------------------------------------------

# Finders of familiar outputs

# Feature info - this gives generic info about the features in the model
# feature_info = readRDS(here(stage2_expdes_repo, "20241127134237_feature_info.RDS"))

# Iterations - this gives info about the structure of the sampling applied by the experimental design.
# iterations = readRDS(here(stage2_expdes_repo, "20241125194101_iterations.RDS"))

# familiar_data - a maxi list containing all info of a run.
# famdat = readRDS(here(stage2_expdes_repo, "familiar_data", "20241128122859_random_forest_ranger_random_forest_ranger_permutation_1_1_ensemble_1_1_development_data.RDS"))

# Model development results - to report model performance
stage2_results = here(stage2_expdes_repo, "results", "pooled_data")

# Models - to run predictions
stage2_model_path = here(stage2_expdes_repo,
                         "trained_models",
                         "xgboost_tree",
                         "random_forest_ranger_permutation")

# Select the model/ensemble we want to work with

stage2_ensemble_names = list.files(stage2_model_path, pattern = "ensemble")
model2_name =
  stage2_ensemble_names %>%
  grep(~.,
       pattern = model2_date,
       value = TRUE) # take the first one in case there are several

model2 = readRDS(here(stage2_model_path, model2_name))
model2

```

### Performance
```{r}
# Performance metrics are stored here
model1_perf_metrics = read.csv2(here(stage1_results, "performance", "performance_metric.csv"))

model1_perf_metrics

# Normalement ces plots sont déjà exportés automatiquement par summon_familiar, mais apparemment pas pour toutes les métriques.
# This takes ~10min with a 200-models ensemble
plot_perf_model1 =
  familiar::plot_model_performance(
  object = model1,
  draw = TRUE,
  facet_by = "metric",
  data = stg1_development %>% select(CELL_ID, CELL_PROP_VOLUME_COOPS, all_of(stg1_allfeatures)),
  estimation_type = "bci",
  metric = c("mse", "rse", "r2_score")) #"rmse",  "explained_variance",

plot_perf_model1[[1]][["layers"]][[1]][["data"]]$data_set <- as.character(plot_perf_model1[[1]][["layers"]][[1]][["data"]]$data_set)

# Save plot
plot_perf_model1

ggsave(
  filename = paste0("performance_metrics.png"),
  path = here("outputs", "familiar", "stage_1", model1_date))

```

### Feature importance
```{r}
# vimp = readRDS(here(stage1_expdes_repo, "variable_importance", "20250117074207_fs_random_forest_ranger_permutation.RDS"))
# 
# vimp = vimp[[1]]
# vimp = vimp@vimp_table %>% as.data.frame()
# vimp = vimp %>% arrange(desc(score))
# vimp
# 
# importance_repo = here(stage1_results, "variable_importance")
# 
# permut_vimp = read.csv2(here(importance_repo, "variable_importance_permutation.csv"))
# 
# vimp_top10 = 
#   permut_vimp %>% 
#   filter(similarity_threshold == 1 & data_set == "development" & value !=0) %>% 
#   arrange(desc(value)) %>% 
#   pull(feature) %>% 
#   head(10)

# The plot_model_signature_occurrence method plots the occurrence of features among the first 5 ranks across an ensemble of models (if available). The rank threshold can be specified using the rank_threshold argument or the eval_aggregation_rank_threshold parameter (summon_familiar).


#Note: -------
# The summon_familiar outputs below are not yet aggregated, they are bootstrap-specific lists of ranked features used in model development.  
# stage1_vimp = here(stage1_expdes_repo, "variable_importance")
# vimp = readRDS(here(stage1_vimp, "20241127134237_fs_random_forest_ranger_permutation.RDS"))
# vimp_hp = readRDS(here(stage1_vimp, "random_forest_ranger", "20241127134237_hyperparameters_random_forest_ranger_1_1.RDS"))
```


Compare familiar to other packages in terms of predictions 
```{r}
cells_topredict = 
  cells_topredict %>% 
  mutate(CELL_SHARE_COOP_FAMIL = CELL_SHARE_COOP)

# Familiar predictions are opposite to caret's and ranger's, which are both very similar.
cor.test(cells_topredict$CELL_SHARE_COOP_FAMIL,
         cells_topredict$CELL_SHARE_COOP_CARET)
cor.test(cells_topredict$CELL_SHARE_COOP_FAMIL,
         cells_topredict$CELL_SHARE_COOP_RANGER)
cor.test(cells_topredict$CELL_SHARE_COOP_RANGER,
         cells_topredict$CELL_SHARE_COOP_CARET)

# Familiar is doing much worse on the training data ((by two orders of magnitude).
cells_topredict %>% 
  filter(CELL_IS_OBSERVED_COOPOUTLET) %>% 
  mutate(
    ERROR_FAMIL = CELL_PROP_VOLUME_COOPS - CELL_SHARE_COOP_FAMIL,
    ERROR_CARET = CELL_PROP_VOLUME_COOPS - CELL_SHARE_COOP_CARET,
    ERROR_RANGER = CELL_PROP_VOLUME_COOPS - CELL_SHARE_COOP_RANGER) %>% 
  select(starts_with("ERROR")) %>% 
  summary()
# Familiar is on average much lower than the true share, while caret and ranger are higher

cells_toplot = 
  cells_topredict %>% 
  mutate(
   # Work on PERCENTAGE, not share, from here, for display. 
    CELL_SHARE_COOP = 100*CELL_SHARE_COOP,
    CELL_COCOA_SHARE = 100*CELL_COCOA_SHARE, 
    # Round hectares
    across(.cols = ends_with("_HA"), .fns = ~round(.x, 0))
    )


```

### Explanation

Here we use Individual Conditional Expectation (ICE) and Partial Dependence (PD) plots.
All this is sketchy, and interpretations are challenging !!! 

```{r}
# These plots are readily available in the repository below. 
explanation_repo = here(stage1_results, "explanation")

# To have a global picture, look at the correlation tests (I am not sure about this!)

pd_corr_table = data.frame()
for(impvar in vimp_top10){
  pdvar = 
    read.csv2(here(explanation_repo, paste0("explanation_pd_", impvar, ".csv"))) %>% 
    filter(data_set == "development") 
  
  cortest_pd = 
    stats::cor.test(x = as.numeric(pdvar$feature_x_value),
                    y = as.numeric(pdvar$predicted_outcome))

  cortest_actual = 
    stats::cor.test(x = stg1_development %>% pull(impvar),
                    y = stg1_development$CELL_PROP_VOLUME_COOPS)

  pd_corr_table[1:2,impvar] = c(cortest_pd$estimate, cortest_pd$p.value)
  
  pd_corr_table[3:4,impvar] = c(cortest_actual$estimate, cortest_actual$p.value)
  row.names(pd_corr_table) = c("Correlation coefficient PD", "p value PD",
                               "Correlation coefficient", "p value")
}

pd_corr_table
```


### Novelty
```{r}
# Novelty in development data 
devdat_prediction_1 = read.csv2(here(stage1_results, "prediction", "prediction.csv"))
# devdat_prediction_2 = read.csv2(here(stage2_results, "prediction", "prediction.csv"))

devdat_prediction_1$novelty %>% as.numeric() %>% summary()

# It is possible to add features that were not in the model initial formula, to gauge novelty (?)
# but would it make sense (to include LU vars) here? 

# Novelty in to-predict data

cells$CELL_NOVELTY = predict(object = model1, newdata = cells, type = "novelty")

cells$CELL_NOVELTY %>% summary()

# Novelty in the Cells data is in the range of novelty in the 1st stage dev data. 
# So we don't take it further into account. 
```


## Hold-out validation 
```{r}

(test_pred_prob <- predict(stg2_mfit, 
                             new_data=stg2_test, 
                             type = "prob"))

stg2_test$LINK_PRED_ACTUAL_COOP_prob = test_pred_prob$`.pred_Actual_links`


(test_pred_class <- predict(stg2_mfit, 
                             new_data=stg2_test, 
                             type = "class")) # this implements a 50% threshold, 
# i.e., if the predicted link is more likely to exist than to not exist, the prediction is classified as "Actual link" 
stg2_test$LINK_PRED_ACTUAL_COOP_class = test_pred_class$.pred_class

stg2_test = 
  stg2_test %>% 
  mutate(TP = LINK_PRED_ACTUAL_COOP_class == "Actual_links"  & LINK_IS_ACTUAL_COOP_class == "Actual_links",
         TN = LINK_PRED_ACTUAL_COOP_class == "Virtual_links" & LINK_IS_ACTUAL_COOP_class == "Virtual_links",
         FP = LINK_PRED_ACTUAL_COOP_class == "Actual_links"  & LINK_IS_ACTUAL_COOP_class == "Virtual_links",
         FN = LINK_PRED_ACTUAL_COOP_class == "Virtual_links" & LINK_IS_ACTUAL_COOP_class == "Actual_links")

```


### Accuracy 
we can calculate the overall accuracy score as a sum of instances where predicted and actual classes match divided by the total number of rows:
```{r}

(accuracy = sum(stg2_test$LINK_PRED_ACTUAL_COOP_class == stg2_test$LINK_IS_ACTUAL_COOP_class) / nrow(stg2_test))
(accuracy = (sum(stg2_test$TP) + sum(stg2_test$TN) ) / (sum(stg2_test$TP) + sum(stg2_test$TN)  + sum(stg2_test$FP) + sum(stg2_test$FN)))

```

### True Positive Rate (Recall) 
```{r}
(recall = sum(stg2_test$TP) / (sum(stg2_test$TP) + sum(stg2_test$FN)))
```

### False Negative Rate (probability of false alarm)
```{r}
(fpr = sum(stg2_test$FP) / (sum(stg2_test$TN) + sum(stg2_test$FP)))
```

### Precision
```{r}
(precision = sum(stg2_test$TP) / (sum(stg2_test$TP) + sum(stg2_test$FP)))

```


### Confusion Matrix  
```{r}
# This requires package caret which fails to install. 
# cm = confusionMatrix(stg2_test$LINK_IS_ACTUAL_COOP_class, stg2_test$LINK_PRED_ACTUAL_COOP_class)
# 
# 
# # Plot it 
# cfm <- as_tibble(cm$table)
# plot_confusion_matrix(cfm, target_col = "Reference", prediction_col = "Prediction", counts_col = "n")

```

### PR-AUC
```{r}
prc_xgboost_test = 
  pr_curve(
    data = stg2_test, 
    truth = LINK_IS_ACTUAL_COOP_class,
    LINK_PRED_ACTUAL_COOP_prob, 
    event_level = "second"
)

(PRAUC = 
  pr_auc(
    data = stg2_test, 
    truth = LINK_IS_ACTUAL_COOP_class,
    LINK_PRED_ACTUAL_COOP_prob, 
    event_level = "second",
    estimator = "binary"
))

ggplot(prc_xgboost_test, aes(x = recall, y = precision)) +
  geom_path() +
  coord_equal() +
  theme_bw()  
  # legend(legend = c(paste0("Main model \nAUC = ",round(PRAUC, 2))))
  
```

### AUROC
```{r}
roc_xgboost_test = 
  roc(
  data = stg2_test, 
  response = LINK_IS_ACTUAL_COOP_class,
  predictor = LINK_PRED_ACTUAL_COOP_prob,
  quiet = TRUE
)

(AUC = roc_xgboost_test$auc)

plot(pROC::smooth(roc_xgboost_test), col = "blue", lwd = 1) 
legend(
  "bottomright",
  col = "blue",
  lwd = 1,
  legend = c(paste0("Main model \nAUC = ",round(AUC, 2)))
)
```


## Prediction (deprecated)
## Simple predict (deprecated)
```{r}
stg1_predict = 
  cells %>% 
  filter(!CELL_NO_POTENTIAL_LINK)


stg1_predict$CELL_PRED_PROP_VOLUME_COOPS = predict(stg1_mfit, data=stg1_predict)$predictions

stg1_predict$CELL_PRED_PROP_VOLUME_COOPS %>% summary()

cells = 
  cells %>% 
  left_join(stg1_predict %>% select(CELL_ID, CELL_PRED_PROP_VOLUME_COOPS), 
            by = "CELL_ID") %>% 
  # this left NAs in cells not in the predict set, i.e. in the development set
  mutate(CELL_PRED_PROP_VOLUME_COOPS = case_when(
    is.na(CELL_PRED_PROP_VOLUME_COOPS) ~ CELL_PROP_VOLUME_COOPS, 
    TRUE ~ CELL_PRED_PROP_VOLUME_COOPS
  ))

cells$CELL_PRED_PROP_VOLUME_COOPS %>% summary()

# at this stage, the NAs come from CELL_PROP_VOLUME_COOPS, in cells with no potential link at all 
stopifnot(cells %>% filter(is.na(CELL_PRED_PROP_VOLUME_COOPS)) %>% pull(CELL_NO_POTENTIAL_LINK) %>% all())

```

- TAKES TOO MUCH TIME, check how to improve efficiency 
```{r}
stg2_predict = links

stg2_predict$LINK_PRED_EXIST_PROB = predict(stg2_mfit, 
                                            new_data=stg2_predict, 
                                            type = "prob")$`.pred_Actual_links`
  

stg2_predict$LINK_PRED_EXIST_PROB %>% summary()

cells = 
  cells %>% 
  left_join(stg2_predict %>% select(CELL_ID, CELL_PRED_PROP_VOLUME_COOPS), 
            by = "CELL_ID") %>% 
  # this left NAs in cells not in the predict set, i.e. in the development set
  mutate(CELL_PRED_PROP_VOLUME_COOPS = case_when(
    is.na(CELL_PRED_PROP_VOLUME_COOPS) ~ CELL_PROP_VOLUME_COOPS, 
    TRUE ~ CELL_PRED_PROP_VOLUME_COOPS
  ))

cells$CELL_PRED_PROP_VOLUME_COOPS %>% summary()

# at this stage, the NAs come from CELL_PROP_VOLUME_COOPS, in cells with no potential link at all 
stopifnot(cells %>% filter(is.na(CELL_PRED_PROP_VOLUME_COOPS)) %>% pull(CELL_NO_POTENTIAL_LINK) %>% all())






```