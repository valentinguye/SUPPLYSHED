---
title: "model"
author: "Valentin"
date: "`r Sys.Date()`"
output: 
  html_document:
      self_contained: false
---
# Set up and inputs
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

library(aws.s3)
aws.signature::use_credentials()
Sys.setenv("AWS_DEFAULT_REGION" = "eu-west-1")


library(tidyverse)
library(sf)
library(readxl)
library(xlsx)
library(stringr)
library(DescTools)
library(rnaturalearth)
library(ggpubr)
library(units)
library(scales)
library(kableExtra)
library(here)
library(readstata13)
library(sjmisc)
library(terra) # put it after {raster} such that it superceeds homonym functions. 
library(exactextractr)
library(stars)
library(pals)
library(modelsummary) # necessary to load it after DescTools

# ML libraries
library(ranger)
library(parsnip)
library(xgboost)
library(pROC)
library(yardstick)

library(familiar)
# and packages required but some uses of familiar: 
library(microbenchmark)
library(glmnet)
library(fastcluster)
library(praznik)
library(power.transform)
library(isotree)
library(laGP)
library(harmonicmeanp)
library(mboost)

set.seed(8888)

# install.packages("https://cran.r-project.org/src/contrib/Archive/isotree/isotree_0.5.5.tar.gz",
#                  repos = NULL, 
#                  type = "source")

dir.create(here("temp_data", "familiar", "stage_1"))
dir.create(here("temp_data", "familiar", "stage_2"))

## Functions
# Trase palettes etc. for plots
source(here("code", "theme_trase.R"))

# load in particular the function fn_trader_to_group_names, str_trans, ... 
source(here("code", "USEFUL_STUFF_supplyshedproj.R"))

# use the projected CRS used by BNETD for their 2020 land use map. 
civ_crs <- 32630

MODEL_RESOLUTION_KM = 4
cell_area_ha =(100*MODEL_RESOLUTION_KM^2)

xlabs = c(-8, -6, -4)
ylabs = c(5, 7, 9)

## Assets

coopbsy = read.csv(file = here("temp_data/private_IC2B/IC2B_v2_coop_bs_year.csv"))
coopbs22 = coopbsy %>% filter(YEAR == 2022)

consol = readRDS(here("temp_data", "actual_links_consolidated.Rdata"))

cells = readRDS(here("temp_data", "prepared_main_dataset", paste0("cell_", MODEL_RESOLUTION_KM, "km.Rdata"))) 

links = readRDS(here("temp_data", "prepared_main_dataset", paste0("cell_links_", MODEL_RESOLUTION_KM, "km.Rdata")))

departements <- read_sf(here("input_data/s3/CIV_DEPARTEMENTS.geojson"))
departements = 
  st_transform(departements, crs = civ_crs)

cells = 
  cells %>% 
  rename(CELL_PROP_COOP_STATUS_COOPCA = `CELL_PROP_COOP_STATUS_COOP-CA`, 
         CELL_PROP_1_NEAREST_COOP_STATUS_COOPCA = `CELL_PROP_1_NEAREST_COOP_STATUS_COOP-CA`, 
         CELL_PROP_5_NEAREST_COOP_STATUS_COOPCA = `CELL_PROP_5_NEAREST_COOP_STATUS_COOP-CA`) %>% 
  # this is just for descriptives a priori
  mutate(CELL_2ND_STAGE_ONLY = CELL_ANY_ACTUAL_COOP_LINK & !CELL_VOLUME_OBSERVED, 
         CELL_NO_ACTUAL_LINK_DATA = CELL_ONLY_VIRTUAL_LINK | (!CELL_ONLY_VIRTUAL_LINK & 
                                                              CELL_ACTUAL_ONLYOTHER_LINK & 
                                                              !CELL_VOLUME_OBSERVED))

links = 
  links %>% 
  rename(COOP_STATUS_COOPCA = `COOP_STATUS_COOP-CA`,
         COOP_DISTRICT_SAN_PEDRO = `COOP_DISTRICT_SAN-PEDRO`,
         COOP_DISTRICT_GRAND_LAHOU = `COOP_DISTRICT_GRAND-LAHOU`,
         COOP_DISTRICT_YAKASSE_ATTOBROU = `COOP_DISTRICT_YAKASSE-ATTOBROU`,
         COOP_DISTRICT_ZOUAN_HOUNIEN = `COOP_DISTRICT_ZOUAN-HOUNIEN`,
         COOP_DISTRICT_KOUN_FAO = `COOP_DISTRICT_KOUN-FAO`,
         COOP_DISTRICT_GRAND_BASSAM = `COOP_DISTRICT_GRAND-BASSAM`, 
         COOP_DISTRICT_MBATTO = `COOP_DISTRICT_M'BATTO`)

# Make target a factor
links = 
  links %>% 
  mutate(LINK_IS_ACTUAL_COOP_class = factor(if_else(LINK_IS_ACTUAL_COOP, "Actual links", "Virtual links"), 
                                               levels = c("Virtual links", "Actual links"))) 

# Make COOP (and not Buying station) ID: 
links = 
  links %>% 
  mutate(LINK_ACTUAL_COOP_ID = gsub("_BS.*", "", LINK_ACTUAL_COOP_BS_ID), 
         LINK_POTENTIAL_COOP_ID = gsub("_BS.*", "", LINK_POTENTIAL_COOP_BS_ID))



```

## Observed links data 
Start from links data, where no under-sampling has been applied (so the biased sampling towards coops is still in there). 
```{r}
obs_links_des = 
  links %>% 
  filter(LINK_IS_ACTUAL) %>% 
  mutate(LINK_SOURCE = case_when(
    grepl("CARGILL", PRO_ID) ~ "Cargill data",
    grepl("SUSTAINCOCOA", PRO_ID) ~ "SC data",
    grepl("JRC", PRO_ID) ~ "JRC data",
  ))

length(unique(na.omit(obs_links_des$LINK_ID_OTHERS)))

totbl = 
  obs_links_des %>% 
  summarise(.by = LINK_SOURCE,  
    # Number of 
    # Links
    `with coops` = length(unique(na.omit(LINK_ID_COOPS))), # if_else(BUYER_IS_COOP, LINK_ID, NA)
    `with others` = length(unique(na.omit(LINK_ID_OTHERS))), 
    # Farms
    `Farms` = length(unique(na.omit(PRO_ID))), 
    `Villages` = length(unique(na.omit(PRO_VILLAGE_NAME))), 
    # Buyers
    Buyers = length(unique(na.omit(BUYER_ID))),
    `IC2B coops` = length(unique(na.omit(LINK_ACTUAL_COOP_ID))),
    `IC2B buying stations` = length(unique(na.omit(LINK_ACTUAL_COOP_BS_ID))),
    `Not coops` = length(unique(na.omit(if_else(!BUYER_IS_COOP, BUYER_ID, NA))))
    
    )


```



## Remove some cell/link categories 
```{r}
# Remove cells with no district (very few since we already removed cells outside inland territory in data preparation)
# cells %>% filter(is.na(CELL_DISTRICT_NAME)) %>% pull(SPLIT) %>% unique()
# cells %>% filter(is.na(CELL_DISTRICT_NAME)) %>% nrow()
cells = 
  cells %>% 
  # Remove the few cells that still fall in no district 
  filter(!is.na(CELL_DISTRICT_NAME)) 

  # Removing the cells where there is little cocoa (less than 1%) and lots of land where cocoa cannot expand (more than 90%
  # does not make a big difference (80 cells). So let's not remove them. It's a display issue. In terms of results, we will anyway aggregate cocoa or forest area 
  # filter(!(CELL_COCOA_HA / cell_area_ha < 0.01 & CELL_IMPOSSIBLE_HA / cell_area_ha > 0.9))

links = 
  links %>% 
  # Remove the few cells that still fall in no district 
  filter(!is.na(CELL_DISTRICT_NAME))
  

# In addition, from links, we also remove: 
links =
  links %>%
  # Remove links with non-IC2B coops or with other buyers, 
  # because they would count as FALSE on LINK_IS_ACTUAL_COOP, the target var, while not being exactly what we are after
  filter(!LINK_IS_ACTUAL_OTHER) %>% 
  # Remove empty links that only represent cells from where no coop is reachable. 
  # We are not interested in describing, learning from, or predicting in these cells. We will recollect them post-estimation
  filter(!CELL_NO_POTENTIAL_LINK)

stopifnot(links %>% 
            filter(!LINK_IS_VIRTUAL) %>% nrow() == sum(links$LINK_IS_ACTUAL_COOP))
```


# MANIPULATE ROWS

## Extract train/test set
Any processing on explanatory features' values should be made before this step so it is done once for all the data
```{r}
cells = 
  cells %>% 
  mutate(SPLIT = if_else(CELL_VOLUME_OBSERVED, "Development set", "No data")) 

stg1_traintest =
  cells %>%
  filter(SPLIT == "Development set") 

links = 
  links %>% 
  mutate(SPLIT = if_else(CELL_ACTUAL_LINK & !CELL_ACTUAL_ONLYOTHER_LINK, "Development set", "No data")) 

stg2_traintest =
  links %>%
  filter(SPLIT == "Development set") 

cells_stg2_traintest = 
  stg2_traintest %>% 
  distinct(CELL_ID, .keep_all = TRUE)
  

```

## Filter predict set
```{r}
stg1_predict = 
  cells %>% 
  filter(!CELL_NO_POTENTIAL_LINK)





```


# DESCRIPTIVES 


## Cells des. stats.

### Structure of the cell data
```{r}
# How many cells with actual links
# cells %>% 
#   select(!starts_with("CELL_PROP") & !starts_with("CELL_COUNT")) %>% 
#   datasummary_skim()

# nocat = 
#   cells %>%
#   filter(!CELL_NO_POTENTIAL_LINK & !CELL_NO_ACTUAL_LINK_DATA & !CELL_2ND_STAGE_ONLY & !CELL_VOLUME_OBSERVED) %>% 
#   View()
# cell_ids_check = nocat$CELL_ID
# links %>% filter(CELL_ID %in% cell_ids_check) %>% 
#   pull(CELL_ACTUAL_ONLYOTHER_LINK) %>% summary()

# export
(datasummary(N + Percent()  ~ 
              (`No buying station within 72 km` = (CELL_NO_POTENTIAL_LINK==T)) + 
              (`No actual link data` = (CELL_NO_ACTUAL_LINK_DATA==T)) + 
              (`Actual link data for 2nd stage only` = (CELL_2ND_STAGE_ONLY==T)) + 
              (`Actual link data for 1st stage` = (CELL_VOLUME_OBSERVED==T)) + 
              1, 
              data = cells,
              fmt = 1,
              align = "cccccc",
              output = here("outputs", paste0("cells_",MODEL_RESOLUTION_KM,"km_destat.png"))))
# Cells with actual link data for both stages have JRC or SC data on actual link existence, size and type of buyer (cooperative or other), that is representative for the whole cell. 

# ?tables::Heading

cells %>% filter(SPLIT == "Development set") %>% nrow()

```

### Map of the cell data
```{r}

# tmptoplot = only_potential_propt %>% filter(!duplicated(CELL_ID))
# tmptoplot_itm = only_potential_itmpt %>% filter(!duplicated(BUYER_LONGITUDE, BUYER_LATITUDE))
# 
# ggplot() +
#   theme_bw() +
#   geom_sf(data = tmptoplot, , size = 0.05, 
#              col = "red") +
#   geom_sf(data = tmptoplot_itm, , size = 1, 
#           col = "blue") +
#     # geom_sf(data = departements, fill = "transparent") +
#     geom_sf(data = civ_boundary, fill = "transparent") 


consol_itm_sf = 
  consol %>% 
  filter(!is.na(BUYER_LONGITUDE) & BUYER_IS_COOP) %>% # This excludes other buyers
  distinct(COOP_BS_ID, .keep_all = TRUE) %>% 
  st_as_sf(coords = c("BUYER_LONGITUDE", "BUYER_LATITUDE"), crs = 4326)

unlinked_coopbs = 
  coopbs22 %>% 
  filter() %>% 
  filter(!is.na(LONGITUDE) & !COOP_BS_ID %in% consol_itm_sf$COOP_BS_ID) %>% # This excludes coops already in observed links data
  distinct(COOP_BS_ID, .keep_all = TRUE) %>% 
  st_as_sf(coords = c("LONGITUDE", "LATITUDE"), crs = 4326)


toplot = 
  cells %>% 
  st_as_sf(coords = c("CELL_LONGITUDE", "CELL_LATITUDE"), crs = 4326) %>% 
  st_transform(civ_crs) %>% 
    mutate(
      `Cell information content` = case_when(
        CELL_NO_POTENTIAL_LINK ~ "No buying station within 72 km", 
        CELL_NO_ACTUAL_LINK_DATA ~ "No actual link data",
        CELL_VOLUME_OBSERVED ~ "Actual link data for 1st stage",
        CELL_2ND_STAGE_ONLY ~ "Actual link data for second stage only",
      )) 
toplot$`Cell information content` %>% table()    


ggplot(toplot) +
    geom_sf(aes(col = `Cell information content`), size = 1) + #, shape = 15
    geom_sf(data = departements, fill = "transparent", col = "black") +
    # scale_colour_brewer(na.value = NULL,
    #                     type = "qual",
    #                     direction = -1,
    #                     palette = "Set3") +
  scale_colour_manual(
    values = c("orange" , "#8B0000", "lightgrey", "darkgrey")
  ) +
  geom_sf(data = consol_itm_sf, size = 1,
          col = "black") +
  geom_sf(data = unlinked_coopbs, size = 0.1,
          col = "black") +
  # scale_size_binned(
  #   values = c(1, 0.1),
  #   labels = c("Coops in observed links data", "Other coops")
  # ) +
  # labs(fill = "Extent of the Ivorian cocoa growing region") + 
    theme_minimal() + 
  theme(legend.key.size = unit(1, "cm")) +
    scale_x_continuous(breaks = xlabs, labels = paste0(xlabs,'°W')) +
    scale_y_continuous(breaks = ylabs, labels = paste0(ylabs,'°N'))
    


```


### Cell data features
In the train/test set 
```{r}
# names(cells)

cells_traintest_features =
  cells %>% 
  select(Sample = SPLIT, 
         `Cooperative outlet share` = CELL_PROP_VOLUME_COOPS, 
         `Cocoa output from cell (kg)` = CELL_VOLUME_KG,
         `# BS within 72km` = CELL_N_BS_WITHIN_DIST, 
         `Avg. distance of the 5 nearest BS (m)` = CELL_AVG_DISTANCE_METERS_5_NEAREST_COOPS, 
         `# registered licensed buyers in department` = CELL_AVG_N_LICBUY_IN_DPT, 
          `Dense forest extent (ha)` = CELL_DENSEFOREST_HA,
          `other forests extent (ha)` = CELL_OTHERFORESTS_HA,
          # `Cocoa extent (ha)` = CELL_COCOA_HA,
          `Coffee extent (ha)` = CELL_COFFEE_HA,
          `Rubber extent (ha)` = CELL_RUBBER_HA,
          `Palm extent (ha)` = CELL_PALM_HA,
          `Coconut extent (ha)` = CELL_COCONUT_HA,
          `Cashew extent (ha)` = CELL_CASHEW_HA,
          `Other agricultural extent (ha)` = CELL_OTHERAG_HA,
          `Settlements extent (ha)` = CELL_SETTLEMENT_HA,
          `Water, rock or infrastructure extent (ha)` = CELL_IMPOSSIBLE_HA,
         `Terrain Ruggedness Index (mm)` = CELL_TRI_MM,
         `Proportion of 5 nearest cooperatives certified` = CELL_PROP_5_NEAREST_COOP_CERTIFIED,
         `Proportion of 5 nearest cooperatives with SSIs` = CELL_PROP_5_NEAREST_COOP_HAS_SSI,
         `Proportion of 5 nearest cooperatives with CCP` = CELL_PROP_5_NEAREST_COOP_SSI_CARGILL,
         `Proportion of 5 nearest cooperatives being COOP-CA` = CELL_PROP_5_NEAREST_COOP_STATUS_SCOOPS,
         `Proportion of 5 nearest cooperatives being SCOOPS` = `CELL_PROP_5_NEAREST_COOP_STATUS_COOPCA`,
         `Total # members of 5 nearest cooperatives` = CELL_COUNT_5_NEAREST_COOP_FARMERS,
         `Total # BS of 5 nearest cooperatives` = CELL_COUNT_5_NEAREST_COOP_N_KNOWN_BS,
         `Avg. # buying companies in 5 nearest cooperatives` = CELL_AVG_5_NEAREST_COOP_N_KNOWN_BUYERS,
         `Avg. TRI around 5 nearest cooperatives (mm)` = CELL_AVG_5_NEAREST_COOP_BS_10KM_TRI,
         `Avg. cocoa extent around 5 nearest cooperatives (ha)` = CELL_AVG_5_NEAREST_COOP_BS_10KM_COCOA_HA,
         `Avg. settlements extent around 5 nearest cooperatives` = CELL_AVG_5_NEAREST_COOP_BS_10KM_SETTLEMENT_HA
         ) %>% 
    filter(Sample == "Development set")


(datasummary_skim(data = cells_traintest_features, 
                 output = here("outputs", paste0("cells_",MODEL_RESOLUTION_KM,"km_train_features.html"))))

```

Cell data balance tests
```{r}
# (datasummary_balance(~SPLIT, 
#                       data = cells_features_tobalance, 
#                       stars = TRUE,
#                       # title = "Summaries and balance tests on cell data sets to train/test and to predict the cell share of cooperative outlet", 
#                       notes = "'BS' stands for cooperative buying stations.", 
#                      output = here("outputs", "cells_balance.png")))        
```


### LU in 2nd stage cells 
```{r}
(cells_stg2_traintest$CELL_COCOA_HA/cell_area_ha) %>% summary()
  


```


## Links descriptive statistics

### Features in the Potential-links data  
With differences between actual and/virtual in train/test? 

(Alternatively, we could look at: 
- Only in actual 
- Between train/test and predict sets)

```{r}
names(stg2_traintest)
stg2_features_tobalance_1 =
  stg2_traintest %>%
  mutate(across(where(is.logical), as.integer)) %>% 
  select(LINK_IS_ACTUAL_COOP_class, 
         LINK_DISTANCE_METERS,
         LINK_IS_WITH_1_NEAREST_COOPS,
         LINK_IS_WITH_5_NEAREST_COOPS,
         CELL_N_BS_WITHIN_DIST,
         CELL_N_COOP_IN_DPT, CELL_AVG_N_LICBUY_IN_DPT,
         CELL_COCOA_HA, CELL_SETTLEMENT_HA, CELL_IMPOSSIBLE_HA, CELL_TRI_MM)

datasummary_balance(~LINK_IS_ACTUAL_COOP_class, 
                    data = stg2_features_tobalance_1, 
                    output = here("outputs", paste0("stg2_",MODEL_RESOLUTION_KM,"km_features.png")))

stg2_features_tobalance_2 =
  stg2_traintest %>%
  mutate(across(where(is.logical), as.integer)) %>% 
  select(LINK_IS_ACTUAL_COOP_class,
         COOP_FARMERS, 
         COOP_N_KNOWN_BUYERS, COOP_N_KNOWN_BS,
         starts_with("COOP_STATUS_"), 
         COOP_CERTIFIED, COOP_RFA, COOP_UTZ, COOP_FT, 
         COOP_HAS_SSI, starts_with("COOP_SSI_"), 
         starts_with("COOP_BS_10KM_"))

datasummary_balance(~LINK_IS_ACTUAL_COOP_class, 
                    data = stg2_features_tobalance_2)

```



# FIRST STAGE 

## Data and formula

Resources on data split here: https://encord.com/blog/train-val-test-split/
```{r}
# Here, we do not do holdout method, so we do not split data into train and test sets. 
anyNA(stg1_traintest)


# names(cells)
# anyNA(cells)
stg1_allfeatures = 
  cells %>% 
  select(
    #CELL_COCOA_HA,
    # CELL_DENSEFOREST_HA
    # CELL_OTHERFORESTS_HA
    # CELL_COFFEE_HA,
    # CELL_RUBBER_HA,
    # CELL_PALM_HA,
    # CELL_COCONUT_HA,
    # CELL_CASHEW_HA,
    # CELL_OTHERAG_HA,
    # CELL_SETTLEMENT_HA,
    # CELL_IMPOSSIBLE_HA,
    # CELL_SETTLEMENT_HA,
    CELL_TRI_MM,
    CELL_N_BS_WITHIN_DIST, CELL_N_COOP_IN_DPT, CELL_MIN_DISTANCE_METERS,
    # Include coop-level features summarised at the 1 nearest, the 5 nearest and the whole-coop levels. 
    # This increases the simple fit performance from an OOB R2 of .17 (with only summaries at 5 nearest coop-level) to .24
    starts_with("CELL_PROP_"), -CELL_PROP_VOLUME_COOPS, -CELL_PROP_VOLUME_OTHERS,
    starts_with("CELL_AVG_"), CELL_AVG_N_LICBUY_IN_DPT, CELL_AVG_DISTANCE_METERS_5_NEAREST_COOPS,
    # -ends_with("_HA"),
    starts_with("CELL_COUNT_")) %>%
  names()  
  
stg1_mformula_allfeatures = as.formula(paste0("CELL_PROP_VOLUME_COOPS ~ ", paste0(stg1_allfeatures, collapse = " + ")))

```
The first stage has `r length(stg1_allfeatures)` potential features.  

## Simple fit 
This is simple because it does not do feature selection nor optimize hyper-parameters.
```{r}
stg1_mfit = ranger(data = stg1_traintest, 
                   importance = 'permutation',
                   case.weights = "CELL_REPRESENTATIVITY_STD_WEIGHT",
                   formula = stg1_mformula_allfeatures, 
                   seed = 8888
                   )
print(stg1_mfit)

```


## Feature importance and optimization
This is the model development. 

This is all handled by package familiar. 

For warm starts: 
precompute_data_assignment --> An experimentData object.
precompute_feature_info    --> An experimentData object.
precompute_vimp            --> An experimentData object.
(each does additional steps)

predict function like anywhere else. 
```{r}
# ?train_familiar
# ?theme_familiar
# ?summon_familiar
# ?as_familiar_data
# ?predict  

anyNA(stg1_traintest)


summon_familiar(
  
  formula = stg1_mformula_allfeatures,
  data = stg1_traintest,
  experiment_dir = here("temp_data", "familiar", "stage_1"),
  sample_id_column = "CELL_ID",
  outcome_name = "Cooperative outlet share",
  outcome_column = "CELL_PROP_VOLUME_COOPS",
  outcome_type = "continuous",
  experimental_design = "bt(fs + mb, 400)", # "cv(bt(fs,100) + mb, 3)",
  
  # Use bt because we have many features compared to the number of observations.  "The most practical application of bt is for repeating feature selection multiple times (e.g. bt(fs,50)+mb+ev), as this allows for aggregating variable importance and reducing the effect of random selection."

  # imbalance_correction_method = "random_undersampling", # do not specify, to avoid any kind of under-sampling, since we did it in external pre-processing.  
  parallel = TRUE, # The default is TRUE, but unnecessary in 1st stage at least. Actually it IS! 
  parallel_nr_cores = detectCores() - 2, # 2 is the default, and UCLouvain machine has 8, in case we need to increase. 
  
  # PRE-PROCESSING ARGS
  # (not all "none" are the defaults)
  filter_method = "none", # no feature to be filtered in pre-processing
  transformation_method = "none", 
  normalisation_method = "none",
  cluster_method = "hclust", # the default. This is categorised in data processing, but it will affect feature selection. 
  # imputation_method does not matter bc we have no NA. 
  
  
  # FEATURE SELECTION & OPTIMIZATION 
  fs_method = "random_forest_ranger_permutation", # see https://cran.r-project.org/web/packages/familiar/vignettes/feature_selection_precompiled.html
  # fs_method_parameter = list("random_forest_ranger_permutation" = list()), # this would need to have this format, but let it unspecified, so that feature selection optimizes on ALL hyper-parameters. 
  # vimp_aggregation_method = "borda", # borda is the default. Check guidance to depart. 
  
  learner = "random_forest_ranger",
  # hyperparameter = list("ranger" = list()), #  "If no parameters are provided, sequential model-based optimisation is used to determine optimal hyperparameters."
  
  # EVALUATION INFERENCE
  detail_level = "hybrid", # the default. See ?summon_familiar and  https://cran.r-project.org/web/packages/familiar/vignettes/evaluation_and_explanation_precompiled.html
  estimation_type = list("auc_data"="bci", "model_performance"="point"), #"point", # "bootstrap_confidence_interval", # the default. Requires several point estimates. 
  # The kind of metric for which we want a CI can be set by providing a list instead of character, e.g.: list("auc_data"="bci", "model_performance"="point")
  confidence_level = 0.95 # the default. 
)


```




## Predict 
```{r}
stg1_predict$CELL_PRED_PROP_VOLUME_COOPS = predict(stg1_mfit, data=stg1_predict)$predictions

stg1_predict$CELL_PRED_PROP_VOLUME_COOPS %>% summary()

cells = 
  cells %>% 
  left_join(stg1_predict %>% select(CELL_ID, CELL_PRED_PROP_VOLUME_COOPS), 
            by = "CELL_ID") %>% 
  # this left NAs in cells not in the predict set, i.e. in the train/test set
  mutate(CELL_PRED_PROP_VOLUME_COOPS = case_when(
    is.na(CELL_PRED_PROP_VOLUME_COOPS) ~ CELL_PROP_VOLUME_COOPS, 
    TRUE ~ CELL_PRED_PROP_VOLUME_COOPS
  ))

cells$CELL_PRED_PROP_VOLUME_COOPS %>% summary()

# at this stage, the NAs come from CELL_PROP_VOLUME_COOPS, in cells with no potential link at all 
stopifnot(cells %>% filter(is.na(CELL_PRED_PROP_VOLUME_COOPS)) %>% pull(CELL_NO_POTENTIAL_LINK) %>% all())

```


# SECOND STAGE 

## Apply sub-sampling
```{r}
links_ss = 
  links %>% 
  filter(!LINK_POSSIBLE_FALSENEG) %>% 
  filter(LINK_TO_KEEP_TO_US_VIRTUAL) # this is only TRUE currently. 
```



## Data split
Resources on data split here: https://encord.com/blog/train-val-test-split/
```{r}
training_size <- 0.8

# Split the data
stg2_traintest =
  links_ss %>%
  filter(SPLIT == "Development set") 

stg2_training =
  stg2_traintest %>%
  slice_sample(prop = training_size) 

stg2_test =
  stg2_traintest %>%
  filter(!LINK_ID %in% stg2_training$LINK_ID) 


stg2_predict = links

```


## Feature selection
Use Boruta here https://cran.r-project.org/web/packages/Boruta/Boruta.pdf
```{r}
# names(links_ss)
anyNA(links_ss)

stg2_allfeatures = 
  links_ss %>% 
  select(
    ends_with("_HA"), CELL_TRI_MM, 
    CELL_N_BS_WITHIN_DIST, # CELL_N_COOP_IN_DPT, CELL_AVG_N_LICBUY_IN_DPT,
    # CELL_AVG_DISTANCE_METERS_5_NEAREST_COOPS, CELL_MIN_DISTANCE_METERS,
    LINK_DISTANCE_METERS,
    starts_with("COOP_FARMERS"), 
    COOP_N_KNOWN_BUYERS, COOP_N_KNOWN_BS,
    COOP_RFA, COOP_UTZ, COOP_FT,
    starts_with("COOP_CERTIFIED"), 
    COOP_HAS_SSI, 
    starts_with("COOP_SSI_"), 
    starts_with("COOP_DISTRICT_"), -COOP_DISTRICT_NAME,
    starts_with("COOP_STATUS_"), 
    starts_with("COOP_BS_10KM")
    ) %>% 
  names() 
  
grep("'", stg2_allfeatures, value = T)

stg2_mformula_allfeatures = as.formula(paste0("LINK_IS_ACTUAL_COOP_class ~ ", paste0(stg2_allfeatures, collapse = " + ")))


```

## Model fit 
```{r}

stg2_mfit =
  boost_tree(
    mode = "classification",
    mtry = 6,
    trees = 5,
    min_n = 5
  ) %>%
  fit(data = stg2_training,
      formula = stg2_mformula_allfeatures)

# stg2mfit = 
#   xgboost(
#   label = stg2_training %>% select(LINK_IS_ACTUAL_COOP_class) %>% as.matrix(), 
#   data = stg2_training %>% select(-LINK_IS_ACTUAL_COOP_class), 
#   nrounds = 6
# )


```

## Evaluation 
```{r}
(test_pred_prob <- predict(stg2_mfit, 
                             new_data=stg2_test, 
                             type = "prob"))

stg2_test$LINK_PRED_ACTUAL_COOP_prob = test_pred_prob$`.pred_Actual links`


(test_pred_class <- predict(stg2_mfit, 
                             new_data=stg2_test, 
                             type = "class")) # this implements a 50% threshold, 
# i.e., if the predicted link is more likely to exist than to not exist, the prediction is classified as "Actual link" 
stg2_test$LINK_PRED_ACTUAL_COOP_class = test_pred_class$.pred_class

stg2_test = 
  stg2_test %>% 
  mutate(TP = LINK_PRED_ACTUAL_COOP_class == "Actual links"  & LINK_IS_ACTUAL_COOP_class == "Actual links",
         TN = LINK_PRED_ACTUAL_COOP_class == "Virtual links" & LINK_IS_ACTUAL_COOP_class == "Virtual links",
         FP = LINK_PRED_ACTUAL_COOP_class == "Actual links"  & LINK_IS_ACTUAL_COOP_class == "Virtual links",
         FN = LINK_PRED_ACTUAL_COOP_class == "Virtual links" & LINK_IS_ACTUAL_COOP_class == "Actual links")


```

### Accuracy 
we can calculate the overall accuracy score as a sum of instances where predicted and actual classes match divided by the total number of rows:
```{r}

(accuracy = sum(stg2_test$LINK_PRED_ACTUAL_COOP_class == stg2_test$LINK_IS_ACTUAL_COOP_class) / nrow(stg2_test))
(accuracy = (sum(stg2_test$TP) + sum(stg2_test$TN) ) / (sum(stg2_test$TP) + sum(stg2_test$TN)  + sum(stg2_test$FP) + sum(stg2_test$FN)))

```

### True Positive Rate (Recall) 
```{r}
(recall = sum(stg2_test$TP) / (sum(stg2_test$TP) + sum(stg2_test$FN)))
```

### False Negative Rate (probability of false alarm)
```{r}
(fpr = sum(stg2_test$FP) / (sum(stg2_test$TN) + sum(stg2_test$FP)))
```

### Precision
```{r}
(precision = sum(stg2_test$TP) / (sum(stg2_test$TP) + sum(stg2_test$FP)))

```


### Confusion Matrix  
```{r}
# This requires package caret which fails to install. 
# cm = confusionMatrix(stg2_test$LINK_IS_ACTUAL_COOP_class, stg2_test$LINK_PRED_ACTUAL_COOP_class)
# 
# 
# # Plot it 
# cfm <- as_tibble(cm$table)
# plot_confusion_matrix(cfm, target_col = "Reference", prediction_col = "Prediction", counts_col = "n")

```

### PR-CURVE
```{r}
prc_xgboost_test = 
  pr_curve(
    data = stg2_test, 
    truth = LINK_IS_ACTUAL_COOP_class,
    LINK_PRED_ACTUAL_COOP_prob, 
    event_level = "second"
)

(PRAUC = 
  pr_auc(
    data = stg2_test, 
    truth = LINK_IS_ACTUAL_COOP_class,
    LINK_PRED_ACTUAL_COOP_prob, 
    event_level = "second",
    estimator = "binary"
))

ggplot(prc_xgboost_test, aes(x = recall, y = precision)) +
  geom_path() +
  coord_equal() +
  theme_bw()  
  # legend(legend = c(paste0("Main model \nAUC = ",round(PRAUC, 2))))
  
```

### AUROC
```{r}
roc_xgboost_test = 
  roc(
  data = stg2_test, 
  response = LINK_IS_ACTUAL_COOP_class,
  predictor = LINK_PRED_ACTUAL_COOP_prob,
  quiet = TRUE
)

(AUC = roc_xgboost_test$auc)

plot(pROC::smooth(roc_xgboost_test), col = "blue", lwd = 1) 
legend(
  "bottomright",
  col = "blue",
  lwd = 1,
  legend = c(paste0("Main model \nAUC = ",round(AUC, 2)))
)
```


## Predict
- TAKES TOO MUCH TIME, check how to improve efficiency 
```{r}
stg2_predict$LINK_PRED_EXIST_PROB = predict(stg2_mfit, 
                                            new_data=stg2_predict, 
                                            type = "prob")$`.pred_Actual links`
  

stg2_predict$LINK_PRED_EXIST_PROB %>% summary()

cells = 
  cells %>% 
  left_join(stg2_predict %>% select(CELL_ID, CELL_PRED_PROP_VOLUME_COOPS), 
            by = "CELL_ID") %>% 
  # this left NAs in cells not in the predict set, i.e. in the train/test set
  mutate(CELL_PRED_PROP_VOLUME_COOPS = case_when(
    is.na(CELL_PRED_PROP_VOLUME_COOPS) ~ CELL_PROP_VOLUME_COOPS, 
    TRUE ~ CELL_PRED_PROP_VOLUME_COOPS
  ))

cells$CELL_PRED_PROP_VOLUME_COOPS %>% summary()

# at this stage, the NAs come from CELL_PROP_VOLUME_COOPS, in cells with no potential link at all 
stopifnot(cells %>% filter(is.na(CELL_PRED_PROP_VOLUME_COOPS)) %>% pull(CELL_NO_POTENTIAL_LINK) %>% all())






```