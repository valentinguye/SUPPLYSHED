---
title: "model"
author: "Valentin"
date: "`r Sys.Date()`"
output: 
  html_document:
      self_contained: false
---
# Set up and inputs
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

library(aws.s3)
aws.signature::use_credentials()
Sys.setenv("AWS_DEFAULT_REGION" = "eu-west-1")


library(tidyverse)
library(sf)
library(readxl)
library(xlsx)
library(stringr)
library(DescTools)
library(rnaturalearth)
library(ggpubr)
library(units)
library(scales)
library(kableExtra)
library(here)
library(tictoc)
library(readstata13)
library(sjmisc)
library(terra) # put it after {raster} such that it superceeds homonym functions. 
library(exactextractr)
library(stars)
library(pals)
library(modelsummary) # necessary to load it after DescTools

# ML libraries
library(ranger)
library(parsnip)
library(xgboost)
library(pROC)
library(yardstick)
library(CAST)
library(caret)

library(familiar)

# and packages required but some uses of familiar: 
library(microbenchmark)
library(glmnet)
library(fastcluster)
library(praznik)
library(power.transform)
library(isotree)
library(laGP)
library(harmonicmeanp)
library(mboost)

set.seed(8888)

# install.packages("https://cran.r-project.org/src/contrib/Archive/isotree/isotree_0.5.5.tar.gz",
#                  repos = NULL, 
#                  type = "source")

dir.create(here("temp_data", "model", "caret", "stage_1"), recursive = TRUE)
dir.create(here("temp_data", "model", "caret", "stage_2"), recursive = TRUE)

# These are were summon_familiar will write its outputs
dir.create(here("temp_data", "model", "familiar", "stage_1"), recursive = TRUE)
dir.create(here("temp_data", "model", "familiar", "stage_2"), recursive = TRUE)

# And to save the predictions 
dir.create(here("temp_data", "model", "predictions", "stage_1"), recursive = TRUE)
dir.create(here("temp_data", "model", "predictions", "stage_2"), recursive = TRUE)

# This is to save some of the outputs from being overwritten subsequently (not really used currently). 
dir.create(here("outputs", "familiar", "stage_1", "performance"), recursive = TRUE)
dir.create(here("outputs", "familiar", "stage_1", "explanation_vimp"), recursive = TRUE)
dir.create(here("outputs", "familiar", "stage_1", "models"), recursive = TRUE)
dir.create(here("outputs", "familiar", "stage_2", "performance"), recursive = TRUE)
dir.create(here("outputs", "familiar", "stage_2", "explanation_vimp"), recursive = TRUE)
dir.create(here("outputs", "familiar", "stage_2", "models"), recursive = TRUE)

## Functions
# Trase palettes etc. for plots
source(here("code", "theme_trase.R"))

# load in particular the function fn_trader_to_group_names, str_trans, ... 
source(here("code", "USEFUL_STUFF_supplyshedproj.R"))

# use the projected CRS used by BNETD for their 2020 land use map. 
civ_crs <- 32630

MODEL_RESOLUTION_KM = 4
cell_area_ha =(100*MODEL_RESOLUTION_KM^2)

xlabs = c(-8, -6, -4)
ylabs = c(5, 7, 9)

## Assets

coopbsy = read.csv(file = here("temp_data/private_IC2B/IC2B_v2_coop_bs_year.csv"))
coopbs22 = coopbsy %>% filter(YEAR == 2022)

consol = readRDS(here("temp_data", "actual_links_consolidated.Rdata"))

cells = readRDS(here("temp_data", "prepared_main_dataset", paste0("cell_", MODEL_RESOLUTION_KM, "km.Rdata"))) 

links = readRDS(here("temp_data", "prepared_main_dataset", paste0("cell_links_", MODEL_RESOLUTION_KM, "km.Rdata")))

departements <- read_sf(here("input_data/s3/CIV_DEPARTEMENTS.geojson"))
departements = 
  st_transform(departements, crs = civ_crs)

```

# Useful pre-ops
## Columns ops
```{r}
cells = 
  cells %>% 
  rename(CELL_PROP_COOP_STATUS_COOPCA = `CELL_PROP_COOP_STATUS_COOP-CA`, 
         CELL_PROP_1_NEAREST_COOP_STATUS_COOPCA = `CELL_PROP_1_NEAREST_COOP_STATUS_COOP-CA`, 
         CELL_PROP_5_NEAREST_COOP_STATUS_COOPCA = `CELL_PROP_5_NEAREST_COOP_STATUS_COOP-CA`) %>% 
  # this is just for descriptives a priori
  mutate(CELL_2ND_STAGE_ONLY = CELL_ANY_ACTUAL_COOP_LINK & !CELL_VOLUME_OBSERVED, 
         CELL_NO_ACTUAL_LINK_DATA = CELL_ONLY_VIRTUAL_LINK | (!CELL_ONLY_VIRTUAL_LINK & 
                                                              CELL_ACTUAL_ONLYOTHER_LINK & 
                                                              !CELL_VOLUME_OBSERVED))

links = 
  links %>% 
  rename(COOP_STATUS_COOPCA = `COOP_STATUS_COOP-CA`,
         COOP_DISTRICT_SAN_PEDRO = `COOP_DISTRICT_SAN-PEDRO`,
         COOP_DISTRICT_GRAND_LAHOU = `COOP_DISTRICT_GRAND-LAHOU`,
         COOP_DISTRICT_YAKASSE_ATTOBROU = `COOP_DISTRICT_YAKASSE-ATTOBROU`,
         COOP_DISTRICT_ZOUAN_HOUNIEN = `COOP_DISTRICT_ZOUAN-HOUNIEN`,
         COOP_DISTRICT_KOUN_FAO = `COOP_DISTRICT_KOUN-FAO`,
         COOP_DISTRICT_GRAND_BASSAM = `COOP_DISTRICT_GRAND-BASSAM`, 
         COOP_DISTRICT_MBATTO = `COOP_DISTRICT_M'BATTO`)

# Make target a factor
links = 
  links %>% 
  mutate(LINK_IS_ACTUAL_COOP_class = factor(if_else(LINK_IS_ACTUAL_COOP, "Actual_links", "Virtual_links"), 
                                               levels = c("Virtual_links", "Actual_links"))) 

# Make COOP (and not Buying station) ID: 
links = 
  links %>% 
  mutate(LINK_ACTUAL_COOP_ID = gsub("_BS.*", "", LINK_ACTUAL_COOP_BS_ID), 
         LINK_POTENTIAL_COOP_ID = gsub("_BS.*", "", LINK_POTENTIAL_COOP_BS_ID))

```

### Productive and prospective cells
```{r}
# Distribution of cocoa extent in cells where we know there are cocoa farms 
cells_actual = 
  cells %>% 
  filter(CELL_ACTUAL_LINK) 

cells_actual$CELL_COCOA_HA %>% summary() # It goes as low as 2 ha, but has on average 443 ha. 
cells_actual$CELL_COCOA_HA %>%quantile(probs = seq(0.01,0.1,0.01))

# 5% of the cells with actual links is a good threshold, because the cocoa area quantile corresponds to 5% of a cell area
# (i.e., 80ha, since one cell is 1600ha)

cells = 
  cells %>% 
  mutate(CELL_PRODUCTION_STATUS = if_else(CELL_COCOA_HA < 80, 
                                          "PROSPECTIVE", 
                                          "PRODUCTIVE"),
         # also compute the share of area under cocoa use, to use as weights later on
         CELL_COCOA_SHARE = CELL_COCOA_HA / 1600, 
         # and of impossible land
         CELL_IMPOSSIBLE_SHARE = CELL_IMPOSSIBLE_HA / 1600 
         )
table(cells$CELL_PRODUCTION_STATUS)

links = 
  links %>% 
  left_join(
    cells %>% select(CELL_PRODUCTION_STATUS, CELL_COCOA_SHARE, CELL_IMPOSSIBLE_SHARE, CELL_ID), 
    by = "CELL_ID")

```



## Remove cell & link instances
### Based on LU
```{r}
# Remove cells with virtually no cocoa and no space for cocoa to grow 
summary(cells$CELL_COCOA_HA)
summary(cells$CELL_IMPOSSIBLE_HA)

# Very few cells have more than 80% of their area impossible for cocoa expansion. 
# Consequently, most cells are possibly of interest to our study, either because they have cocoa, or because 
# they have some space for cocoa to expand. 
cells = 
  cells %>%  
  filter(!(CELL_COCOA_SHARE < 0.01 & CELL_IMPOSSIBLE_SHARE > 0.8))

links = 
  links %>% 
  filter(!(CELL_COCOA_SHARE < 0.01 & CELL_IMPOSSIBLE_SHARE > 0.8))
```

### That make no sense
```{r}
# Remove cells with no district (very few since we already removed cells outside inland territory in data preparation)
# cells %>% filter(is.na(CELL_DISTRICT_NAME)) %>% pull(SPLIT) %>% unique()
# cells %>% filter(is.na(CELL_DISTRICT_NAME)) %>% nrow()
cells = 
  cells %>% 
  # Remove the few cells that still fall in no district 
  filter(!is.na(CELL_DISTRICT_NAME)) 

  # Removing the cells where there is little cocoa (less than 1%) and lots of land where cocoa cannot expand (more than 90%
  # does not make a big difference (80 cells). So let's not remove them. It's a display issue. In terms of results, we will anyway aggregate cocoa or forest area 
  # filter(!(CELL_COCOA_HA / cell_area_ha < 0.01 & CELL_IMPOSSIBLE_HA / cell_area_ha > 0.9))

links = 
  links %>% 
  # Remove the few cells that still fall in no district 
  filter(!is.na(CELL_DISTRICT_NAME))
  

# In addition, from links, we also remove: 
links =
  links %>%
  # Remove links with non-IC2B coops or with other buyers, 
  # because they would count as FALSE on LINK_IS_ACTUAL_COOP, the target var, while not being exactly what we are after
  filter(!LINK_IS_ACTUAL_OTHER) %>% 
  # Remove empty links that only represent cells from where no coop is reachable. 
  # We are not interested in describing, learning from, or predicting in these cells. We will recollect them post-estimation
  filter(!CELL_NO_POTENTIAL_LINK)

stopifnot(links %>% 
            filter(!LINK_IS_VIRTUAL) %>% nrow() == sum(links$LINK_IS_ACTUAL_COOP))

```



## Under-sample virtual links
```{r}
links_us = 
  links %>% 
  filter(!LINK_POSSIBLE_FALSENEG) %>% 
  filter(LINK_TO_KEEP_TO_US_VIRTUAL) # this is only TRUE currently. 
```

## Extract development sets
Any processing on explanatory features' values should be made before this step so it is done once for all the data
```{r}
cells = 
  cells %>% 
  mutate(SPLIT = if_else(CELL_VOLUME_OBSERVED, "Development set", "No data")) 

stg1_development =
  cells %>% filter(SPLIT == "Development set") 

links_us = 
  links_us %>% 
  mutate(SPLIT = if_else(CELL_ACTUAL_LINK & !CELL_ACTUAL_ONLYOTHER_LINK, "Development set", "No data")) 

stg2_development =
  links_us %>%
  filter(SPLIT == "Development set") 

cells_stg2_development =
  stg2_development %>%
  distinct(CELL_ID, .keep_all = TRUE) %>% 
  select(starts_with("CELL_"))
  


```



# FIRST STAGE 

## Formula
```{r}
anyNA(stg1_development)
# names(cells)
# anyNA(cells)
# stg1_development %>% names() %>% grep(~., pattern = "CELL_COUNT_", value = T)
# stg1_development %>% select(
#     contains("_HA") & contains("CELL_AVG"),
# ) %>% names() %>% grep(~., pattern = "CELL_AVG_", value = T)


stg1_allfeatures = 
  cells %>% 
  select(
    # Exclude all LU vars, on both cell and coop level, except for settlements. 
    (starts_with("CELL_AVG_") & !ends_with("_HA")) | 
    contains("SETTLEMENT")                         |
    starts_with("CELL_PROP_")                      | 
    starts_with("CELL_COUNT_"),
    # some corrections/additions
    -CELL_PROP_VOLUME_COOPS, -CELL_PROP_VOLUME_OTHERS,
    
    CELL_TRI_MM, CELL_N_BS_WITHIN_DIST, CELL_N_COOP_IN_DPT, CELL_MIN_DISTANCE_METERS, CELL_MIN_TRAVEL_METERS
  ) %>%
  names()

# Include coop-level features summarised at the 1 nearest, the 5 nearest and the whole-coop levels. This increases the simple fit performance from an OOB R2 of .17 (with only summaries at 5 nearest coop-level) to .24  names()  
  
# Restricted list of more exogenous features regarding the further applications of the model. 
# This excludes SSI and certification related variables, as well as the number of known buyers. 
stg1_exofeatures = 
  stg1_allfeatures %>% 
  grep(~., pattern = "DISTANCE|TRAVEL|N_BS_WITHIN|N_KNOWN_BS|N_COOP_IN|LICBUY|STATUS|_TRI|SETTLEMENT|COOP_FARMERS$", 
       value = TRUE) %>% 
  unique()

setdiff(stg1_allfeatures, stg1_exofeatures)

stg1_formula_allfeatures = as.formula(paste0("CELL_PROP_VOLUME_COOPS ~ ", paste0(stg1_allfeatures, collapse = " + ")))
stg1_mformula_exofeatures = as.formula(paste0("CELL_PROP_VOLUME_COOPS ~ ", paste0(stg1_exofeatures, collapse = " + ")))

```

The first stage has `r length(stg1_allfeatures)` potential features, `r length(stg1_exofeatures)` of which are exogenous to further applications. 

## Simple fit 
This is simple because it does not do feature selection nor optimize hyper-parameters.
```{r}
stg1_mfit = ranger(data = stg1_development, 
                   importance = 'permutation',
                   case.weights = "CELL_REPRESENTATIVITY_STD_WEIGHT",
                   formula = stg1_formula_allfeatures, 
                   seed = 8888
                   )
print(stg1_mfit)

stg1_mfit = ranger(data = stg1_development, 
                   importance = 'permutation',
                   case.weights = "CELL_REPRESENTATIVITY_STD_WEIGHT",
                   formula = stg1_mformula_exofeatures, 
                   seed = 8888
                   )
print(stg1_mfit)

```

## Model development

### caret
```{r}
# 400 boostraps prend quand même ~1h+ ! 
model1_caret = 
  train(
    x = stg1_development %>% select(all_of(stg1_allfeatures)),
    y = stg1_development$CELL_PROP_VOLUME_COOPS, 
    method="rf", 
    importance=TRUE, 
    tuneLength=5,
    trControl=trainControl(method="boot632",number=400,savePredictions=T)
  )

saveRDS(model1_caret, 
        file = here("temp_data", "model", "caret", "stage_1", "allfeatures_rf_400boot632_5tuneL"))

model1_caret_xgb = 
  train(
    x = stg1_development %>% select(all_of(stg1_allfeatures)),
    y = stg1_development$CELL_PROP_VOLUME_COOPS, 
    method="xgbTree", 
    importance=TRUE, 
    tuneLength=5,
    trControl=trainControl(method="boot632",number=4,savePredictions=T)
  )

saveRDS(model1_caret_xgb, 
        file = here("temp_data", "model", "caret", "stage_1", "allfeatures_xgbtree_4boot632_5tuneL"))
```


### familiar
Doing feature ranking and hyper-parameter optimization
This is the model development. 

This is all handled by package familiar. 

For warm starts: 
precompute_data_assignment --> An experimentData object.
precompute_feature_info    --> An experimentData object.
precompute_vimp            --> An experimentData object.
(each does additional steps)

predict function like anywhere else. 
```{r}
# ?train_familiar
# ?theme_familiar
# ?summon_familiar
# ?as_familiar_data
# ?predict  

anyNA(stg1_development)

# Experimental design 
exp_design = "bt(fs+mb,40)"

# Repository for familiar 1st stage 
# summon_familiar writes all it does in a repo. It returns nothing here.  
stage1_expdes_repo = here("temp_data", "model", "familiar", "stage_1", gsub(",","_",exp_design))
dir.create(stage1_expdes_repo)

# let's go
summon_familiar(
  
  formula = stg1_formula_allfeatures,
  data = stg1_development %>% select(CELL_ID, CELL_PROP_VOLUME_COOPS, all_of(stg1_allfeatures)),
  experiment_dir = stage1_expdes_repo,
  sample_id_column = "CELL_ID",
  outcome_name = "Cooperative outlet share",
  outcome_column = "CELL_PROP_VOLUME_COOPS",
  outcome_type = "continuous",
  experimental_design = exp_design, #"fs + mb", #  # "cv(bt(fs,100) + mb, 3)",
  
  # Use bt because we have many features compared to the number of observations.  "The most practical application of bt is for repeating feature selection multiple times (e.g. bt(fs,50)+mb+ev), as this allows for aggregating variable importance and reducing the effect of random selection."

  # imbalance_correction_method = "random_undersampling", # do not specify, to avoid any kind of under-sampling, since we did it in external pre-processing.  
  parallel = TRUE,
  parallel_nr_cores = detectCores() - 1, 
  
  # PRE-PROCESSING ARGS
  # (not all "none" are the defaults)
  filter_method = "none", # no feature to be filtered in pre-processing
  transformation_method = "none", 
  normalisation_method = "none",
  cluster_method = "hclust", # the default. This is categorised in data processing, but it will affect feature selection. 
  # imputation_method does not matter bc we have no NA. 
  
  # FEATURE SELECTION & OPTIMIZATION 
  fs_method = "random_forest_ranger_permutation", # see https://cran.r-project.org/web/packages/familiar/vignettes/feature_selection_precompiled.html
  # fs_method_parameter = list("random_forest_ranger_permutation" = list()), # this would need to have this format, but let it unspecified, so that feature selection optimizes on ALL hyper-parameters. 
  # vimp_aggregation_method = "borda", # borda is the default. Check guidance to depart. 
  
  learner = "random_forest_ranger",
  # hyperparameter = list("ranger" = list()), #  "If no parameters are provided, sequential model-based optimisation is used to determine optimal hyperparameters."
  
  # EVALUATION INFERENCE
  # The default is "bootstrap_confidence_interval" or "bci", so we have to tell the model to do just quick point estimates where we don't care much about inference. Especially as bci can easily be queried in post-processing.  
  skip_evaluation_elements = c("feature_expressions", "feature_similarity", 
                               "fs_vimp",  "hyperparameters", "ice_data", 
                                "permutation_vimp", "univariate_analysis"), #"model_vimp", 
  
  # part of this list elements are useless now, since they are skipped by the above. 
  estimation_type = list("prediction_data" = "bci", "model_performance"="bci", 
                         "permutation_vimp" = "point", 
                         "ice_data"= "point", 
                         "auc_data" = "point", 
                         "decision_curve_analyis" = "point"), 
  # estimation_type = list("prediction_data" = "hybrid", "model_performance"="hybrid", 
  #                        "permutation_vimp" = "ensemble", 
  #                        "ice_data"= "ensemble", 
  #                        "auc_data" = "ensemble", 
  #                        "decision_curve_analyis" = "ensemble"), 
  confidence_level = 0.95, # the default. 
  detail_level = "hybrid" # the default. See evaluation vignette. 
)

```
The results folder outputed by summon_familiar is saved manually to different place, to protect it from being overwritten in a subsequent run of the above block. 


#### Finders
This block produces convenient finder objects to access familiar outputs. 
```{r}
## ID TO SELECT MODEL TO WORK WITH IN ALL OF BELOW SCRIPT
# ---------------------------------------------
model_date = "20241204190335"
# ---------------------------------------------

# Finders of familiar outputs

# Feature info - this gives generic info about the features in the model 
# feature_info = readRDS(here(stage1_expdes_repo, "20241127134237_feature_info.RDS"))

# Iterations - this gives info about the structure of the sampling applied by the experimental design. 
# iterations = readRDS(here(stage1_expdes_repo, "20241125194101_iterations.RDS"))

# familiar_data - a maxi list containing all info of a run. 
# famdat = readRDS(here(stage1_expdes_repo, "familiar_data", "20241128122859_random_forest_ranger_random_forest_ranger_permutation_1_1_ensemble_1_1_development_data.RDS"))

# Model development results - to report model performance
stage1_results = here(stage1_expdes_repo, "results", "pooled_data")

# Models - to run predictions
stage1_model_path = here(stage1_expdes_repo, 
                         "trained_models", 
                         "random_forest_ranger", 
                         "random_forest_ranger_permutation")

# Select the model/ensemble we want to work with 

stage1_ensemble_names = list.files(stage1_model_path, pattern = "ensemble")
model1_name = 
  stage1_ensemble_names %>% 
  grep(~.,  
       pattern = model_date, 
       value = TRUE) # take the first one in case there are several
                                          
model1 = readRDS(here(stage1_model_path, model1_name))
model1

```

## Performance
### caret
```{r}
# That's the average across all the 500 trees
model1_caret$finalModel$mse %>% mean()
# But the ensemble's mse is averaged with weights equal to each tree's performance (or something)
model1_caret$finalModel

# And this is for all the 400 repetitions of the ensemble 
# It is quite worse and I am not sure why.
resamples <- model1_caret$resample
mean(resamples$RMSE^2)

sd(resamples$RMSE)

mean(resamples$Rsquared)
sd(resamples$Rsquared)

# For XGB
resample1_xgb = model1_caret_xgb$resample 

mean(resample1_xgb$RMSE^2)
sd(resample1_xgb$RMSE^2)
mean(resample1_xgb$Rsquared)
sd(resample1_xgb$Rsquared)



# This is to access performance metrics aggregated over resamples for different models 
resamps <- resamples(list(first_caret = model1_caret_xgb, first_caret2 = model1_caret))
summary(resamps)

```

```{r}
# Performance metrics are stored here
perf_metrics = read.csv2(here(stage1_results, "performance", "performance_metric.csv"))

perf_metrics

# Normalement ces plots sont déjà exportés automatiquement par summon_familiar, mais apparemment pas pour toutes les métriques. 
# This takes ~10min with a 200-models ensemble
plot_perf_model1 = 
  familiar::plot_model_performance(
  object = model1,
  draw = TRUE,
  facet_by = "metric",
  data = stg1_development %>% select(CELL_ID, CELL_PROP_VOLUME_COOPS, all_of(stg1_allfeatures)),
  estimation_type = "bci",
  metric = c("mse", "rse", "r2_score")) #"rmse",  "explained_variance", 

# Save plot
plot_perf_model1
ggsave(
  filename = paste0("performance_metrics.png"),
  path = here("outputs", "familiar", "stage_1", model_date))

```

Several things to say: 


The relative squared error being lower than 1, the model still performs better than the mean predictor. 




## Feature importance & explanation
Importance ranks features wrt. each other but cannot tell direction or magnitude. 
Explanation is produced by feature, and can be interpreted in target scale. 

### Feature importance
#### caret
```{r}
varImp(model1_caret)

model1_caret$finalModel$importance %>% as.data.frame() %>% arrange(desc(`%IncMSE`))


```

#### familiar
```{r}
importance_repo = here(stage1_results, "variable_importance")

permut_vimp = read.csv2(here(importance_repo, "variable_importance_permutation.csv"))

vimp_top10 = 
  permut_vimp %>% 
  filter(similarity_threshold == 1 & data_set == "development" & value !=0) %>% 
  arrange(desc(value)) %>% 
  pull(feature) %>% 
  head(10)

# The plot_model_signature_occurrence method plots the occurrence of features among the first 5 ranks across an ensemble of models (if available). The rank threshold can be specified using the rank_threshold argument or the eval_aggregation_rank_threshold parameter (summon_familiar).


#Note: -------
# The summon_familiar outputs below are not yet aggregated, they are bootstrap-specific lists of ranked features used in model development.  
# stage1_vimp = here(stage1_expdes_repo, "variable_importance")
# vimp = readRDS(here(stage1_vimp, "20241127134237_fs_random_forest_ranger_permutation.RDS"))
# vimp_hp = readRDS(here(stage1_vimp, "random_forest_ranger", "20241127134237_hyperparameters_random_forest_ranger_1_1.RDS"))
```
The most important variables, across the ensemble of models, are: 
1/ the number of (CELL_COUNT_5_NEAREST_COOP_N_KNOWN_BS); 
2/ the road distance to the closest buying station (CELL_MIN_DISTANCE_METERS);
3/ the number of RFA farmers in the 5 nearest cooperatives (CELL_COUNT_5_NEAREST_COOP_FARMERS_RFA); 
4/ the cocoa extent in a 10km buffer around all the cooperatives located within ~70km (CELL_AVG_COOP_BS_10KM_COCOA_HA); 
5/ the number of buying stations within 70km (CELL_N_BS_WITHIN_DIST);
6/ the proportion of simplified coops in the 5 nearest cooperatives (CELL_PROP_5_NEAREST_COOP_STATUS_SCOOPS); 
7/ the proportion of sustainable sourcing initiatives in the 5 nearest cooperatives (CELL_PROP_5_NEAREST_COOP_HAS_SSI);

Now, let's see whether these features contribute to increase or decrease the cell's share of cooperative outlet.  

### Explanation
Here we use Individual Conditional Expectation (ICE) and Partial Dependence (PD) plots.
All this is sketchy, and interpretations are challenging !!! 

```{r}
# These plots are readily available in the repository below. 
explanation_repo = here(stage1_results, "explanation")

# To have a global picture, look at the correlation tests (I am not sure about this!)

pd_corr_table = data.frame()
for(impvar in vimp_top10){
  pdvar = 
    read.csv2(here(explanation_repo, paste0("explanation_pd_", impvar, ".csv"))) %>% 
    filter(data_set == "development") 
  
  cortest_pd = 
    stats::cor.test(x = as.numeric(pdvar$feature_x_value),
                    y = as.numeric(pdvar$predicted_outcome))

  cortest_actual = 
    stats::cor.test(x = stg1_development %>% pull(impvar),
                    y = stg1_development$CELL_PROP_VOLUME_COOPS)

  pd_corr_table[1:2,impvar] = c(cortest_pd$estimate, cortest_pd$p.value)
  
  pd_corr_table[3:4,impvar] = c(cortest_actual$estimate, cortest_actual$p.value)
  row.names(pd_corr_table) = c("Correlation coefficient PD", "p value PD",
                               "Correlation coefficient", "p value")
}

pd_corr_table
```
All important features positively explain the share of coop outlet, except the distance of the closest coop (which is expected), and the number of RFA farmers (which is surprising). 



# SECOND STAGE 

## Formula
```{r}
stg2_development %>% names() %>% grep(~., pattern = "CELL_PROP_", value = T)
stg2_development %>% select(
     (ends_with("_HA"))
) %>% names() %>% grep(~., pattern = "CELL_AVG_", value = T)

stg2_allfeatures = 
  stg2_development %>% 
  select(
    # Do not include any LU var, on either cell or coop level, except for settlements. 
    # Do not include SSI-related vars either, because the development data is biased towards CCP, and thus ANY_SSI and not any other SSI. 
    # stg2_development %>% filter(grepl("CARGILL", PRO_ID)) %>% pull(COOP_SSI_CARGILL) %>% summary()
    
    # Cell level
    CELL_TRI_MM, CELL_SETTLEMENT_HA,
    
    # Topologic - i.e. other-link level
    CELL_N_BS_WITHIN_DIST, CELL_N_COOP_IN_DPT, CELL_AVG_N_LICBUY_IN_DPT, 
    # We could add cell level average/prop/count of nearest and potential coops, but they are not in the link data set currently. 
    # CELL_MIN_DISTANCE_METERS, this does not exist either currently. 
    
    # Link level
    LINK_TRAVEL_MINUTES, LINK_TRAVEL_METERS, LINK_DISTANCE_METERS, 
    
    # Coop level 
    starts_with("COOP_FARMERS"), 
    COOP_N_KNOWN_BS, 
    # starts_with("COOP_DISTRICT_"), -COOP_DISTRICT_NAME,
    starts_with("COOP_STATUS_"), 
    COOP_BS_10KM_TRI,
    
    # More endogenous: 
    COOP_N_KNOWN_BUYERS, 
    COOP_RFA, COOP_UTZ, COOP_FT,
    COOP_CERTIFIED 
    # Never include those, because our observed links data is biased towards link existence when there is a SSI (the CCP, Cargill's SSI). 
    # Don't include dummies for other SSIs than Cargill's, because having many training data from Cargill means our observed links data is biased towards link existence when there is no other SSI than Cargill's. 
    # COOP_HAS_SSI, 
    # starts_with("COOP_SSI_"), 
   
  ) %>%
  names()  
  
# Restricted list of more exogenous features regarding the further applications of the model. 
# This additionally exclude certification related variables, as well as the number of known buyers. 
stg2_exofeatures = 
  stg2_allfeatures %>% 
  grep(~., pattern = "DISTANCE|TRAVEL|N_BS_WITHIN|N_KNOWN_BS|N_COOP_IN|LICBUY|STATUS|_TRI|SETTLEMENT|COOP_FARMERS$", 
       value = TRUE) %>% 
  unique()

setdiff(stg2_allfeatures, stg2_exofeatures)

stg2_formula_allfeatures = as.formula(paste0("LINK_IS_ACTUAL_COOP_class ~ ", paste0(stg2_allfeatures, collapse = " + ")))
stg2_mformula_exofeatures = as.formula(paste0("LINK_IS_ACTUAL_COOP_class ~ ", paste0(stg2_exofeatures, collapse = " + ")))



```

## Simple fit
```{r}
set.seed(8888)

stg2_simple =
  boost_tree(
    mode = "classification",
    mtry = 6,
    trees = 5,
    min_n = 5
  ) %>%
  fit(data = stg2_development,
      formula = stg2_formula_allfeatures)

stg2_simple

```



## Model development

### caret
```{r}
# Custom summary function (otherwise not possible to obtain all metrics in a single run of caret::train)
customSummary <- function(data, lev = NULL, model = NULL) {
  # Calculate twoClassSummary metrics
  twoClass <- twoClassSummary(data, lev, model)
  
  # Calculate prSummary metrics
  pr <- prSummary(data, lev, model)
  
  # Combine the results
  out <- c(twoClass, pr)
  
  # Add accuracy
  accuracy <- sum(data$obs == data$pred) / length(data$obs)
  out <- c(out, Accuracy = accuracy)
  
  return(out)
}

set.seed(8888)
seeds <- vector(mode = "list", length = 6)
for(i in 1:6) seeds[[i]] <- sample.int(1000, 36)
seeds[[6]] = sample.int(1000, 1)

model2_caret_xgb = 
  train(
    x = stg2_development %>% select(all_of(stg2_allfeatures)),
    y = stg2_development$LINK_IS_ACTUAL_COOP_class, 
    method="xgbTree", 
    metric = "ROC",
    tuneLength=3,
    # More about this function here: https://topepo.github.io/caret/model-training-and-tuning.html#control
    trControl=trainControl(method="cv",number=5,
                           savePredictions=T, # TRUE is equivalent to "all" and is necessary to use thresholder() later on. 
                           #returnResamp = TRUE,
                           verboseIter = TRUE,
                           summaryFunction = customSummary, 
                           classProbs = TRUE,  # necessary to obtain ROC
                           seeds = seeds
                           ) 
  )

model2_caret_xgb

saveRDS(model2_caret_xgb, 
        here("temp_data", "model", "caret", "stage_2", "model2_caret_xgb"))

model2_caret_xgb = readRDS(here("temp_data", "model", "caret", "stage_2", "model2_caret_xgb"))

```


### familiar
```{r}
stop()
# Columns with NAs
names(which(colSums(is.na(stg2_development)) > 0)) %>% intersect(stg2_allfeatures)


# Experimental design 
exp_design = "cv(fs+mb, 5)"  # "bt(fs+mb,5)"

# Repository for familiar 1st stage 
# summon_familiar writes all it does in a repo. It returns nothing here.  
stage2_expdes_repo = here("temp_data", "model", "familiar", "stage_2", gsub(",","_",exp_design))
dir.create(stage2_expdes_repo)

# let's go
summon_familiar(
  
  formula = stg2_formula_allfeatures,
  data = stg2_development %>% select(LINK_ID, LINK_IS_ACTUAL_COOP_class, all_of(stg2_allfeatures)),
  experiment_dir = stage2_expdes_repo,
  sample_id_column = "LINK_ID",
  outcome_name = "Link existence",
  outcome_column = "LINK_IS_ACTUAL_COOP_class",
  outcome_type = "binomial",
  experimental_design = exp_design, #"fs + mb", #  # "cv(bt(fs,100) + mb, 3)",

  # Use bt because we have many features compared to the number of observations.  "The most practical application of bt is for repeating feature selection multiple times (e.g. bt(fs,50)+mb+ev), as this allows for aggregating variable importance and reducing the effect of random selection."

  learner = "xgboost_tree",
  
  # Set hyper-parameters so that feature selection and model building stages do not tune this. 
  # Otherwise, training the model is too long! 
  # However, do NOT specify the signature size (i.e. the number of features to include "sign_size"), to let this be optimized based on feature importance. 
                                            
  # input default values in ranger
  fs_method_parameter = list("random_forest_ranger_permutation"=
                               list("n_tree" = 500,
                                     "sample_size" = 0.632,
                                     "m_try" = 0.5, # ranger's default is round(sqrt(length(stg2_allfeatures)),0),
                                      # but apparently familiar uses a share 
                                     "node_size" = 10,
                                     "tree_depth" = 10 # ranger's default is 0 which means infinite. Here Inf is not accepted, so put the higher bound of familiar's default range. 
                                     )),
  # Input default values in xgboost::xgb.train 
  # hyperparameter = list("xgboost_tree"=
  #                         list("n_boost" = 2.7, # this means 500 trees
  #                              "learning_rate" = -1.5, # for log10 to make ~0.3, the default (eta)
  #                              "alpha" = -6, # for log10 to make 0, the default
  #                              "lambda" = 0, # for log10 to make 1, the default
  #                              "tree_depth" = 6,
  #                              "sample_size" = 0.632,
  #                              "min_child_weight" = 0.3, # for log10 -1 to make 1, the default
  #                              "gamma" = -1 # no default in xgboost, just the middle of familiar's default range
  #                              
  #                               )),
  
  # imbalance_correction_method = "random_undersampling", # do not specify, to avoid any kind of under-sampling, since we did it in external pre-processing.  
  parallel = TRUE,
  parallel_nr_cores = detectCores() - 1, 
  
  # PRE-PROCESSING ARGS
  # (not all "none" are the defaults)
  filter_method = "none", # no feature to be filtered in pre-processing
  transformation_method = "none", 
  normalisation_method = "none",
  cluster_method = "hclust", # the default. This is categorised in data processing, but it will affect feature selection. 
  # imputation_method does not matter bc we have no NA. 
  
  # FEATURE SELECTION & OPTIMIZATION 
  fs_method = "random_forest_ranger_permutation", # see https://cran.r-project.org/web/packages/familiar/vignettes/feature_selection_precompiled.html

  # vimp_aggregation_method = "borda", # borda is the default. Check guidance to depart. 
  
  # hyperparameter = list("ranger" = list()), #  "If no parameters are provided, sequential model-based optimisation is used to determine optimal hyperparameters."
  
  # EVALUATION INFERENCE
  # The default is "bootstrap_confidence_interval" or "bci", so we have to tell the model to do just quick point estimates where we don't care much about inference. Especially as bci can easily be queried in post-processing.  
  skip_evaluation_elements = c("feature_expressions", "feature_similarity", 
                               "fs_vimp",  "hyperparameters", "ice_data", 
                                "univariate_analysis"), #"model_vimp", "permutation_vimp", 
  
  # part of this list elements are useless now, since they are skipped by the above. 
  estimation_type = list("prediction_data" = "bci", 
                         "model_performance"="bci", 
                         "permutation_vimp" = "point", 
                         "ice_data"= "point", 
                         "auc_data" = "point", 
                         "decision_curve_analyis" = "point"), 
  # estimation_type = list("prediction_data" = "hybrid", "model_performance"="hybrid", 
  #                        "permutation_vimp" = "ensemble", 
  #                        "ice_data"= "ensemble", 
  #                        "auc_data" = "ensemble", 
  #                        "decision_curve_analyis" = "ensemble"), 
  confidence_level = 0.95, # the default. 
  detail_level = "hybrid" # the default. See evaluation vignette. 
)

```

#### Finders
This block produces convenient finder objects to access familiar outputs. 
```{r}
## ID TO SELECT MODEL TO WORK WITH IN ALL OF BELOW SCRIPT
# ---------------------------------------------
model2_date = "20241206091347"
# ---------------------------------------------

# Finders of familiar outputs

# Feature info - this gives generic info about the features in the model 
# feature_info = readRDS(here(stage2_expdes_repo, "20241127134237_feature_info.RDS"))

# Iterations - this gives info about the structure of the sampling applied by the experimental design. 
# iterations = readRDS(here(stage2_expdes_repo, "20241125194101_iterations.RDS"))

# familiar_data - a maxi list containing all info of a run. 
# famdat = readRDS(here(stage2_expdes_repo, "familiar_data", "20241128122859_random_forest_ranger_random_forest_ranger_permutation_1_1_ensemble_1_1_development_data.RDS"))

# Model development results - to report model performance
stage2_results = here(stage2_expdes_repo, "results", "pooled_data")

# Models - to run predictions
stage2_model_path = here(stage2_expdes_repo, 
                         "trained_models", 
                         "xgboost_tree", 
                         "random_forest_ranger_permutation")

# Select the model/ensemble we want to work with 

stage2_ensemble_names = list.files(stage2_model_path, pattern = "ensemble")
model2_name = 
  stage2_ensemble_names %>% 
  grep(~.,  
       pattern = model2_date, 
       value = TRUE) # take the first one in case there are several
                                          
model2 = readRDS(here(stage2_model_path, model2_name))
model2

```


## Performance 
```{r}
# Access resampling results
resamples <- model2_caret_xgb$resample

# Calculate mean and standard deviation of performance metrics
mean_roc <- mean(resamples$ROC)
sd_roc <- sd(resamples$ROC)
mean_sens <- mean(resamples$Sens)
sd_sens <- sd(resamples$Sens)
mean_spec <- mean(resamples$Spec)
sd_spec <- sd(resamples$Spec)

# This is to access performance metrics aggregated over resamples for different models 
resamps <- resamples(list(first_caret = model2_caret_xgb, first_caret2 = model2_caret_rf))
summary(resamps)

# And to run statistical tests of the diff in performance between models: 
# https://topepo.github.io/caret/model-training-and-tuning.html#extracting-predictions-and-class-probabilities


```

## Feature importance & explanation
Importance ranks features wrt. each other but cannot tell direction or magnitude. 
Explanation is produced by feature, and can be interpreted in target scale. 

### Feature importance
```{r}
varImp(model2_caret_xgb)

model2_caret_xgb$finalModel$importance

```

### Explanation
```{r}

```




# PREDICTION
Here, we build the prediction setS for each stage. 
Based on AoA of 1st stage model. 
Moreover, we report here on the Novelty of cells and links in (and out of) the prediction sets. 

## Read developed models
```{r}

```


## AoA
```{r}
# Requires the model to be trained with caret, with cross-validation and not bootstraps

model1_caret_cv = 
  train(
    x = stg1_development %>% select(all_of(model1@required_features)),
    y = stg1_development$CELL_PROP_VOLUME_COOPS, 
    method="rf", 
    importance=TRUE, 
    tuneLength=1,
    trControl=trainControl(method="cv",number=5,savePredictions=T)
  )

model1_caret_cv$results

aoa1 = 
  aoa(newdata = cells %>% select(all_of(model1@required_features)),
      model = model1_caret_cv)

cells$CELL_IS_IN_AOA = if_else(aoa1$AOA == 1, TRUE, FALSE)

cells$CELL_IS_IN_AOA %>% summary()

# Map 

toplot = 
  cells %>% 
  st_as_sf(coords = c("CELL_LONGITUDE", "CELL_LATITUDE"), crs = 4326) %>% 
  st_transform(civ_crs) 

ggplot(toplot) +
    geom_sf(aes(col = CELL_IS_IN_AOA), size = 1) + #, shape = 15
    geom_sf(data = departements, fill = "transparent", col = "black")

# note that it discards some Cargill links (~330)

```


## Novelty
### caret
The caret package does not have built-in functionality specifically for computing the "novelty" of samples in the same way the familiar package does. 
One alternative approach proposed by Copilot is to use distance-based methods or leverage the Mahalanobis distance to measure how far new samples are from the training data distribution.
```{r}
# Calculate Mahalanobis distance for test data
cov_matrix <- cov(stg2_development %>% select(all_of(stg2_allfeatures)))
center <- colMeans(stg2_development %>% select(all_of(stg2_allfeatures)))
links$LINK_mahalanobis_dist <- mahalanobis(links %>% select(all_of(stg2_allfeatures)), center, cov_matrix)

```

### familiar
```{r}
# Novelty in development data 
devdat_prediction_1 = read.csv2(here(stage1_results, "prediction", "prediction.csv"))
devdat_prediction_2 = read.csv2(here(stage2_results, "prediction", "prediction.csv"))

devdat_prediction_1$novelty %>% as.numeric() %>% summary()

# It is possible to add features that were not in the model initial formula, to gauge novelty (?)
# but would it make sense (to include LU vars) here? 

# Novelty in to-predict data

cells$CELL_NOVELTY = predict(object = model1, newdata = cells, type = "novelty")

cells$CELL_NOVELTY %>% summary()

# Novelty in the Cells data is in the range of novelty in the 1st stage dev data. 
# So we don't take it further into account. 
```



## Prediction sets
```{r}
# Keep cells that are sufficiently close in terms of AoA, and, 
#to reduce computation, that have no close (72km) coop, as for those we attribute null probas.  
cells_topredict = 
  cells %>% 
  filter(CELL_IS_IN_AOA & !CELL_NO_POTENTIAL_LINK)

links = 
  links %>%
  left_join(cells %>% select(CELL_ID, CELL_IS_IN_AOA), 
            by = "CELL_ID")


# Should it be the under-sampled data rather here? NO, I don't think so. We DO want to make predictions for all virtual links! 
links_topredict =
  links %>% 
  filter(CELL_IS_IN_AOA & !CELL_NO_POTENTIAL_LINK)


```

## Development/prediction balance tests
 
```{r}

```


## Predict 1st stage
```{r}
tic()
cells_topredict$CELL_SHARE_COOP = predict(model1, newdata = cells_topredict)$predicted_outcome
toc()
# From here, to shorten code, let's start a fresh naming convention for the target feature LINK_PROBA. 

cells_topredict$CELL_SHARE_COOP %>% summary()

cells = 
  cells %>% 
  left_join(cells_topredict %>% select(CELL_ID, CELL_SHARE_COOP), 
            by = "CELL_ID")

```

### Threshold
1st stage threshold: whether the share of coop outlet is significantly different from zero (one-sided t-test) 
```{r}
# In each cell, conduct a one-sided t-test that the predicted share of coop outlet is positive. 

# Shall produce variable CELL_SHARE_COOP_SIGNI which is the same as CELL_SHARE_COOP, but 0 where not significantly different from zero. 

cells =
  cells %>% 
  mutate(CELL_SHARE_COOP_SIGNI = if_else(CELL_SHARE_COOP < 0.1,  
                                         0, 
                                         CELL_SHARE_COOP))

cells$CELL_SHARE_COOP %>% summary()
cells$CELL_SHARE_COOP_SIGNI %>% summary()

```

### Determinist prediction
This is just where we are sure that cocoa can be sold to a cooperative because there is no coop. 
```{r}
cells =
  cells %>% 
  mutate(CELL_SHARE_COOP_SIGNI = if_else(CELL_NO_POTENTIAL_LINK, 
                                          0, 
                                          CELL_SHARE_COOP_SIGNI))

cells$CELL_SHARE_COOP_SIGNI %>% summary()

stopifnot(sum(is.na(cells$CELL_SHARE_COOP_SIGNI)) == sum(aoa1$AOA==0))
```


## Predict 2nd stage
```{r}
# Predict the existence that a link exists
tic()
links_topredict$LINK_PROBA = predict(model2_caret_xgb, 
                                     newdata = links_topredict %>% select(all_of(model2_caret_xgb$finalModel$feature_names)), 
                                     type = "prob")$Actual_links
toc()

# From here, to shorten code, let's start a fresh naming convention for the target feature LINK_PROBA. 

links_topredict$LINK_PROBA %>% summary()

links = 
  links %>% 
  left_join(links_topredict %>% select(LINK_ID, LINK_PROBA), 
            by = "LINK_ID")

```


### Threshold
2nd stage threshold: whether the (still conditional and not normalised) proba that there is a link is higher than the threshold that minimises false positives. 
```{r}
# Identify the optimal class probability threshold, and apply it to every link

# To be conservative regarding the extent of supply sheds, we have a preference for false negatives with regards to false positives. Thus, we choose a threshold that maximises the performance metric about false positives: we minimise the   
optimal_thresholds = thresholder(model2_caret_xgb, threshold = seq(0, 1, by = 0.05))
# seq(0.0001, 0.001, by = 0.0001)

(thresh = 
  optimal_thresholds %>% 
  arrange(desc(Precision)) %>% 
  head(1) %>% 
  pull(prob_threshold))

links = 
  links %>% 
  mutate(LINK_PROBA_THRESH = if_else(LINK_PROBA < thresh, 
                                     0, 
                                     LINK_PROBA)) # returns NA for NA probas (!AoA)
```


### Determinist prediction
This is just where we are sure that no link can exist because there is no coop. 
```{r}
links =
  links %>% 
  mutate(LINK_PROBA_THRESH = if_else(CELL_NO_POTENTIAL_LINK, 
                              0, 
                              LINK_PROBA_THRESH)) # it could also be 1 here, since we normalise then. But keep some variation between links with a positive proba. 

# After this, the number of links that are NA is 297074
links$LINK_PROBA_THRESH %>% summary()
# which is the same as the number of links in cells outside the AoA. 
n_links_outside_aoa = nrow(filter(links, !CELL_IS_IN_AOA)) 

stopifnot(
  links %>% filter(is.na(LINK_PROBA_THRESH)) %>% nrow() ==  n_links_outside_aoa
)


links$LINK_PROBA %>% quantile(seq(0.99, 1, 0.001), na.rm = T)
links$LINK_PROBA_THRESH %>% quantile(seq(0.999, 1, 0.0001), na.rm = T)

```





# POST-PROCESSING 

## Unconditional probabilities
Here we multiply probabilities predicted in each stage by each other and/or by the share of cocoa presence. 
This is all at the Links level. 
There are several combinations, yielding different interpretations: 

```{r}
links_beforepost = links

links = 
  links %>% 
  left_join(
    cells %>% select(CELL_SHARE_COOP_SIGNI, CELL_ID), # CELL_SHARE_COOP, not even sure it is necessary to join this. 
    by = "CELL_ID"
  ) 

links = 
  links %>% 
  mutate(
    LINK_UNCOND_PROBA = CELL_SHARE_COOP_SIGNI * LINK_PROBA, # include this just for exploration. 
    LINK_UNCOND_PROBA_THRESH = CELL_SHARE_COOP_SIGNI * LINK_PROBA_THRESH
  )

```



## Normalisation (cell perspective)
We do it after unconditioning, to normalise unconditional probabilities too. 
```{r}
    # Work non-thresholded proba for now, but we will surely not keep them. 

links = 
  links %>% 
  group_by(CELL_ID) %>% 
  mutate(
    CELL_NORMSUM_THRESH = sum(LINK_PROBA_THRESH), 
    CELL_UNCOND_NORMSUM_THRESH = sum(LINK_UNCOND_PROBA_THRESH)
  ) %>% 
  ungroup() %>% 
  mutate(
    # When denominator is null, i.e. all links in cell have probas = 0, then make the normalised probas = 0.
    # When it's NA, it means we are in a cell outside the AoA, so leave it NA
    LINK_PROBA_THRESH_NORMED = case_when(
      CELL_NORMSUM_THRESH > 0 ~ LINK_PROBA_THRESH / CELL_NORMSUM_THRESH, 
      is.na(CELL_NORMSUM_THRESH) ~ NA,
      TRUE ~ 0
      ), 
    LINK_UNCOND_PROBA_THRESH_NORMED = case_when(
      CELL_UNCOND_NORMSUM_THRESH > 0 ~ LINK_UNCOND_PROBA_THRESH / CELL_UNCOND_NORMSUM_THRESH, 
      is.na(CELL_UNCOND_NORMSUM_THRESH) ~ NA,
      TRUE ~ 0
      )
  )

stopifnot(
links %>% filter(is.na(LINK_PROBA_THRESH_NORMED)) %>% pull(CELL_IS_IN_AOA) %>% any() == FALSE
)

links %>% filter(is.na(LINK_PROBA_THRESH_NORMED)) %>% nrow() 
links %>% filter(is.na(LINK_UNCOND_PROBA_THRESH_NORMED)) %>% nrow() 

summary(links$CELL_NORMSUM_THRESH)
summary(links$CELL_UNCOND_NORMSUM_THRESH)
# It is not anormal that these sums are higher than 1, that's precisely why we normalise actually. 

# links %>% filter(CELL_NORMSUM_THRESH > 1) %>% View()


stopifnot(
  links$LINK_PROBA_THRESH_NORMED %>% sum(na.rm = T) %>% round(5) + 
    length(unique(pull(filter(links, CELL_NORMSUM_THRESH == 0), CELL_ID))) == 
    length(unique(links_topredict$CELL_ID)) |
    
  links$LINK_UNCOND_PROBA_THRESH_NORMED %>% sum(na.rm = T) %>% round(5) + 
    length(unique(pull(filter(links, CELL_UNCOND_NORMSUM_THRESH == 0), CELL_ID))) == 
    length(unique(links_topredict$CELL_ID)) 
)

```


## Links most likely
This is an important step, where we define, for every cell, to which coop it most likely supplies (conditional on supplying any coop in the first place). 

This is not yet non-overlapping polygons though, as we still have many equi-probabilities... 

```{r}
# Find the max conditional, normalised proba in every cell. 
# Using the unconditional probas would yield exactly the same result, as it just differs by a cell-level quantity, 
# EXCEPT in cells that don't pass the 1st stage threshold. There, unconditional probas would all be null, but conditional probas aren't. So using conditional probas allows to also determine which coop is most likely supplied in cells not selling to coops at all. 

links = 
  links %>% 
  group_by(CELL_ID) %>% 
  mutate(
    LINK_PROBA_THRESH_NORMED_CELLMAX = max(LINK_PROBA_THRESH_NORMED), 
  ) %>% 
  ungroup() %>% 
  mutate(LINK_IS_MOST_LIKELY = case_when(
    LINK_PROBA_THRESH_NORMED_CELLMAX == 0 | # where all probas are null
      # or where their maximum is strictly higher than the link's proba, the link is not most likely
    LINK_PROBA_THRESH_NORMED_CELLMAX > LINK_PROBA_THRESH_NORMED ~ FALSE, 
    is.na(LINK_PROBA_THRESH_NORMED_CELLMAX) ~ NA,
    TRUE ~ TRUE
    )
  ) %>% 
  group_by(CELL_ID) %>% 
  mutate(CELL_N_MOST_LIKELY_LINKS = sum(LINK_IS_MOST_LIKELY, na.rm = T)) %>% 
  ungroup() 

# Equi-probabilities are allowed. We have them for many of the observed links, which 
# predicted probability to exist is 1, and thus normalised probability is 1/n with n is the number of observed links in cell. 
# Plus a few truly predicted probas. 
links %>%   
  filter(CELL_N_MOST_LIKELY_LINKS > 1) %>% 
  arrange(CELL_ID) %>% 
  View()
  
links %>%   
  filter(CELL_N_MOST_LIKELY_LINKS > 1) %>% 
  filter(!is.na(PRO_ID)) %>% 
  nrow()

```



## Productive vs. unproductive 
What is the task exactly here? 
I think we've already catagorised cells according to presence of cocoa at the top of the script. Nothing more needed here... 
```{r}
table(cells$CELL_PRODUCTION_STATUS)

links = 
  links %>% 
  mutate(
    across(.cols = contains("_PROBA_"), 
           .fns = ~ .x  * CELL_COCOA_SHARE, 
           .names = paste0("{.col}", "_COCOA"))
  )

```


## Normalisation (coop perspective)
We normalise, from the perspective of every coop, for six types of supply sheds, i.e. the combinations of 
either conditional or unconditional probabilities to be linked with the coop, for three kinds of cells: productive only, prospective only, and either. 
```{r}

```



# EXPORT 
```{r}
saveRDS(cells, 
        here("temp_data", "model", "cells.Rdata"))

saveRDS(links, 
        here("temp_data", "model", "links.Rdata"))
```
































# DEPRECATED CODE   


## Hold-out validation 
```{r}

(test_pred_prob <- predict(stg2_mfit, 
                             new_data=stg2_test, 
                             type = "prob"))

stg2_test$LINK_PRED_ACTUAL_COOP_prob = test_pred_prob$`.pred_Actual_links`


(test_pred_class <- predict(stg2_mfit, 
                             new_data=stg2_test, 
                             type = "class")) # this implements a 50% threshold, 
# i.e., if the predicted link is more likely to exist than to not exist, the prediction is classified as "Actual link" 
stg2_test$LINK_PRED_ACTUAL_COOP_class = test_pred_class$.pred_class

stg2_test = 
  stg2_test %>% 
  mutate(TP = LINK_PRED_ACTUAL_COOP_class == "Actual_links"  & LINK_IS_ACTUAL_COOP_class == "Actual_links",
         TN = LINK_PRED_ACTUAL_COOP_class == "Virtual_links" & LINK_IS_ACTUAL_COOP_class == "Virtual_links",
         FP = LINK_PRED_ACTUAL_COOP_class == "Actual_links"  & LINK_IS_ACTUAL_COOP_class == "Virtual_links",
         FN = LINK_PRED_ACTUAL_COOP_class == "Virtual_links" & LINK_IS_ACTUAL_COOP_class == "Actual_links")

```


### Accuracy 
we can calculate the overall accuracy score as a sum of instances where predicted and actual classes match divided by the total number of rows:
```{r}

(accuracy = sum(stg2_test$LINK_PRED_ACTUAL_COOP_class == stg2_test$LINK_IS_ACTUAL_COOP_class) / nrow(stg2_test))
(accuracy = (sum(stg2_test$TP) + sum(stg2_test$TN) ) / (sum(stg2_test$TP) + sum(stg2_test$TN)  + sum(stg2_test$FP) + sum(stg2_test$FN)))

```

### True Positive Rate (Recall) 
```{r}
(recall = sum(stg2_test$TP) / (sum(stg2_test$TP) + sum(stg2_test$FN)))
```

### False Negative Rate (probability of false alarm)
```{r}
(fpr = sum(stg2_test$FP) / (sum(stg2_test$TN) + sum(stg2_test$FP)))
```

### Precision
```{r}
(precision = sum(stg2_test$TP) / (sum(stg2_test$TP) + sum(stg2_test$FP)))

```


### Confusion Matrix  
```{r}
# This requires package caret which fails to install. 
# cm = confusionMatrix(stg2_test$LINK_IS_ACTUAL_COOP_class, stg2_test$LINK_PRED_ACTUAL_COOP_class)
# 
# 
# # Plot it 
# cfm <- as_tibble(cm$table)
# plot_confusion_matrix(cfm, target_col = "Reference", prediction_col = "Prediction", counts_col = "n")

```

### PR-AUC
```{r}
prc_xgboost_test = 
  pr_curve(
    data = stg2_test, 
    truth = LINK_IS_ACTUAL_COOP_class,
    LINK_PRED_ACTUAL_COOP_prob, 
    event_level = "second"
)

(PRAUC = 
  pr_auc(
    data = stg2_test, 
    truth = LINK_IS_ACTUAL_COOP_class,
    LINK_PRED_ACTUAL_COOP_prob, 
    event_level = "second",
    estimator = "binary"
))

ggplot(prc_xgboost_test, aes(x = recall, y = precision)) +
  geom_path() +
  coord_equal() +
  theme_bw()  
  # legend(legend = c(paste0("Main model \nAUC = ",round(PRAUC, 2))))
  
```

### AUROC
```{r}
roc_xgboost_test = 
  roc(
  data = stg2_test, 
  response = LINK_IS_ACTUAL_COOP_class,
  predictor = LINK_PRED_ACTUAL_COOP_prob,
  quiet = TRUE
)

(AUC = roc_xgboost_test$auc)

plot(pROC::smooth(roc_xgboost_test), col = "blue", lwd = 1) 
legend(
  "bottomright",
  col = "blue",
  lwd = 1,
  legend = c(paste0("Main model \nAUC = ",round(AUC, 2)))
)
```


## Prediction (deprecated)
## Simple predict (deprecated)
```{r}
stg1_predict = 
  cells %>% 
  filter(!CELL_NO_POTENTIAL_LINK)


stg1_predict$CELL_PRED_PROP_VOLUME_COOPS = predict(stg1_mfit, data=stg1_predict)$predictions

stg1_predict$CELL_PRED_PROP_VOLUME_COOPS %>% summary()

cells = 
  cells %>% 
  left_join(stg1_predict %>% select(CELL_ID, CELL_PRED_PROP_VOLUME_COOPS), 
            by = "CELL_ID") %>% 
  # this left NAs in cells not in the predict set, i.e. in the development set
  mutate(CELL_PRED_PROP_VOLUME_COOPS = case_when(
    is.na(CELL_PRED_PROP_VOLUME_COOPS) ~ CELL_PROP_VOLUME_COOPS, 
    TRUE ~ CELL_PRED_PROP_VOLUME_COOPS
  ))

cells$CELL_PRED_PROP_VOLUME_COOPS %>% summary()

# at this stage, the NAs come from CELL_PROP_VOLUME_COOPS, in cells with no potential link at all 
stopifnot(cells %>% filter(is.na(CELL_PRED_PROP_VOLUME_COOPS)) %>% pull(CELL_NO_POTENTIAL_LINK) %>% all())

```

- TAKES TOO MUCH TIME, check how to improve efficiency 
```{r}
stg2_predict = links

stg2_predict$LINK_PRED_EXIST_PROB = predict(stg2_mfit, 
                                            new_data=stg2_predict, 
                                            type = "prob")$`.pred_Actual_links`
  

stg2_predict$LINK_PRED_EXIST_PROB %>% summary()

cells = 
  cells %>% 
  left_join(stg2_predict %>% select(CELL_ID, CELL_PRED_PROP_VOLUME_COOPS), 
            by = "CELL_ID") %>% 
  # this left NAs in cells not in the predict set, i.e. in the development set
  mutate(CELL_PRED_PROP_VOLUME_COOPS = case_when(
    is.na(CELL_PRED_PROP_VOLUME_COOPS) ~ CELL_PROP_VOLUME_COOPS, 
    TRUE ~ CELL_PRED_PROP_VOLUME_COOPS
  ))

cells$CELL_PRED_PROP_VOLUME_COOPS %>% summary()

# at this stage, the NAs come from CELL_PROP_VOLUME_COOPS, in cells with no potential link at all 
stopifnot(cells %>% filter(is.na(CELL_PRED_PROP_VOLUME_COOPS)) %>% pull(CELL_NO_POTENTIAL_LINK) %>% all())






```